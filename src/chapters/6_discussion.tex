\chapter{Discussion}

\textbf{NeuRD-fix:}
The NeuRD-loss has also been previously applied on top of algorithms to improve performance or
induce convergence in competitive and cooperative settings.
Chhablani et.al,~\cite{chhablaniCounterfactual2021} showed improved performance in
identical-interest games by applying the NeuRD fix to COMA~\cite{foersterCounterfactual2018}.
Perolat et.al,~\cite{perolatMastering2022} used NeuRD loss to approximate Replicator dynamics as a
part of the DeepNash algorithm.
They used the same reward transformation based adaptive regularization~\cite{perolatPoincare2021}
to induce last-iterate convergence in training agents for the game of Stratego.
The improved performance of adapting the NeuRD-fix with mirror-descent based methods, and other
techniques for last-iterate convergence is also evident from our experimental evaluations.
Through this, we reinforce the idea that the NeuRD-loss can be more generally adapted into loss
functions of various algorithms in multi-agent settings.
\blue{is there a benefit to adding NeuRD in single-agent setting? even if the reward function is not dynamic.}
\\

\textbf{Entropy Regularization:}
As noted in the tabular experiments, and as observed from the performance of MDPO in the deep MARL
experiments, there exist faster methods to induce last-iterate convergence that adaptive
regularization.
\\

\textbf{Trust-region constraints:}
Trust-region constraints form the basis of state-of-the-art RL algorithms like PPO.
The presence of a relatively strongly-convex proximal operator is necessary for deriving algorithms
with strong performance guarantees.

