\chapter{Discussion}

\textbf{NeuRD-fix:}
The NeuRD-loss has also been previously applied on top of algorithms to improve performance or
induce convergence in competitive and cooperative settings.
Perolat et.~al,~\cite{perolatMastering2022} introduced Regularized Nash Dynamics (R-NAD) that
utilizes NeuRD as a fixed-point approximator along with adaptive
regularization~\cite{perolatPoincare2021} to achieve last-iterate convergence in the game of
Stratego.
Chhablani et.~al,~\cite{chhablaniCounterfactual2021} motivated the choice of an advantage baseline
in COMA~\cite{foersterCounterfactual2018} through no-regret literature and demonstrated that
applying the NeuRD loss to COMA's objective improved its performance even in cooperative settings
such as identical-interest games.
The improved performance of adapting the NeuRD-fix with mirror-descent based methods, and other
techniques for last-iterate convergence is also evident from our experimental evaluations.
Through this, we reinforce the idea that the NeuRD-loss can be more generally adapted into loss
functions of various algorithms in multi-agent settings.
\fillin{is there a benefit to adding NeuRD in single-agent setting? even if the reward function is not dynamic.}
\\

\textbf{Entropy Regularization:}
As noted in the tabular experiments, and as observed from the performance of MDPO in the deep MARL
experiments, there exist faster methods to induce last-iterate convergence that adaptive
regularization.
\\

\textbf{Trust-region constraints:}
Trust-region constraints form the basis of state-of-the-art RL algorithms like PPO.
The presence of a relatively strongly-convex proximal operator is necessary for deriving algorithms
with strong performance guarantees.

