\chapter{Deep Multi-Agent RL Experiments}
All of the methods discussed in this work are typically implemented in function approximation
settings.
Hence, it is important to evaluate if the observed performance improvements carry over when the
policy parameters, and the value approximation are both represented using neural networks.

As mentioned, in this section we study the following variants of MPG by incorporating combinations
of the following modifications - NeuRD Fix (nrd), Entropy regularization (ent), KL Penalty (kl),
Optimistic Updates (opt).
So for instance, a variant that uses the NeuRD fix and entropy regularization will be referred to
as ``MPG-nrd-ent'' within the experiments.
Expanding on our the observations from the tabular experiments, we proceed to evaluate the
algorithms in large-scale 2p0s games to answer question~\ref{qn3}.
For the neural experiments, we study the effect of the NeuRD-fix, and the utilization of
Optimistic-variants of the popular Adam optimizer (Optimistic-Adam) when paired with deep RL
algorithms.

\section{Experimental Domains}
% In this section, we evaluate the deep RL variant of these algorithms in extensive-form game
% representations of popular large-scale two-player zero-sum game variants.
Large-scale real-world games serve as challenging benchmarks for multi-agent reinforcement learning
algorithms by imposing long range dependencies between actions, and rewards, and also present
various tricky dynamics that the agents have to identify to perform well.
Many of these games such as Chess, Go, etc., can be naturally represented as extensive-form games,
and have been major milestones in the progress towards developing general learning agents that
adapt and perform well in various problem domains.

% A major achievement in recent times for reinforcement learning methods in large-scale game benchmarks was 
% AlphaGo's performance against professional Go players including winning against the world champion. 
We follow the suit of~\cite{sokotaUnified2023}, and choose four 2p0s EFGs for the evaluation,
namely - Kuhn Poker, Abrupt Dark Hex, and Phantom Tic-tac-toe.
Kuhn Poker, and 2x2 Dark Hex are smaller benchmarks that were used to test, and tune the
algorithms.
Phantom Tic-tac-toe, and 3x3 Dark Hex, are large-scale benchmarks that are used here to evaluate
the scalability, and stability of these algorithms in converging to an equilibrium.
Please refer to Appendix~\ref{apx:gamedesc} for an overview of these games.

\section{Evaluation Metrics}
The measures used in tabular experiments are harder to compute for large-scale EFGs.
Distance to an equilibrium point is harder to compute due to the absence of a unique equilibrium
point.
For larger games, the computation of exact best responses to measure the exact exploitability is
prohibitive due to the large state space.
However, we can approximate the exploitability computation if we can approximate the best response
for a given fixed policy.

\subsection{Approximate Exploitability}
For poker games, a local best response approximator (LBR)~\cite{lisyEquilibrium2017} has been
commonly used in place of an exact best response computation by combining local search along with
truncation based on a heuristic value function.
However, the value function is based on poker domain-specific knowledge, and does not translate to
other games.
Instead, an alternative LBR formulation was proposed in~\cite{timbersApproximate2022} that uses
MCTS~\footnote{It uses an extension of MCTS to imperfect-information settings called information
	state MCTS (ISMCTS)~\cite{cowlingInformation2012}} and an approximate learnt value function.
The exploitability is then computed by

Then the exploitability can be
approximated by sampling trajectories and measuring the average reward the best response agent
acheived against the exploited policy.
In our experiments, we train the best response agent against the fixed joint-policy learn through
self-play.
Let $\pi_{BR}$ be a learnt best-response approximator, and $\pi_{fixed}=(\pi_1, \pi_2)$ be the
joint policy to be exploited, then the approximate exploitability is given by:
\begin{equation}
	\label{eqn:appxexp} \text{Exp}_{appx} (\pi_{BR}, \pi_{fixed}) = \frac{1}{N} \left[ \sum_{i=1}^{N/2}
		\E_{a \sim \pi_1, \pi_{BR}} [R] + \sum_{i=1}^{N/2} \E_{a \sim \pi_{BR}, \pi_2} [R] \right]
\end{equation} \fillin{TBD: rephrase the equation in terms of cumulative episodic rewards.
}

\section{Experiment Setup}
Due to the strong performance exhibited by MMD~\cite{sokotaUnified2023} as a deep MARL algorithm
for 2p0s games, we consider vanilla-MMD to be our baseline.
For a proper comparison of MMD's performance against the proposed algorithms, we setup our
experiments following~\cite{sokotaUnified2023}, and we outline the various implementation details,
and experimental choices made in the former work here.

MMD was implemented as a modification of RLLib's~\cite{liangRLlib2018} PPO, by modifying the
entropy, and KL annealing schedules.
RLLib's PPO implementation uses MLPs to represent the policy, and value networks, which is
sufficient for the vector representation of information states in OpenSpiel.
The networks have 2 fully connected layers with [256, 256] hidden units, and use tanh activations.
We also note that RLLib's implementation makes use of Generalized Advantage Estimates
(GAE)~\cite{schulmanHighDimensional2018} instead of regular advantage estimates.
The experiments used OpenSpiel~\cite{lanctotOpenSpiel2020} for all the benchmark game
implementations.
RLLib's environment adapter was used for the algorithms to interact with the OpenSpiel
environments, and the RLLib's adapter implementation was modified to work with information states
instead of observations as required by the problem specification.
All algorithms were trained using self-play, and the evaluations were done on the final trained
joint-policy.
We retain all the above choices with PyTorch as the framework of choice within RLLib, while only
making changes for the addition the proposed changes.
For NeuRD, we modify the policy loss component of MMD implementation to use logit-thresholded
advantages, with a $\beta$ of 2.0 (which is the default in OpenSpiel, and we find this to work well
among a few choices that we experimented with [1.0, 2.0, 5.0, 10.0]).
For Optimistic variants of the algorithms, we adapt existing PyTorch implementations of optimistic
versions of popular neural network optimizers (SGD, and Adam).
We use a training batch size of 4000 trajectories, and within each training iteration, we sample
mini-batches of size 128 to perform mini-batch SGD updates.
We perform 30 SGD updates per iteration, and use a constant learning rate of 5e-5 for all the
experiments.

We use the smaller games (Kuhn Poker, and Abrupt Dark Hex 2$\times$2) to identify modifications
that are generally more performant and stable in their implementation as deep RL algorithms.
We then evaluate a subset of these variants that demonstrate a good performance in the large-scale
benchmark games (Abrupt Dark Hex 3$\times$3, and Phantom TTT).
For the smaller games, we train the agents in self-play for 200k steps, and we report the exact
exploitability curves across the training iterations.
For 3x3 Dark Hex, and Phantom TTT, we train the agents for 5M steps.
Due to the difficulty in computing exact exploitability we only report the approximate
exploitability at the end of training.
To compute the approximate exploitability, we train a DQN approximate best-response agent against
the trained joint-policy of the players for 5M steps.
Then, we evaluate the performance of the DQN agent against the fixed joint-policy for 1000
episodes, with the DQN agent starting first in 500, and the exploitee player starting first in the
other 500.
We report the win-rate of the DQN agent across these episodes as the approximate exploitability
which ranges between 0 and 1, as done in~\cite{sokotaUnified2023}.

\section{Results}
\ref{fig:neuralsmall1} plots the exact exploitabililty of the joint policy as a function of the
iterations, and~\ref{fig:neuralsmall1} shows the approximate exploitability at the end of training for these games.
These results show correspondence between exact and approximate exploitability as expected
qualifying the latter as a valid evaluation metric for the larger games.
In agreement to the tabular experiments, the NeuRD versions of these algorithms have a better
performance.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.6\textwidth}
		\includegraphics[width=\textwidth]{figs/kpoker.png}
		\caption{Kuhn Poker}
	\end{subfigure}
	\begin{subfigure}[b]{0.6\textwidth}
		\includegraphics[width=\textwidth]{figs/ahex22.png}
		\caption{Abrupt Dark Hex (2$\times$2) (\red{1M steps version to be added})}
	\end{subfigure}
	\caption{Performance in small EFGs, measured by exact exploitability.}
	\label{fig:neuralsmall1}
\end{figure}

% We also compute approximate exploitability for the smaller games by training a DQN best-response
% agent for 1M steps against the fixed trained agents.

\ref{fig:neural2} shows improvement in performance by applying the NeuRD-fix in Abrupt Dark Hex
(3$\times$3) and Phantom TTT, as measured by the approximate exploitability.
\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figs/pttt.png}
		\caption{Phantom Tic-tac-toe}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figs/ahex33.png}
		\caption{Abrupt Dark Hex (3$\times$3)}
	\end{subfigure}
	\caption{Performance in larger EFGs, measured by approximate exploitability.}
	\label{fig:neural2}
\end{figure}

\section{Algorithm Design Choices}

Firstly, echoing the observations from the tabular experiments, we observe that the addition of
NeuRD-fix improved the performance of MMD.
Secondly, there is no consistent benefit of using the optimistic version of the optimizers in
contrast to their effect in normal-form games.

\fillin{We also evaluate these algorithms by having the trained agents play against each other in a
	head-to-head manner.}

\subsection{NeuRD-fix}
The NeuRD-loss has also been previously applied on top of algorithms to improve performance or
induce convergence in competitive and cooperative settings.
Perolat et.~al,~\cite{perolatMastering2022} introduced Regularized Nash Dynamics (R-NAD) that
utilizes NeuRD as a fixed-point approximator along with adaptive
regularization~\cite{perolatPoincare2021} to achieve last-iterate convergence in the game of
Stratego.
Chhablani et.~al,~\cite{chhablaniCounterfactual2021} motivated the choice of an advantage baseline
in COMA~\cite{foersterCounterfactual2018} through no-regret literature and demonstrated that
applying the NeuRD loss to COMA's objective improved its performance even in cooperative settings
such as identical-interest games.
The improved performance of adapting the NeuRD-fix with mirror-descent based methods, and other
techniques for last-iterate convergence is also evident from our experimental evaluations.
Through this, we reinforce the idea that the NeuRD-loss can be more generally adapted into loss
functions of various algorithms in multi-agent settings.
%is there a benefit to adding NeuRD in single-agent setting? even if the reward function is not dynamic.

\subsection{Entropy Regularization}
As noted in the tabular experiments, in some problem settings there might be faster methods to
induce last-iterate convergence that adaptive regularization.
However, entropy regularization is more widely studied within single, and multi-agent RL.
Adaptive entropy regularization is also relatively easier to implement, and less constraining in
terms of how each player updates their policy as compared to modifications like EG, or Optimism.
In single-agent RL, entropy regularization is motivated as a means to encourage exploration, and
make the policies more robust.
In multi-agent RL, it has been studied more specifically to induce convergence even including in
NeuRD, and MMD.
Given the universal inclusion, and multiple perceived benefits, entropy regularization is
considered a standard design choice to include in most algorithms.

\subsection{Trust-region constraints}
Trust-region constraints form the basis of state-of-the-art RL algorithms like PPO.
The presence of a relatively strongly-convex proximal operator is necessary for deriving algorithms
with strong performance guarantees.

\subsection{EG and Optimism}
Finally, we highlight that the best performing variants are on top of SPG as opposed to MDPO, or
MMD as can be seen from~\ref{fig:tabne_last}.
This is surprising as the latter methods were proposed as improvements over SPG in single, and
multi-agent settings.
But, this indicates that EG/Optimistic updates provide a better last-iterate convergence rates
compared to adaptive entropy regularization with trust-region constraints.
While the convergence rates of EG, and Optimistic updates are well-studied, it is not the case for
adaptive regularization.
Most notably, Optimistic-NeuRD (SPG + NeuRD + Optimism) exhibits a faster last-iterate convergence
that is straightforward to extend to more complicated settings.
