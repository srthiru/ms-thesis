\chapter{Discussion}

\begin{itemize}

    \item{
        \textbf{NeuRD fix:} Since the typical neural network representation of policies use softmax projection on the logits,
        the idea of fixing the gradient updates can be applied more generally to algorithms beyond SPG.
        The NeuRD loss has been adapted into other algorithms to improve performance or induce convergence
        in competitive and cooperative settings.
        Chhablani et.al,~\cite{chhablaniCounterfactual2021} showed improved performance in
        identical-interest games by applying the NeuRD fix to COMA~\cite{foersterCounterfactual2018}.
        Perolat et.al,~\cite{perolatMastering2022} used NeuRD loss to approximate Replicator dynamics 
        as a part of the DeepNash algorithm. 
        They used the same reward transformation based adaptive regularization~\cite{perolatPoincare2021} to induce last-iterate convergence in training agents for the game of Stratego.
        
    }

\end{itemize}