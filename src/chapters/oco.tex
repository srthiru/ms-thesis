%\documentclass{article}

%\usepackage{dsfont}
%\usepackage{amsfonts}

%\begin{document}

%\title{Derivations for the main thesis}

\chapter{Online Learning and Online Convex Optimization}

\section{Online Learning}

Online Learning is a sub-domain of machine learning that has important theoretical and practical applications. 
In Online Learning, a learner is tasked with predicting the answer to a set of questions over a sequence of consecutive rounds.
At each round t, a question $x_t$ is taken from an instance domain $\mathcal{X}$, and the learner is required to predict 
an answer, $p_t$ to this question. After the prediction is made, the correct answer $y_t$, from a target domain $\mathcal{Y}$ 
is revealed and the learner suffers a loss $l(p_t, y_t)$. The prediction $p_t$ could belong to $\mathcal{Y}$ or a larger set, 
$\mathcal{D}$.

There are many special cases of Online learning that translate to popular Online learning problems. Some common ones are,

Online Classification: $\mathcal{Y}=\mathcal{D}=\{0,1\}$, and typically the loss function is the 0-1 loss: $l(p_t, y_t)=|p_t - y_t|$.

Online Regression:

Expert's case:


The goal of an Online learning algorithm is to minimize the cumulative loss across all the rounds it has been through so far.
The learner uses the information from the previous rounds to improve its prediction on present and future rounds.

The sequence of questions can be deterministic, stochastic or even adversarial. This means, for any online learning algorithm 
an adversary can make the cumulative loss unbounded, by simply providing an opposing answer to the algorithm's answer as the correct 
answer. To make learning possible, certain restrictions are imposed on the structure of the problem.

Realizability: It is assumed that the answers are generated by a target mapping $h^*: \mathcal{X} \rightarrow \mathcal{Y}$, and that $h^*$ is 
taken from a fixed set, $\mathcal{H}$ called the hypothesis class. Now, for any Online learning algorithm, A, $M_A(\mathcal{H})$ is the number 
of mistakes $A$ makes on a sequence of questions, labelled by some $h^* \in \mathcal{H}$. $M_A(\mathcal{H})$ is called the $\textit{mistake-bound}$ 
of $A$.

A relaxation from realizable assumption is that the answers are not generated by some fixed mapping $h^*$, but the learner is still only required 
to be competitive with the best fixed predictor from $\mathcal{H}$. This is the regret of an Online learning algorithm for not having followed a 
fixed hypothesis $h^* \in \mathcal{H}$.

\begin{equation}\label{eqn_regretdef}
    Regret_T(h^*) = \sum_{t=1}^T l(p_t, y_t) - \sum_{t=1}^T l(h^*(x_t), y_t),
\end{equation}

The regret of $A$ with $\mathcal{H}$ is,

\begin{equation}\label{eqn_regretdef_all}
    Regret_T(\mathcal{H}) = max_{h^* \in \mathcal{H}} Regret_T(h^*)
\end{equation}


\section{Online Convex Optimization}

An established approach to design efficient online learning algorithm has been using convex optimization. This typically frames online learning as an 
online convex optimization problem as follows:

input: a convex set S
for t = 1, 2. $\ldots$
predict a vector $w_t \in S$
receive a convex loss function $f_t: S \mapsto \mathbb{R}$

Reframing \ref{eqn_regretdef} in terms of convex optimization, we refer to a competing hypothesis here as some vector $u$ from the convex set $S$.

\begin{equation}
    Regret_T(u) = \sum_{t=1}^T f_t(w_t) - \sum_{t=1}^T f_t(u)
\end{equation}

and similarly, the regret with respect to a set of competing vectors $U$ is,
\begin{equation}
    Regret_T(U) = max_{u \in U} Regret_T(u)
\end{equation}

As stated in the case of online learning, the set $U$ can be same as $S$ or different in other cases. In this work, the default setting is $U=S$ and 
$S=\mathbb{R^d}$ unless specified otherwise.

% Convexification can be added here briefly

\subsection{FoReL}

Follow-the-Regularized-leader (FoReL) is a classic learning algorithm for online convex optimization, where the algorithm tries to minimize the loss on 
all past rounds along with a regularization term. The regularization term is used to stabilize the solution and prevent it from oscillating too much every 
round preventing converging to a solution.

The learning rule can be written as,

$$\forall t, w_t = argmin_{w \in S} \sum_{i=1}^{t-1} f_i(w) + R(w).$$

where $R(w)$ is the regularization term. Different regularization functions lead to different algorithms with varying regret bounds.


In the case of linear loss functions with respect to some $z_t$, i.e., $f_t(w) = \langle w, z_t \rangle$, and $S=\mathbb{R}^d$,  if FoReL is run with 
$l_2$-norm regularization $R(w) = \frac{1}{2 \eta} \|w\|_2^2$ , then the learning rule can be written as,

\begin{equation}
    w_{t+1} = -\eta \sum_{i=1}^t z_i = w_t - \eta z_t
\end{equation}

Since, $\nabla f_t(w_t) = z_t$, this can also be written as, $w_{t+1} = w_t - \eta \nabla f_t(w_t)$. This update rule is also commonly known as Online Gradient Descent.
The regret of FoReL run on Online linear optimization with a euclidean-norm regularizer is:

$$Regret_T(U) \leq BL \sqrt {2T}.$$

where $U = {u : \|u\| \leq B}$ and $\frac{1}{T} \sum_{t=1}^T \|z_t\|_2^2 \leq L^2$ with $\eta = \frac{B}{L\sqrt{2T}}$.

This can also be generalized to Convex Functions in general through linearization using the property of convex functions. For a convex set S, a convex function $f: S \mapsto \mathbb{R}$ is convex iff $\forall w \in S, \exists z$ such that,

\begin{equation}
    \forall u \in S, f(u) \leq f(w) + \langle u-w, z \rangle  
\end{equation}

Following this, in Online Convex Optimization for each round $t$, there exists a $z_t$ such that for all competing hypothesis $u$, 

$$f_t(w_t) - f_t(u) \leq \langle w_t - u, z_t \rangle.$$

where $z_t \in \partial f_t(w_t)$ is a sub-gradient of $f_t$ at $w_t$.

Then, for a sequence of convex loss functions $f_1, \ldots, f_T$ and vectors $w_1, \ldots, w_T$ and if for all $t$, $z_t \in \partial f_t(w_t)$,

\begin{equation}
    \sum_{t=1}^T (f_t(w_t) - f_t(u)) \leq \sum_{t=1}^T (\langle w_t, z_t\rangle - \langle u, z_t \rangle)
\end{equation}

This implies, the regret of an algorithm for Online Convex Optimization is upper bounded by the regret with respect to the linearization of the 
sequence of convex functions.

% Add details about theorem 2.4 and discuss about the requirement of the norm of z_t to be bounded by L, and how it relates to the lipschitzness of the loss function

Beyond Euclidean regularization, FoReL can also be run with other regularization functions and yield similar regret bounds given that the regularization functions are 
strongly convex.

\begin{definition}
    For any $\sigma$-strongly-convex function $f: S \mapsto \mathbb{R}$ with respect to a norm $\|.\|$, for any $w \in S$,
    \begin{equation}
        \forall z \in \partial f(w), \forall u \in S, f(u) \geq f(w) + \langle z, u - w\rangle + \frac{\sigma}{2}\| u - w \|^2.
    \end{equation}
\end{definition}


% Lemma 2.3 to be shifted above

\begin{lemma}\label{lem:forelrb}
    For a FoReL algorithm producing a sequence of vectors $w_1, \ldots, w_T$ with a sequence of loss functions $f_1, \ldots, f_T$, for all $u \in S$, 
    $$\sum_{t=1}^T (f_t(w_t) - f_t(u)) \leq R(u) - R(w_1) + \sum_{t=1}^T (f_t(w_t) - f_t(w_{t+1}))$$
\end{lemma}

\subsection{FoReL with Strongly Convex Regularizers}
From Lemma \ref{lem:forelrb}, the regret bound is given by,

$$\sum_{t=1}^T (f_t(w_t) - f_t(u)) \leq R(u) - R(w_1) + \sum_{t=1}^T (f_t(w_t) - f_t(w_{t+1}))$$

If $f_t$ is $L$-Lipschitz with respect to some norm $\|.\|$ then,

$$f_t(w_t) - f_t(u) \leq L \| w_t - w_{t+1} \|$$

If $\| w_t - w_{t+1} \|$ is small that leads to a better regret bound. It can be shown that if the regularization function $R(w)$ is strongly convex with 
respect to the same norm $\|.\|$ then $\|w_t - w_{t+1}\|$ is also bounded.

For a sequence of predictions $w_1, w_2, \ldots$ of the FoReL algorithm, with a regularizer $R: S \mapsto \mathbb{R}$,

$$f_t(w_t) - f_t(w_{t+1}) \leq L_t \|w-t - w_{t+1} \| \leq \frac{L_t^2}{\sigma}.$$

if $f_t$ is $L$-Lipschitz with respect to $\|.\|$ and $R$ is $\sigma$-strongly-convex.

\begin{theorem}\label{thm:forelregret}
    FoReL run on a sequence of convex functions $f_1, \ldots, f_T$ such that $f_t$ is $L_t$-Lipschitz, with a $\sigma$-strongly-convex regularization function 
    has a regret bound given by, 

    $$Regret_T(u) \leq R(u) - min_{v \in S} R(v) + \frac{TL^2}{\sigma}$$

    where $\frac{1}{T} \sum_{t=1}^T L_t^2 \leq L^2$.
\end{theorem}

\textcolor{red}{To add: derived regret bounds for euclidean and entropic regularizers}

\section{Online Mirror Descent}
a


% 

\begin{comment}
Mirror Descent:

Mirror descent with entropy regularization

Mirror descent with KL Divergence regularizations 


Mirror Descent Policy optimization (MDPO)

The update rule for on-policy MDPO is given by,

$\theta_{k+1} \leftarrow argmax_{\theta \in \Theta \psi(\theta, \theta_k)}$

$\psi(\theta, \theta_k) = \mathds{E}_{s ~ \rho_{\theta_k}}[\mathds{E}_{a~\pi_{\theta}}[A^{\theta_k}(S, a)] - \frac{1}{t_k} \textrm{KL}(s; \pi_{\theta}, \pi_{\theta_k})]$

\end{comment}

%\end{document}