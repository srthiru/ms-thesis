\chapter{Tabular Experiments}

We now evaluate our proposed methods experimentally in both tabular and function approximation
settings as approximate equilibrium solvers.
Through the experiments, we aim to answer the following questions:
\begin{enumerate}
	\item {What is
	      the last-iterate and average-iterate convergence behavior of these algorithms?
	      }\label{qn1}
	\item {How does the addition of NeuRD-fix, Extragradient updates, and Optimistic upates
	      affect the convergence rate of these algorithms (alone and in combination)?}\label{qn2}
	\item {Do these performance improvements scale well with the size of the game?}\label{qn3}
\end{enumerate}

MMD has theoretical converegence guarantees only as a QRE solver, but shows a strong empirical
performance for finding approximate Nash Equilibriums.
In this work our main focus is convergence to the Nash Equilibrium, and as such we focus our main
experimental results for the same.
We provide some additional results for the performance of different algorithms as QRE solvers in
the appendix.

\section{Experimental Domains}
We evaluate the algorithms on two normal form games namely, Perturbed RPS and Matching Pennies.

% Is this duplicate with part 2?
(\revdone{Not sure what goes here.
	This seems likely to at least in part duplicate 2 as laid out, so be careful about that})

% \begin{noindent}
\begin{table}
	\begin{subtable}[c]{0.5\textwidth}
	\centering
		\begin{tabular}{c|c c} 
			\text{ } & H & T \\ 
			\hline 
			H & 1 & -1\\ 
			T & -1 & 1 \\ 
		\end{tabular} 
		\subcaption{Matching Pennies Payoffs.}
	\end{subtable}
	\begin{subtable}[c]{0.5\textwidth}
		\centering
			\begin{tabular}{c|c c c} 
				\text{ } & R & P & S \\ 
				\hline 
				R & 0 & -v & v \\ 
				P & v & 0 & -1 \\ 
				S & -v & 1 & 0 \\
			\end{tabular} 
			\subcaption{Perturbed RPS Payoffs.}
	\end{subtable}
	\caption{Tabular NFG Payoffs}
	\label{tab:payoffs}
\end{table}
% \end{noindent}

\textbf{Matching Pennies} Matching
Pennies is a classic example from Game Theory that demonstrates decision making in multiagent
settings in the simplest form.
In matching pennies, two players toss coins independently and the row player wins if they both land
on the same side, the column player wins otherwise.
The payoffs for Matching Pennies are shown in~\ref{tab:payoffs}.

\textbf{Perturbed RPS}
Perturbed/Biased RPS is a modified version of the canonical normal form game Rock-Paper-Scissors
with biased payoffs for each action.
\ref{tab:payoffs} shows the payoff matrix for Perturbed RPS.

Being symmetric matrix games, both games have a unique Nash Equilibrium: Perturbed RPS
$(\frac{1}{7}, \frac{3}{7}, \frac{3}{7})$, Matching Pennies $(\frac{1}{2}, \frac{1}{2})$.

\section{Evaluation Metrics}
Our main focus being the convergence behaviors and rates, we need a notion of distance from the
equilibrium point to measure the performance of these algorithms.

\subsection{Divergence to the equilibrium}
In settings with a known unique equilibrium, we can compute the distance of the current policy to
the known equilibrium using a measure of distance in the policy space such as the KL-Divergence.
Given the policy at iteration $t$, $pi_t$, and an equilibrium policy $\pi_*$, the metric we measure
is: $KL(\pi_t || \pi_*) = \sum_{a \in A} \pi_t(a) \log \left( \frac{\pi_t(a)}{\pi_*(a)} \right)$.

\subsection{Exploitability}
In general, there might not be a unique Nash equilibrium and we might not know which equilibrium
point the current policy is converging towards.
This makes it tricky to use a direct measure of distance as the above metric.
Exploitability is another metric that is commonly used as a notion of optimality in game theory.
Exploitability measures the gain in value a player can achieve by deviating from the current
policy.
We measure the value that a worst-case opponent can achieve by keeping the current policy fixed by
computing a best response at every state.
The difference between the value that this best-response opponent can acheive and the game value is
the exploitability of the current policy.

\fillin{ - Exploitability formal expression in terms of best responses, and value.}

\section{Experiment Setup}
 (\revdone{something wrong; this sentence doesn't parse.
  Has enough facts in it that it is probably better to split into multiple sentences.
 })
 (\revdone{Explain how this was chosen as you did for other parameters below.})
In the tabular experiments we use softmax-parameterized tabular policies and assume an all-action setting (full-feedback updates).
For all the algorithms, we train both the players for 5000 training steps with alternating updates
using the exact payoff vectors.
For the EG variants, we train the players for only 2500 steps as they use two gradient computation
per step for a fair comparison.
We use the $m=10$ gradient steps per iteration with a learning rate of 0.1 for all the runs.
For MMD and MDPO, we anneal the temperature (entropy coefficient) with the schedule $\alpha_t =
	1/\sqrt{t}$.
For the KL-coefficient, we use different schedules for MMD ($\eta_t = \max(1 / \sqrt{t}, 0.2)$),
and MDPO ($\eta_t = \max(1 - t/T, 0.2)$).
MDPO's KL schedule is motivated by mirror-descent theory, while the entropy annealing and MMD's KL
schedule are closer to the annealing schedule used in the original MMD experiments found through a
hyperparameter sweep.
For both of these methods we cap the KL-coefficient at 0.2 as very low values destabilize the
updates, especially for the NeuRD version of the algorithms.

\section{Results}
 (\revdone{iankash: Currently the writing here is very choppy.
	 Even when presenting results like this you should be telling the reader a story, not just spitting
	 a list of facts at them.
 })

 (\revdone{iankash: Keep this as just part of 5.3})

 (\revdone{iankash: Relatedly, one plot with 21 lines in it is illegible (at least in places).
	 Can you break this giant plot into multiple plots from the same data that tell each part of the
	 story in an easier to digest way?
 })

 (\revdone{iankash: Seems like we should also have plots for the average iterate?})
 (\revdone{iankash: Figures are missing x-axis label; Explain EG correction in them})

We plot the last-iterate convergence in terms of both evaluation metrics in~\ref{fig:tabne_last} as
a function of the iterations.
We also plot the average-iterate convergence for all the methods in~\ref{fig:tabne_avg}.
As mentioned in the experiment setup, the experiment was run for only 2500 steps and one iteration
counts as 2.
``No mod.'' indicates the base version of these algorithms without any modifications that serves as a baseline
for comparison.
% From these plots, we observe that the modifications made on top of these methods induce
% last-iterate convergence in some cases, and speed up convergence in others.

\begin{figure}[h] \centering \hspace*{-2cm} \scalebox{0.6}[0.6]{\input{figs/tabular/nash_last.pgf}}
	\tiny{Note: One iteration counts as 2 for all EG variants.}
	\caption{Last-iterate converegence in PerturbedRPS.
	}\label{fig:tabne_last}
\end{figure}

\begin{figure}
	\centering
	\hspace*{-2cm}
	\scalebox{0.6}[0.6]{\input{figs/tabular/nash_avg.pgf}}
	\tiny{Note: One iteration counts as 2 for all EG variants.}
	\caption{Average-iterate converegence in PerturbedRPS.}\label{fig:tabne_avg}
\end{figure}

There are many observations that can be made from the performances of these algorithms in the
presence of these modifications.
We group these observations by the modifications, and contrast their effects on SPG, MDPO, and MMD.

\textbf{NeuRD Fix:}
SPG, and MDPO do not have last-iterate or average-iterate convergence guarantees, and the same is
observed experimentally for their \textit{No mod.
} baselines.
The NeuRD variants of SPG, and MDPO display average-iterate convergence~\ref{fig:tabne_avg}, as
guaranteed by its no-regret properties.
However, as expected the addition of NeuRD fix does not induce last-iterate convergence for SPG,
and MDPO.
For MMD, which already has average iterate convergence guarantees, it speeds up the convergence
speed.
In fact, NeuRD-fix is the only modification that has a significant impact in terms of convergence
for MMD.

\textbf{EG, and Optimism:}
The baseline version of MMD has last-iterate convergence as guaranteed by its theory.
As seen in~\ref{fig:tabne_last}, EG, and Optimistic updates either induces last-iterate convergence
or speeds it up in most cases.
Interestingly, there are a few algorithm specific differences in terms of the improvement gained
through these modifications.
EG updates induce last-iterate convergence in SPG, and MDPO.
On the contrary, the addition of EG updates slows down MMD.
While Optimistic updates work in SPG, they do not help with convergence for MDPO.

\textbf{Combining the modifications:}
We also experiment with combination of these modifications to see if this leads to a superior
performance.
Combining EG, and Optimism leads to marginal or no improvement, as they both perform a similar
modification of extrapolating the gradients.
However, we can see a significant improvement in convergence speeds when EG, and Optimistic updates
are combined with the NeuRD fix.
Hence, contrary to trust-region constraints, the NeuRD-fix provides an improvement that is
orthogonal to EG/Optimistic Updates that also help in reducing the cycling around the equilibrium.
For both SPG, and MDPO, `NeuRD + EG + Optimism' provides the fastest last-iterate convergence.
Whereas, for MMD, `NeuRD', `NeuRD + Optimism' are the fastest variants.

\textbf{Average vs Last Iterate:}
A primary motivation for our study is achieving last-iterate convergence, but it is also important
to discuss the behavior of average iterate convergence and how it compares to the last-iterate
convergence.
For instance, it was shown that the last-iterate convergence is slower than the average-iterate for
the EG method for smooth convex-concave saddle point problems in~\cite{golowichLast2020} by proving
a tight lower bound for the last-iterate.
% However, our problem setting in this section is two-player zero-sum normal form games which are
% bilinear saddle-point problems.
We note that from our tabular experimental results, the last-iterate convergence is better than the
average iterate for all the algorithms.

The tabular experiments, and the above observations provide some answers to questions~\ref{qn1},
and~\ref{qn2}.

\section{Algorithm Design Choices}

\subsection{NeuRD-fix}
The NeuRD-loss has also been previously applied on top of algorithms to improve performance or
induce convergence in competitive and cooperative settings.
Perolat et.~al,~\cite{perolatMastering2022} introduced Regularized Nash Dynamics (R-NAD) that
utilizes NeuRD as a fixed-point approximator along with adaptive
regularization~\cite{perolatPoincare2021} to achieve last-iterate convergence in the game of
Stratego.
Chhablani et.~al,~\cite{chhablaniCounterfactual2021} motivated the choice of an advantage baseline
in COMA~\cite{foersterCounterfactual2018} through no-regret literature and demonstrated that
applying the NeuRD loss to COMA's objective improved its performance even in cooperative settings
such as identical-interest games.
The improved performance of adapting the NeuRD-fix with mirror-descent based methods, and other
techniques for last-iterate convergence is also evident from our experimental evaluations.
Through this, we reinforce the idea that the NeuRD-loss can be more generally adapted into loss
functions of various algorithms in multi-agent settings.
\fillin{is there a benefit to adding NeuRD in single-agent setting? even if the reward function is not dynamic.}

\subsection{Entropy Regularization}
As noted in the tabular experiments, in some problem settings there might be faster methods to
induce last-iterate convergence that adaptive regularization.
However, entropy regularization is more widely studied within single, and multi-agent RL.
Adaptive entropy regularization is also relatively easier to implement, and less constraining in
terms of how each player updates their policy as compared to modifications like EG, or Optimism.
In single-agent RL, entropy regularization is motivated as a means to encourage exploration, and
make the policies more robust.
In multi-agent RL, it has been studied more specifically to induce convergence even including in
NeuRD, and MMD.
Given the universal inclusion, and multiple perceived benefits, entropy regularization is
considered a standard design choice to include in most algorithms.

\subsection{Trust-region constraints}
Trust-region constraints form the basis of state-of-the-art RL algorithms like PPO.
The presence of a relatively strongly-convex proximal operator is necessary for deriving algorithms
with strong performance guarantees.

\subsection{EG and Optimism}
Finally, we highlight that the best performing variants are on top of SPG as opposed to MDPO, or
MMD as can be seen from~\ref{fig:tabne_last}.
This is surprising as the latter methods were proposed as improvements over SPG in single, and
multi-agent settings.
But, this indicates that EG/Optimistic updates provide a better last-iterate convergence rates
compared to adaptive entropy regularization with trust-region constraints.
While the convergence rates of EG, and Optimistic updates are well-studied, it is not the case for
adaptive regularization.
Most notably, Optimistic-NeuRD (SPG + NeuRD + Optimism) exhibits a faster last-iterate convergence
that is straightforward to extend to more complicated settings.

Based on the above findings, and discussion, we cast the various algorithms discussed as extensions
of policy gradient methods with different added components.
In the next section we perform an empirical study of these extensions under function approximation.
