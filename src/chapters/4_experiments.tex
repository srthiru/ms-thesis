\chapter{Experiments}

We now evaluate our proposed methods experimentally in both tabular and function approximation settings.
Though these algorithms can be applied both as approximate equilibrium solvers QRE and Nash equilibirum.
We present some results for convergence to QRE in the tabular setting but focus on Nash convergence mainly 
for the tabular and function approximation setting.

Through the experiments, we aim to answer the following questions:
\begin{itemize}
	\item How does the addition of NeuRD-fix, Extragradient updates, and Optimistic upates
	      affect the convergence rate of these algorithms in solving for QREs, and Nash equilibrium?
	\item What is the last-iterate vs average-iterate convergence behavior of these algorithms in the presence of these modifications?
	\item Do these performance improvements scale well with the size of the game?
\end{itemize}

\section{Evaluation Metrics}
Our main focus being the convergence behaviors and speed of convergence we need a notion of 
distance from the equilibrium point to measure the performance of these algorithms.

\subsection{Divergence to the equilibrium}
In settings with a known unique equilibrium such as for QREs, or Nash equilibrium in symmetric markov games, 
we can compute the distance of the current policy to the known equilibrium using a measure of distance in the policy space such as the KL-Divergence.

\subsection{Exploitability}
In general, there might not be a unique Nash equilibrium. 
In such cases, another notion of distance from the equilibrium is needed to measure convergence.
Exploitability measures the gain in value by deviating from the current strategy.
This is measured by computing the utility of a best response agent against the current policy, as an indiciation 
of incentive to deviate from the current policy.

\blue{ - Exploitability formal expression in terms of best responses, and value.}

\section{Tabular Experiments}
For the tabular domain, we study the learning behavior of these algorithms on
PerturbedRPS and Matching Pennies. 
PerturbedRPS, a modified version of the standard Rock Paper Scissors, is a symmetric Normal form game 
that has a unique Nash Equlibrium.
Although it is a simple problem, standard RL algorithms fail to converge due to the non-stationarity 
induced by the opponent.
We employ the standard model of alternating updates to learn a parameterized policy for both agents. 
We use softmax parameterization, and perform updates to learn the logits associated with each action. 
The logits then act as action preferences which are sampled from using the softmax function to get a stochastic policy.

For PerturbedRPS, we use the QRE solutions computed through Gambit and, we also derive the unique
Nash Equilibrium (please refer to~\ref{sec:rpsne} in the appendix for the derivation) for
PerturbedRPS.\

\ref{tab:tabres} summarizes the last and average iterate convergences behaviors of 3 different algorithms -
MMD, MDPO, SPG under the proposed modifications.
We present the results for NE, and QRE convergence (with 0.5 temperature). Fig

% \begin{noindent}

\begin{table}[htbp]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
	\hline
	\multirow{3}{*}{\textbf{Algorithm}} &
	\multicolumn{4}{c|}{\textbf{Nash}} &
	\multicolumn{2}{c|}{\textbf{QRE ($\alpha$=0.5)}} \\
	\cline{2-7}
	& 
	\multicolumn{2}{c|}{\textbf{MMD}} & 
	\multicolumn{2}{c|}{\textbf{MDPO}} &
	\multicolumn{2}{c|}{\textbf{MMD}} \\
	\cline{2-7}
	& \textbf{Avg} & \textbf{Last} & \textbf{Avg} & \textbf{Last}
	& \textbf{Avg} & \textbf{Last} \\
	\hline
	Base	 	& y & y & x & y 		& y & y \\
	\hline
	NeuRD 		& y & y & y & \red{x} 	& y & y \\
	\hline
	EG 			& y & y & y & y 	& y & y \\
	\hline
	OPT 		& y & y & y & n 	& y & y \\
	\hline
	EG-OPT 		& y & y & y & y 	& y & y \\
	\hline
	EG-N 		& y & y & y & y 	& y & y \\
	\hline
	OPT-N 		& y & y & y & n 	& y & y \\
	\hline
	EG-OPT-N	& y & y & y & y 	& y & y \\
	\hline
	\end{tabular}
	\caption{QRE and Nash convergence in Perturbed RPS for last, and average iterates.}
	\label{tab:tabres}
\end{table}
% \end{noindent}

\begin{figure}[H]
	% \subfigure{\includegraphics[width=0.5\linewidth]{plots/MDPO_NE.png}}
	% \subfigure{\input{plots/tikz/nash.pgf}}
\end{figure}


\begin{figure}[H]
	\includegraphics[width=15cm]{plots/MMD_NE.png}
\end{figure}

\begin{itemize}
	\item{MDPO-EG has superlinear last iterate convergence to Nash
	            equilibrium.
	      }
	\item{MMD with NeuRD fix and extragradient }
\end{itemize}

From the above experimental results, we note the following:

\begin{itemize}
	\item {MDPO with extragradient updates has superlinear convergence to Nash equilibrium}
	\item {Extragradient updates, and the NeuRD fix speedup convergence in both MMD and MDPO for convergence 
	to both Nash equilibrium and QREs.}
\end{itemize}

Based on the above findings, we test some of these variants in the function approximation setting for 
two-player zero-sum games with a large state space.

