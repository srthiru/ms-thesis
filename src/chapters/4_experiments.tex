\chapter{Tabular Experiments}

We now evaluate our proposed methods experimentally in both tabular and function approximation
settings as approximate equilibrium solvers.
Through the experiments, we aim to answer the following questions:
\begin{enumerate}
	\item {What is
	      the last-iterate and average-iterate convergence behavior of these algorithms?
	      }\label{qn1}
	\item {How does the addition of NeuRD-fix, Extragradient updates, and Optimistic upates
	      affect the convergence rate of these algorithms (alone and in combination)?}\label{qn2}
	\item {Do these performance improvements scale well with the size of the game?}\label{qn3}
\end{enumerate}

MMD has theoretical converegence guarantees only as a QRE solver, but shows a strong empirical
performance for finding approximate Nash Equilibriums.
In this work our main focus is convergence to the Nash Equilibrium, and as such we focus our main
experimental results for the same.
We provide some additional results for the performance of different algorithms as QRE solvers in
the appendix.

\section{Experimental Domains}
We evaluate the algorithms on two normal form games namely, Perturbed RPS and Matching Pennies.

% \begin{noindent}
\begin{table}
	\begin{subtable}[c]{0.5\textwidth}
	\centering
		\begin{tabular}{c|c c} 
			\text{ } & H & T \\ 
			\hline 
			H & 1 & -1\\ 
			T & -1 & 1 \\ 
		\end{tabular} 
		\subcaption{Matching Pennies Payoffs.}
	\end{subtable}
	\begin{subtable}[c]{0.5\textwidth}
		\centering
			\begin{tabular}{c|c c c} 
				\text{ } & R & P & S \\ 
				\hline 
				R & 0 & -v & v \\ 
				P & v & 0 & -1 \\ 
				S & -v & 1 & 0 \\
			\end{tabular} 
			\subcaption{Perturbed RPS Payoffs.}
	\end{subtable}
	\caption{Tabular NFG Payoffs}
	\label{tab:payoffs}
\end{table}
% \end{noindent}

\textbf{Matching Pennies}
Matching Pennies is a classic example from Game Theory that demonstrates decision making in
multiagent settings in the simplest form.
In matching pennies, two players toss coins independently and the row player wins if they both land
on the same side, the column player wins otherwise.
The payoffs for Matching Pennies are shown in~\ref{tab:payoffs}.

\textbf{Perturbed RPS}
Perturbed/Biased RPS is a modified version of the canonical normal form game Rock-Paper-Scissors
with biased payoffs for each action.
\ref{tab:payoffs} shows the payoff matrix for Perturbed RPS.

Being symmetric matrix games, both games have a unique Nash Equilibrium: Perturbed RPS
$(\frac{1}{7}, \frac{3}{7}, \frac{3}{7})$, Matching Pennies $(\frac{1}{2}, \frac{1}{2})$.

\section{Evaluation Metrics}
Our main focus being the convergence behaviors and rates, we need a notion of distance from the
equilibrium point to measure the performance of these algorithms.

\subsection{Divergence to the equilibrium}
In settings with a known unique equilibrium, we can compute the distance of the current policy to
the known equilibrium using a measure of distance in the policy space such as the KL-Divergence.
Given the policy at iteration $t$, $pi_t$, and an equilibrium policy $\pi_*$, the metric we measure
is: $KL(\pi_t || \pi_*) = \sum_{a \in A} \pi_t(a) \log \left( \frac{\pi_t(a)}{\pi_*(a)} \right)$.

\subsection{Exploitability}
In general, there might not be a unique Nash equilibrium and we might not know which equilibrium
point the current policy is converging towards.
This makes it tricky to use a direct measure of distance as the above metric.
Exploitability is another metric that is commonly used as a notion of optimality in game theory.
Exploitability measures the gain in value a player can achieve by deviating from the current
policy.
We measure the value that a worst-case opponent can achieve by keeping the current policy fixed by
computing a best response at every state.
The difference between the value that this best-response opponent can acheive and the game value is
the exploitability of the current policy.

\note{Added a formal expression of exploitability in terms of best responses, and value.}

\section{Experiment Setup}
For experiments in this section, we use softmax-parameterized tabular policies and assume an
all-action setting (full-feedback updates).
For all the algorithms, we train both the players for 5000 training steps with alternating updates
using the exact payoff vectors.
For the EG variants, we train the players for only 2500 steps as they use two gradient computation
per step for a fair comparison.
We use the $m=10$ gradient steps per iteration with a learning rate of 0.1 for all the runs.
For MMD and MDPO, we anneal the temperature (entropy coefficient) with the schedule $\alpha_t =
	1/\sqrt{t}$.
For the KL-coefficient, we use different schedules for MMD ($\eta_t = \max(1 / \sqrt{t}, 0.2)$),
and MDPO ($\eta_t = \max(1 - t/T, 0.2)$).
MDPO's KL schedule is motivated by mirror-descent theory, while the entropy annealing and MMD's KL
schedule are closer to the annealing schedule used in the original MMD experiments found through a
hyperparameter sweep.
For both of these methods we cap the KL-coefficient at 0.2 as very low values destabilize the
updates, especially for the NeuRD version of the algorithms.

\section{Results}
We plot the last-iterate convergence in terms of both evaluation metrics in~\ref{fig:tabne_last} as
a function of the iterations.
We also plot the average-iterate convergence for all the methods in~\ref{fig:tabne_avg}.
As mentioned in the experiment setup, the experiment was run for only 2500 steps and one iteration
counts as 2.
``No mod.'' indicates the base version of these algorithms without any modifications that serves as a baseline
for comparison.
% From these plots, we observe that the modifications made on top of these methods induce
% last-iterate convergence in some cases, and speed up convergence in others.

\begin{figure}[h] \centering \hspace*{-2cm} \scalebox{0.6}[0.6]{\input{figs/tabular/nash_last.pgf}}
	\tiny{Note: One iteration counts as 2 for all EG variants.}
	\caption{Last-iterate converegence in PerturbedRPS.
	}\label{fig:tabne_last}
\end{figure}

\begin{figure}
	\centering
	\hspace*{-2cm}
	\scalebox{0.6}[0.6]{\input{figs/tabular/nash_avg.pgf}}
	\tiny{Note: One iteration counts as 2 for all EG variants.}
	\caption{Average-iterate converegence in PerturbedRPS.}\label{fig:tabne_avg}
\end{figure}

There are many observations that can be made from the performances of these algorithms in the
presence of these modifications.
We group these observations by the modifications, and contrast their effects on SPG, MDPO, and MMD.

\textbf{NeuRD Fix:}
SPG, and MDPO do not have last-iterate or average-iterate convergence guarantees, and the same is
observed experimentally for their \textit{No mod.
} baselines.
The NeuRD variants of SPG, and MDPO display average-iterate convergence~\ref{fig:tabne_avg}, as
guaranteed by its no-regret properties.
However, as expected the addition of NeuRD fix does not induce last-iterate convergence for SPG,
and MDPO.
For MMD, which already has average iterate convergence guarantees, it speeds up the convergence
speed.
In fact, NeuRD-fix is the only modification that has a significant impact in terms of convergence
for MMD.

\textbf{EG, and Optimism:}
The baseline version of MMD has last-iterate convergence as guaranteed by its theory.
As seen in~\ref{fig:tabne_last}, EG, and Optimistic updates either induces last-iterate convergence
or speeds it up in most cases.
Interestingly, there are a few algorithm specific differences in terms of the improvement gained
through these modifications.
EG updates induce last-iterate convergence in SPG, and MDPO.
On the contrary, the addition of EG updates slows down MMD.
While Optimistic updates work in SPG, they do not help with convergence for MDPO.

\textbf{Combining the modifications:}
We also experiment with combination of these modifications to see if this leads to a superior
performance.
Combining EG, and Optimism leads to marginal or no improvement, as they both perform a similar
modification of extrapolating the gradients.
However, we can see a significant improvement in convergence speeds when EG, and Optimistic updates
are combined with the NeuRD fix.
Hence, contrary to trust-region constraints, the NeuRD-fix provides an improvement that is
orthogonal to EG/Optimistic Updates that also help in reducing the cycling around the equilibrium.
For both SPG, and MDPO, `NeuRD + EG + Optimism' provides the fastest last-iterate convergence.
Whereas, for MMD, `NeuRD', `NeuRD + Optimism' are the fastest variants.

\textbf{Average vs Last Iterate:}
A primary motivation for our study is achieving last-iterate convergence, but it is also important
to discuss the behavior of average iterate convergence and how it compares to the last-iterate
convergence.
For instance, the last-iterate convergence for the EG method is slower than the average-iterate
when applied to smooth convex-concave saddle point problems in~\cite{golowichLast2020}.
In contrast, from~\ref{fig:tabne_last} and~\ref{fig:tabne_avg} we note that the last-iterate
convergence is better than the average iterate for all variations of the algorithms discussed in
this work, for the problem setting of two-player zero-sum normal form games.

The tabular experiments, and the above observations provide some answers to questions~\ref{qn1},
and~\ref{qn2}.
<<<<<<< Updated upstream
Based on the above discussion on the experimental results, we cast the various algorithms discussed
as extensions of policy gradient methods with different added components.
These components are namely
\begin{itemize}
	\item NeuRD fix
	\item Entropy regularization
	\item KL
	      Penalty / Trust-region constraints
    	\item Extragradient and Optimistic Updates
\end{itemize}

Both MDPO, and MMD can be viewed as variants of PG methods with the added
components of an annealed KL-Divergence penalty, and adaptive entropy regularization.
Hence we dub variants of PG as MPG (Modified Policy Gradients), and study variations of this
algorithm with the addition of the above components (by themselves, and in combination) in
large-scale benchmark games under function approximation to evaluate their applicability, and
scalability in this challenging setting.
=======
In the next section, we implement the methods as deep RL algorithms and evaluate them on
extensive-form games.
>>>>>>> Stashed changes
