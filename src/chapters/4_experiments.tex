\chapter{Tabular Experiments}

We now evaluate our proposed methods experimentally in both tabular and function approximation
settings as approximate equilibrium solvers.
Through the experiments, we aim to answer the following questions:
\begin{enumerate}
	\item {What is
	      the last-iterate and average-iterate convergence behavior of these algorithms?
	      }\label{qn1}
	\item {How does the addition of NeuRD-fix, Extragradient updates, and Optimistic upates
	      affect the convergence rate of these algorithms (alone and in combination)?}\label{qn2}
	\item {Do these performance improvements scale well with the size of the game?}\label{qn3}
\end{enumerate}

MMD has theoretical converegence guarantees only as a QRE solver, but shows a strong empirical
performance for finding approximate Nash Equilibriums.
In this work our main focus is convergence to the Nash Equilibrium, and as such we focus our main
experimental results for the same.
We provide some additional results for the performance of different algorithms as QRE solvers in
the appendix.

\section{Experimental Domains}
We evaluate the algorithms on two normal form games namely, Perturbed RPS and Matching Pennies.

\textbf{Perturbed RPS}

\textbf{Matching Pennies}

\section{Evaluation
  Metrics}
Our main focus being the convergence behaviors and rates, we need a notion of distance from the
equilibrium point to measure the performance of these algorithms.

\subsection{Divergence to the equilibrium}
In settings with a known unique equilibrium, we can compute the distance of the current policy to
the known equilibrium using a measure of distance in the policy space such as the KL-Divergence.
Given the policy at iteration $t$, $pi_t$, and an equilibrium policy $\pi_*$, the metric we measure
is: $KL(\pi_t || \pi_*) = \sum_{a \in A} \pi_t(a) \log \left( \frac{\pi_t(a)}{\pi_*(a)} \right)$.

\subsection{Exploitability}
In general, there might not be a unique Nash equilibrium and we might not know which equilibrium
point the current policy is converging towards.
This makes it tricky to use a direct measure of distance as the above metric.
Exploitability is another metric that is commonly used as a notion of optimality in game theory.
Exploitability measures the gain in value a player can achieve by deviating from the current
policy.
We measure the value that a worst-case opponent can achieve by keeping the current policy fixed by
computing a best response at every state.
The difference between the value that this best-response opponent can acheive and the game value is
the exploitability of the current policy.

\blue{ - Exploitability formal expression in terms of best responses, and value.}

\subsection{Experiment Setup}
The policies that are logit-parameterized with a softmax projection and all the algorithms use
full-feedback gradient updates as discussed in Chapter 3.
Being symmetric matrix games, both games have a unique Nash Equilibrium: Perturbed RPS
$(\frac{1}{7}, \frac{3}{7}, \frac{3}{7})$, Matching Pennies $(\frac{1}{2}, \frac{1}{2})$.
For all the algorithms, we train both the players for 5000 training steps with alternating updates
using the exact payoff vectors.
For the EG variants, we train the players for only 2500 steps as they use two gradient computation
per step for a fair comparison.
We use the $m=10$ gradient steps per iteration with a learning rate of 0.1 for all the runs.
For MMD and MDPO, we anneal the temperature (entropy coefficient) with the schedule $\alpha_t =
	1/\sqrt{t}$.
For the KL-coefficient, we use different schedules for MMD ($\eta_t = \max(1 / \sqrt{t}, 0.2)$),
and MDPO ($\eta_t = \max(1 - t/T, 0.2)$).
MDPO's schedule is motivated by mirror-descent theory, while MMD's schedule is closer to the type
of annealing schedule used for the original MMD experiments found through a hyperparameter sweep.
For both of these methods we cap the KL-coefficient at 0.2 because very low values destabilize
the updates, especially for the NeuRD version of the algorithms.

\section{Results} \ref{tab:tabres} summarizes the last and average
iterate convergences behaviors of 3 different algorithms - MMD, MDPO, SPG under the proposed
modifications.
% We present the results for NE, and QRE convergence (with 0.5 temperature).
Fig~\ref{fig:tabne} summarizes the results for convergence to the Nash Equilibrium.

% \begin{noindent}

\begin{table}[htbp]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
	\hline
	\multirow{2}{*}{\textbf{Algorithm}} &
	% \multicolumn{4}{c|}{\textbf{Nash}} &
	% \multicolumn{2}{c|}{\textbf{QRE ($\alpha$=0.5)}} \\
	% \cline{2-7}
	% & 
	\multicolumn{2}{c|}{\textbf{PG}} & 
	\multicolumn{2}{c|}{\textbf{MDPO}} &
	\multicolumn{2}{c|}{\textbf{MMD}} \\
	\cline{2-7}
	& \textbf{Avg} & \textbf{Last} & \textbf{Avg} & \textbf{Last}
	& \textbf{Avg} & \textbf{Last} \\
	\hline
	Base	 	& \red{\texttimes} 	& \red{\texttimes} 	& \red{\texttimes} 	& \red{\texttimes}		& \checked 	& \checked \\ \hline
	NeuRD 		& \checked 			& \red{\texttimes} 	& \checked 			& \red{\texttimes}		& \checked 	& \checked \\ \hline
	EG 			& \checked 			& \checked 			& \checked 			& \checked 				& \checked 	& \checked \\ \hline
	OPT 		& \checked 			& \checked 			& \checked 			& \checked 				& \checked 	& \checked \\ \hline
	EG-OPT 		& \checked 			& \checked 			& \checked 			& \checked 				& \checked 	& \checked \\ \hline
	EG-N 		& \checked 			& \checked 			& \checked 			& \checked 				& \checked 	& \checked \\ \hline
	OPT-N 		& \checked 			& \checked 			& \checked 			& \checked 				& \checked 	& \checked \\ \hline
	EG-OPT-N	& \checked 			& \checked 			& \checked 			& \checked 				& \checked 	& \checked \\ \hline
	\end{tabular}
	\caption{Convergence in Perturbed RPS for last, and average iterates.}
	\label{tab:tabres}
\end{table}
% \end{noindent}

\begin{figure}[H]
	\centering
	\scalebox{0.7}[0.6]{\input{figs/tabular/nash.pgf}}
	\caption{Last-iterate converegence plots in PerturbedRPS.}
	\label{fig:tabne}
\end{figure}

\subsection{Observations}
From the above experimental results, we note the following:

\begin{itemize}
	\item {NeuRD-fix speeds up convergence in all the algorithms with or without EG and Optimism.
	      }
	\item {Most notably, Optimistic-NeuRD (PG-nrd-opt2) exhibits faster last-iterate convergence.
	      }
	      % \item {EG updates provide the most improvement in terms of convergence speeds.}
	\item {MMD shows no performance improvements with the addition of Extragradient or Optimistic updates.
	      }
	      % \item {Entropy annealing (MMD) shows slower convergence compared to EG, or Optimism as a last-iterate
	      %       inducing mechanism, however it is easier to implement in practice compared to the latter.}
	\item {In the presence of EG/Optimism to enable last-iterate convergence, trust-region constraints and entropy
	      regularization slow-down the learning, as evidenced by the better performacne of SPG variants compared to the MDPO, and MMD variants.}
	\item {Combining Optimism with EG updates provides minimal improvements as they fulfill a similar role.
	      }
\end{itemize}

The tabular experiments, and the above observations provide some answers to questions~\ref{qn1},
and~\ref{qn2}.
