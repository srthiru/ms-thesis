\chapter{Modified Updates for Last-iterate Convergence}
\label{chp:updates}

\old{Our main contribution is adapting existing techniques for inducing convergence in multiagent
	settings with the algorithms discussed in the previous section.}
(\revdone{Too brief.
	Slow down and explain to the reader what the problem is that these modified updates are intended to
	solve.
})
Last-iterate convergence of algorithms is an attractive property that has been studied widely in the context
of optimization, and equilibirium learning algorithms.
While there have been numerous methods proposed for equilibrium solving in games, many of them only
guarantee convergence in the ergodic sense, i.e., only the average of their iterates converge to an
equilibrium.
However, in cases where the strategies are represented using function approximators, this presents
a problem as it becomes complicated to maintain a history of past strategies or in terms of
averaging them.

The main contribution of our work is in studying a few modifications to the algorithms discussed in
the previous section, with the objective of either inducing and speeding up last-iterate
convergence in two-player zero-sum settings.
More specifically, we study the following three techniques, namely - Neural Replicator
Dynamics~\cite{hennesNeural2020}, Extragradient updates~\cite{korpelevichextragradient1976}, and
Optimistic gradient updates~\cite{popovmodification1980}, that have been previously studied in the
context of learning in games, and solving saddle point problems.
In this section we describe these techniques in more detail and provide our proposed modifications
to the algorithms from the previous section.

\section{Neural Replicator Dynamics (NeuRD)}

% The continuous-time single-population replicator dynamics is defined by the following system of
% differential equations:
% \begin{equation}
% 	\label{eqn:rd} \dot{\pi}(a) = \pi(a)[u(a, \pi) -
% 		\bar{u}(\pi)], \forall a \in \mathcal{A}
% \end{equation}

Consider the problem of learning a parameterized policy $\pi_{\theta}$ in a single-state all-action
problem setting.
In such a setting, SPG employs the following update at each iteration
$t$~\cite[Section~A.1]{hennesNeural2020}: \[y_{\theta_t}(a) = y_{\theta_{t-1}} (a) + \eta_t
	\pi_{\theta_{t-1}}(a) [r_t(a) - \bar{r_t}], \forall a \in A\] where $y$ represents the logits,
$r_t(a)$ is the immediate reward associated with action $a$, and $\bar{r}_t$ is the average reward
of the state.

\subsection{Replicator Dynamics and No-regret Learning}
The above SPG update is equivalent to the instant regret scaled by the current policy.
This scaling makes it difficult for SPG to adapt to sudden changes in rewards associated with
actions that are already less likely under the current policy.
This problem is more evident in multiagent settings where the opponent's policy can change
arbitrarily affecting rewards associated with actions even in single-state settings.

Motivated by this observation, Hennes et.
al~\cite{hennesNeural2020} propose to make modifications
to SPG using the idea of Replicator dynamics from Evolutionary game theory.
Replicator Dynamics defiens operators that describe the dynamics of a population's evolution when
attempting to maximize some arbitrary utility function (\note{cite}).
Replicator dynamics also have a strong connection to no-regret algorithms, and
in~\cite[Statement~1]{hennesNeural2020}, the authors establish the equivalence between Hedge, and
discrete time RD.
Due to this equivalence, RD also inherits the property of no-regret algorithms that their
time-average policy converges to the Nash equilibrium (\note{cite}).

\subsection{NeuRD}
Neural Replicator Dynamics (NeuRD) is an adaptation of discrete-time Replicator Dynamics to
reinforcement learning for function approximation settings (\revdone{Include the math, not just the
	english}).
For a parameterized policy $\pi_\theta$, the NeuRD-update rule is given by:

\begin{equation}
	\label{eqn:nrd} \theta_t \leftarrow \theta_{t-1} + \eta_t \sum_{a'}
	\nabla_{\theta} \text{Y}_{\theta_{t-1}}(s, a') A(s, a')
\end{equation}

where
$Y_{\theta_{t-1}}$ represent the logits of the parameterized policy at timestep $t-1$, and $A$
represents the advantage value.

The only difference between the NeuRD update, and the SPG update is that the gradient of the
parameters are computed directly with respect to the logits.
This can be viewed as a modification to SPG making it more adaptive in non-stationary settings.
To prevent numerical issues stemming from accumulating the advantages into the logits resulting in
unstable gradients, a logit gap based thresholding is proposed in~\cite{hennesNeural2020}.
This thresholding operator is: $\nabla_{\theta}(f(\theta), \eta, \beta) \doteq [\eta\nabla_\theta
		f(\theta)] \mathds{I}\{f(\theta + \eta\nabla_{\theta}f(\theta)) \in [-\beta, \beta]\}$, where
$\beta$ is the \textit{NeuRD-threshold} that determines how arbitrarily close to 0 or 1 the
probabilities can get to.
% While NeuRD has average iterate convergence,  also induce last iterate convergence in
% imperfect information settings for NeuRD using reward transformation based regularization~\cite{perolatPoincare2021}.
Since most PG methods, including the ones discussed in the previous chapter utilize softmax
parameterization, the NeuRD fix is easily extendable to the policy loss component of their
respective loss functions without any major changes.
In our work, we also apply the NeuRD fix to the policy loss component of the MMD and MDPO loss
functions (\fillin{rewrite this sentence}).

\subsection{Relation to Natural Policy Gradients}

\subsection{Alternating vs Simultaneous gradient updates}
As an aside, we wish to discuss two possible update schemes for discrete learning dynamics, namely
- Simultaneous and Alternating updates.
We also observe that while alternating updates shows average-iterate convergence, simultaneous
updates do not converge.
\begin{figure}[H]
	\centering
	\scalebox{0.6}[0.6]{\input{figs/tabular/altvsim.pgf}}
	\caption[Simultaneous gradient updates vs Alternate gradient updates updates]{NeuRD converges with alternating updates, but not with simultaneous updates.}
\end{figure}
We note that all the experiments in~\cite{hennesNeural2020} also employed alternating updates to
evaluate NeuRD and SPG.
A similar behavior of simultaneous gradient updates diverging was also observed
in~\cite{gidelVariational2020} for unconstrained min-max problems.
They note that the sequence of iterates under alternating updates have a bounded error, and thus
the average of the iterates converges to the solution.
Based on these observations, we use alternating udpates for all our tabular experiments.
Both of these are symmetric update schemes in the sense that both the players perform equal number
of updates every iteration.
Asymmetric updates have also been studied in a similar context and have been shown to improve the
speed of convergence~\cite{daskalakisTraining2018}.

\section{Extragradient and Optimistic Gradient}
 (\revdone{Since you say EG builds on
	 this (Arrow-Hurwicz) a few lines later, you need to tell the reader at least a bit more about what it is})
 (\revdone{For opt updates: explain what
	 has changed})
 (\revdone{Make sure you have defined the various notions of convergence somewhere, probably in 2 or 3})

While the NeuRD fix modifies the loss function to better adjust to changing dynamics in multiagent
settings, there is no inherent change to the learning procedure as such.
Iterative methods such as fictitous play, are part of a broader set of learning dynamics studied to
identify a simple sequence of iterative updates that each of the player in a multiagent setting can
follow to converge to some solution set like the set of equilibrium points.
It is common for these methods to borrow from first-order optimization
algorithms~\cite{beckFirstOrder2017} that have been studied extensively in varying contexts
including solving variational inequalities, saddle-point problems, and convex optimization.
In particular, saddle-point problems are of much interest due to their application in modeling
various problems including learning in games, generative models, and robust optimization problems.

The classic Arrow-Hurwicz (inexact Uwaza) method~\cite{arrowStudies1958} is a popular
gradient-based for saddle point problems that can be expressed as a sequence of updates:
%\begin{noindent}
\begin{equation}
	\label{eqn:arrowhurwicz}
	\begin{split}
		x_{k+1} = P_{\calX} (x - \nabla f_x(x_k, y_k)) \\
		x_{k+1} = P_{\calY} (y + \nabla f_y(x_{k+1}, y_k)) \\
	\end{split}
\end{equation}
%\end{noindent}

While first-order gradient based iterative methods like
the above work very well for function minimization problems, convergence for saddle point problems
is only possible under strong-convexity assumptions even in the unconstrained
case~\cite{heConvergence2022}.
However, extensions to the above have been proposed and analyzed for solving of variational
inequalities, and saddle point problems, that have better convergence guarantees under restrictive
assumptions.

\textbf{Extragradient Updates (EG):}
The Extragradient method was introduced by G.~M.~Korpelevich~\cite{korpelevichextragradient1976} as
a modification to the Arrow-Hurwicz method for solving convex-concave saddle point problems, and
variational inequality problems with strongly monotone operators.
The EG algorithm follows the sequence of updates as given below: %\begin{noindent}
\begin{equation}
	\label{eqn:eg}
	\begin{split}
		\bar{x}_{k} = P_{\calX}[x_k - \eta f(x_k, y_k)] \\ 
		\bar{y}_{k} = P_{\calY}[y_k + \eta f(x_k, y_k)] \\ 
		x_{k+1} = P_{\calX}[x_k - \eta f(\bar{x}_k, \bar{y}_k)] \\ 
		y_{k+1} = P_{\calY}[y_k + \eta f(\bar{x}_k, \bar{y}_k)]
	\end{split}
\end{equation}
% \end{noindent}

\textbf{Optimistic updates (OPT):} Optimistic updates are another modification
to the Arrow-Hurwicz gradient method proposed by L.~D.~Popov~\cite{popovmodification1980} for
solving convex-concave saddle point problems.
The algorithm follows the below sequence of updates: % \begin{noindent}
\begin{equation}
	\label{eqn:opt}
	\begin{split}
		\bar{x}_{k} = P_{\calX}[x_k - \eta f(\bar{x}_{k-1}, \bar{y}_{k-1})] \\ 
		\bar{y}_{k} = P_{\calY}[y_k + \eta f(\bar{x}_{k-1}, \bar{y}_{k-1})] \\ 
		x_{k+1} = P_{\calX}[x_k - \eta f(\bar{x}_k, \bar{y}_k)] \\
		y_{k+1} = P_{\calY}[y_k + \eta f(\bar{x}_k, \bar{y}_k)]
	\end{split}
\end{equation}
% \end{noindent}

Contrast to the EG method, optimistic updates reuse the gradients of the leading points of the
previous iteration $f(\bar{x}_{k-1}, \bar{y}_{k-1})$ instead of computing an \textit{extragradient}
in each iteration $f(\bar{x_{k}}, \bar{y_{k}})$ as done in~\ref{eqn:eg}.
Optimistic and Extragradient updates are also interpreted as a variants extrapolation methods where
EG uses an additional gradient computation, and Optimistic updates approximates this extrapolation
by reusing previous iteration's gradient.
Optimistic updates were studied in the context of online learning under the name \textit{Optimistic
	Mirror Descent (OMD)}~\cite{rakhlinOptimization2013}.
OMD when applied to saddle-point problems leads to two popularly studied variants - Optimistic
Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Updates (OMWU), that use
euclidean and entropic mirror maps respectively.

Due to their broad applicability, and theoretical grounding, convergence rates for these algorithms
have been studied under various settings.
% If $f$ is $L$-smooth, and $0 < \eta < 1/L$ is the step size, EG has asymptotic last-iterate
% convergence.
For VIs with strongly monotone operators or composite structures, EG has a linear last-iterate
convergence rate~\cite{tsenglinear1995}.
A linear last-iterate convergence for OGDA in bilinear games and an optimistic version of the Adam
optimizer was also given in~\cite{daskalakisTraining2018}.
~\cite{mertikopoulosOptimistic2019} also study the EG method for training GANs by establishing a framework of
\textit{coherence}, and show that EG has last-iterate convergence for coherent VI problems.
A unified analysis of both these algorithms was also presented~\cite{mokhtariUnified2020} studied
both these algorithms in a unified way as an approximation of the Proximal point method, and showed
that both these algorithms have linear last-iterate convergence for the strongly-convex
strongly-convex and bilinear cases.
More recently, these results have been extended to monotone VIs, constrained saddle point problems,
markov games, and extensive form games under various
conditions~\cite{weiLINEAR2021,cenFaster2022,caiTight2022,gorbunovExtragradient2022,gorbunovLastIterate2022}.
In our work, we incorporate extragradient and optimistic updates into the algorithms discussed in
the previous chapter and experimentally evaluate the improvement gains when applied to two-player
zero-sum games.

