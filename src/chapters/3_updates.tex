\chapter{Modified Updates for Mirror-descent based methods}

We propose applying a few modifications to the methods discussed in the previous section and
investigate the effect of these modifications in terms of their performance and convergence
behaviors in two-player zero-sum games.
We first introduce the proposed modifications to be applied on the methods discussed in the
previous section, and then provide a description of how these modifications fit into those
algorithms.

\section{Neural Replicator Dynamics (NeuRD)}
Neural Replicator Dynamics (NeuRD)~\cite{hennesNeural2020} is a model-free sample-based algorithm
that is an application of Replicator Dynamics to Policy Gradients.
Replicator Dynamics is an idea from Evolutionary game theory (EGT) that defines operators to update
the dynamics of a population in order to maximize some pay-off defined by a fitness function.

The single-population replicator dynamics is defined by the following system of differntial
equations:

\begin{equation}
	\label{eqn:rd} \dot{\pi}(a) = \pi(a)[u(a, \pi) -
		\bar{u}(\pi)], \forall a \in \mathcal{A}
\end{equation}

Hennes
et.al,~\cite{hennesNeural2020} show equivalence between Softmax Policy gradients~\ref{sec:spg} and
continuous-time Replicator Dynamics~\cite[THEOREM 1, on p5]{hennesNeural2020}\label{thm:spgrd}.

NeuRD can be implemented as a single line change to the SPG algorithm.
This can be seen as applying a fix to the update of SPG to make it more responsive to changes in a
non-stationary environment.
Since the typical neural network representation of policies use softmax projection on the logits,
the idea of fixing the gradient updates can be applied more generally to algorithms beyond SPG.
The NeuRD loss has been adapted into other algorithms to improve performance or induce convergence
in competitive and cooperative settings.
Chhablani et.al,~\cite{chhablaniCounterfactual2021} showed improved performance in
identical-interest games by applying the NeuRD fix to COMA~\cite{foersterCounterfactual2018}.
Perolat et.al,~\cite{perolatMastering2022} used a NeuRD based loss function along with adaptive
regularization~\cite{perolatPoincare2021} to induce last-iterate convergence in Stratego.

\subsection{MMD-N}

\subsection{MDPO-N}

\section{Extragradient updates}

The Extragradient method was first introduced by
G.M.Korpelevich~\cite{korpelevichextragradient1976} as a modification of gradient descent methods
in solving saddle point problems.
Extragradient is a classical method for solving smooth and strongly convex-concave bilinear saddle
point problems with a linear rate of convergence.
Extragradient and Optimistic Gradient Descent Ascent methods have been shown to be approximations
of proximal-point method for solving saddle point methods~\cite{mokhtariUnified2020}.

\subsection{MMD-EG}

\subsection{MDPO-EG}

\section{Optimism}

% Optimism is another proximal

\subsection{Optimistic Mirror Descent}

\subsection{OMMD}
\subsection{OMDPO}

