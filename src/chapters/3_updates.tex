\chapter{Modified Updates for Mirror-descent based methods}

The main work of this thesis is exploring a few existing techniques that have been proven to be 
effective in multi-agent settings in the context of the above alogrithms.
These techniques are namely - Neural Replicator Dynamics~\cite{hennesNeural2020}, Extragradient methods~\cite{korpelevichextragradient1976}, and Optimistic updates \red{[?]}. 
We propose combining these techniques to the methods discussed in the previous section and
investigate their behavior through experiments in two-player zero-sum games.
We now discuss these techniques and how they have generally been applied in the literature. 
While discussing each technique, we also outline how they fit into the algorithms discussed 
in the previous section and hypothesize about the potential effects these modifications.

\section{Neural Replicator Dynamics (NeuRD)}
Replicator Dynamics is an idea from Evolutionary game theory (EGT) that defines operators to update
the dynamics of a population in order to maximize some pay-off defined by a fitness function.
Neural Replicator Dynamics (NeuRD) is an extension of Replicator Dynamics 
to function approximation settings.

The single-population replicator dynamics is defined by the following system of differntial
equations:

\begin{equation}
	\label{eqn:rd} \dot{\pi}(a) = \pi(a)[u(a, \pi) -
		\bar{u}(\pi)], \forall a \in \mathcal{A}
\end{equation}

Hennes et.al,~\cite{hennesNeural2020} show equivalence between Softmax Policy gradients~\ref{sec:pg} and
continuous-time Replicator Dynamics~\cite[THEOREM 1, on p5]{hennesNeural2020}\label{thm:spgrd}.

With knowledge of this equivalence they detail how NeuRD can be implemented as a one line change to SPG.
This can be viewed as a fix to the regular SPG update to make it more suited to multi-agent settings 
by making the policy updates more responsive to changes in dynamics.

Replicator dynamics also have a strong connection to no-regret algorithms, and in this work, the authors also establish 
the equivalence between single-state all actions tabular NeuRD, Hedge, and discrete time RD~\cite[Statement 1, p5]{hennesNeural2020}.
Due to this equivalence, NeuRD also inherits the no-regret properties of algorithms like Hedge.
While NeuRD has average iterate convergence, the authors also induce last iterate convergence in imperfect 
information settings for NeuRD using reward transformation based regularization~\cite{perolatPoincare2021}.

e As seen in Fig.., tabular SPG does not converge to the Nash Equilibrium, 
whereas the average policy of tabular NeuRD converges to the Nash Equlibrium.

\subsection{Alternating vs Simultaneous updates}

We also observe that while alternating updates shows average-iterate convergence, simultaneous updates do not converge.
\b{More background about this}

\subsection{NeuRD fix in MMD, and MDPO}
The NeuRD fix can be applied to MMD, and MDPO in a similar way to SPG.


\section{Extragradient updates}

The Extragradient method (EG) was first introduced by
G.M.Korpelevich~\cite{korpelevichextragradient1976} as a modification of gradient descent methods
in solving saddle point problems.
EG is a classical method for solving smooth and strongly convex-concave bilinear saddle
point problems with a linear rate of convergence.
Extragradient and Optimistic Gradient Descent Ascent methods have been shown to be approximations
of proximal-point method for solving saddle point methods~\cite{mokhtariUnified2020}.

EG also has linear last-iterate convergence guarantees for variational inequality problems with a 
strongly monotone operator. This 
\cite{tsenglinear1995} - Last iterate linear convergence using extragradient method for VI problems.

\subsection{MMD-EG}

\subsection{MDPO-EG}

\section{Optimism}

