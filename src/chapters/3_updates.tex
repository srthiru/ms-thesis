\chapter{Modified Updates for Last-iterate Convergence}
\label{chp:updates}

\old{Our main contribution is adapting existing techniques for inducing convergence in multiagent
	settings with the algorithms discussed in the previous section.}
(\revdone{Too brief.
	Slow down and explain to the reader what the problem is that these modified updates are intended to
	solve.
})
Last-iterate convergence of algorithms is an attractive property that has been studied widely in the context
of optimization, and equilibirium learning algorithms.
While there have been numerous methods proposed for equilibrium solving in games, many of them only
guarantee convergence in the ergodic sense, i.e., only the average of their iterates converge to an
equilibrium.
However, in cases where the strategies are represented using function approximators, this presents
a problem as it becomes complicated to maintain a history of past strategies or in terms of
averaging them.

The main contribution of our work is in studying a few modifications to the algorithms discussed in
the previous section, with the objective of either inducing and speeding up last-iterate
convergence in two-player zero-sum settings.
More specifically, we study the following three techniques, namely - Neural Replicator
Dynamics~\cite{hennesNeural2020}, Extragradient updates~\cite{korpelevichextragradient1976}, and
Optimistic gradient updates~\cite{popovmodification1980}, that have been previously studied in the
context of learning in games, and solving saddle point problems.
In this section we describe these techniques in more detail and provide our proposed modifications
to the algorithms from the previous section.

\section{Neural Replicator Dynamics (NeuRD)}

% The continuous-time single-population replicator dynamics is defined by the following system of
% differential equations:
% \begin{equation}
% 	\label{eqn:rd} \dot{\pi}(a) = \pi(a)[u(a, \pi) -
% 		\bar{u}(\pi)], \forall a \in \mathcal{A}
% \end{equation}

Consider the problem of learning a parameterized policy $\pi_{\theta}$ in a single-state all-action
problem setting.
In such a setting, SPG employs the following update at each iteration
$t$~\cite[Section~A.1]{hennesNeural2020}: \[y_{\theta_t}(a) = y_{\theta_{t-1}} (a) + \eta_t
	\pi_{\theta_{t-1}}(a) [r_t(a) - \bar{r_t}], \forall a \in A\] where $y$ represents the logits,
$r_t(a)$ is the immediate reward associated with action $a$, and $\bar{r}_t$ is the average reward
of the state.

\subsection{Replicator Dynamics and No-regret Learning}
The above SPG update is equivalent to the instant regret scaled by the current policy.
This scaling makes it difficult for SPG to adapt to sudden changes in rewards associated with
actions that are already less likely under the current policy.
This problem is more evident in multiagent settings where the opponent's policy can change
arbitrarily affecting rewards associated with actions even in single-state settings.

Motivated by this observation, Hennes et.
al~\cite{hennesNeural2020} propose to make modifications
to SPG using the idea of Replicator dynamics from Evolutionary game theory.
Replicator Dynamics defiens operators that describe the dynamics of a population's evolution when
attempting to maximize some arbitrary utility function (\note{cite}).
Replicator dynamics also have a strong connection to no-regret algorithms, and
in~\cite[Statement~1]{hennesNeural2020}, the authors establish the equivalence between Hedge, and
discrete time RD.
Due to this equivalence, RD also inherits the property of no-regret algorithms that their
time-average policy converges to the Nash equilibrium (\note{cite}).

\subsection{NeuRD}
Neural Replicator Dynamics (NeuRD) is an adaptation of discrete-time Replicator Dynamics to
reinforcement learning for function approximation settings (\revdone{Include the math, not just the
	english}).
For a parameterized policy $\pi_\theta$, the NeuRD-update rule is given by:

\begin{equation}
	\label{eqn:nrd} \theta_t \leftarrow \theta_{t-1} + \eta_t \sum_{a'}
	\nabla_{\theta} \text{Y}_{\theta_{t-1}}(s, a') A(s, a')
\end{equation}

where
$Y_{\theta_{t-1}}$ represent the logits of the parameterized policy at timestep $t-1$, and $A$
represents the advantage value.

The only difference between the NeuRD update, and the SPG update is that the gradient of the
parameters are computed directly with respect to the logits.
This can be viewed as a modification to SPG making it more adaptive in non-stationary settings.
To prevent numerical issues stemming from accumulating the advantages into the logits resulting in
unstable gradients, a logit gap based thresholding is proposed in~\cite{hennesNeural2020}.
This thresholding operator is: $\nabla_{\theta}(f(\theta), \eta, \beta) \doteq [\eta\nabla_\theta
		f(\theta)] \mathds{I}\{f(\theta + \eta\nabla_{\theta}f(\theta)) \in [-\beta, \beta]\}$, where
$\beta$ is the \textit{NeuRD-threshold} that determines how arbitrarily close to 0 or 1 the
probabilities can get to.
% While NeuRD has average iterate convergence,  also induce last iterate convergence in
% imperfect information settings for NeuRD using reward transformation based regularization~\cite{perolatPoincare2021}.
Since most PG methods, including the ones discussed in the previous chapter utilize softmax
parameterization, the NeuRD fix is easily extendable to the policy loss component of their
respective loss functions without any major changes.
In our work, we also apply the NeuRD fix to the policy loss component of the MMD and MDPO loss
functions (\fillin{rewrite this sentence}).

\subsection{Relation to Natural Policy Gradients}

\subsection{Alternating vs Simultaneous gradient updates}
As an aside, we wish to discuss two possible update schemes for discrete learning dynamics, namely
- Simultaneous and Alternating updates.
We also observe that while alternating updates shows average-iterate convergence, simultaneous
updates do not converge.
\begin{figure}[H]
	\centering
	\scalebox{0.6}[0.6]{\input{figs/tabular/altvsim.pgf}}
	\caption[Simultaneous gradient updates vs Alternate gradient updates updates]{NeuRD converges with alternating updates, but not with simultaneous updates.}
\end{figure}
We note that all the experiments in~\cite{hennesNeural2020} also employed alternating updates to
evaluate NeuRD and SPG.
A similar behavior of simultaneous gradient updates diverging was also observed
in~\cite{gidelVariational2020} for unconstrained min-max problems.
They note that the sequence of iterates under alternating updates have a bounded error, and thus
the average of the iterates converges to the solution.
Based on these observations, we use alternating udpates for all our tabular experiments.
Both of these are symmetric update schemes in the sense that both the players perform equal number
of updates every iteration.
Asymmetric updates have also been studied in a similar context and has been shown to improve the
speed of convergence~\cite{daskalakisTraining2018}.

\section{Extragradient and Optimistic Gradient}
First-order optimization algorithms are studied extensively in varying contexts including solving
variational inequalities, saddle-point problems, and convex optimization.
While gradient based methods including the classic Arrow-Hurwicz (\rev{Since you say EG builds on
	this a few lines later, you need to tell the reader at least a bit more about what it is}) method
work well for many optimization problems, for saddle point problems they converge only under
strong-convexity even in the unconstrained case~\cite{heConvergence2022}.

\subsection{Extragradient (EG)}
The Extragradient method was introduced by G.~M.~Korpelevich~\cite{korpelevichextragradient1976} as
a modification to the Arrow-Hurwicz method for solving convex-concave saddle point problems, and
variational inequality problems with strongly monotone operators.
The EG algorithm follows the sequence of updates as given below: % \begin{noindent}
\begin{equation}
	\label{eqn:eg}
	\begin{split}
		\bar{x}_{k} = P_{\calX}[x_k - \eta f(x_k, y_k)] \\ 
		\bar{y}_{k} = P_{\calY}[y_k + \eta f(x_k, y_k)] \\ 
		x_{k+1} = P_{\calX}[x_k - \eta f(\bar{x}_k, \bar{y}_k)] \\ 
		y_{k+1} = P_{\calY}[y_k + \eta f(\bar{x}_k, \bar{y}_k)]
	\end{split}
\end{equation}
% \end{noindent}

If $f$ is $L$-smooth, and $0 < \eta < 1/L$ is the step size, EG has asymptotic
last-iterate convergence.
(\rev{Make sure you have defined the various notions of convergence somewhere, probably in 2 or 3})
~\cite{tsenglinear1995} showed that for VIs with strongly monotone operators
or composite VIs the EG method has a linear last-iterate convergence rate.
(\fillin{expand more})

\subsection{Optimistic gradient (OPT)}
Another modification to the Arrow-Hurwicz gradient method was proposed by L.~D.~Popov
in~\cite{popovmodification1980}, for solving convex-concave saddle point problems.
The proposed algorithm, popularly known as \textbf{Optimistic updates} can be expressed through the
following sequence of updates: % \begin{noindent}
\begin{equation}
	\label{eqn:opt}
	\begin{split}
		\bar{x}_{k} = P_{\calX}[x_k - \eta f(\bar{x}_{k-1}, \bar{y}_{k-1})] \\ 
		\bar{y}_{k} = P_{\calY}[y_k + \eta f(\bar{x}_{k-1}, \bar{y}_{k-1})] \\ 
		x_{k+1} = P_{\calX}[x_k - \eta f(\bar{x}_k, \bar{y}_k)] \\
		y_{k+1} = P_{\calY}[y_k + \eta f(\bar{x}_k, \bar{y}_k)]
	\end{split}
\end{equation}
% \end{noindent}

(\revdone{explain what
	has changed}) Contrast to the EG method, Optimistic updates reuse the gradients of the leading
points of the previous iteration $f(\bar{x}_{k-1}, \bar{y}_{k-1})$ instead of computing an
\textit{extragradient} in each iteration $f(\bar{x_{k}}, \bar{y_{k}})$ as done in~\ref{eqn:eg}.
Variations of the above algorithm have been studied throughout the literature.
Most notably, Optimistic Gradient Descent Ascent and Optimistic Multiplicative Weights Update are
well-established algorithms that use Optimistic updates and are instances of the more general
Optimisitc Mirror Descent.

\subsection{Optimistic Mirror Descent}
\cite{rakhlinOptimization2013} studied this algorithm in the context of Online learning with
predictable sequences under the name Optimistic Mirror Descent.
Optimistic updates coincides with the idea of using a predictor for the performance of next
iteration to guide the updates in Online learning~\cite{}.
While this assumption is valid for the problem of learning with predictable sequences, it is also
valid in multiagent settings if the objective function is convex and both players use regularized
learning algorithms.
Within this work, OMD was also studied in the context of zero-sum games with strongly uncoupled
dynamics.
% This work showed a convergence rate of $O(log T/T)$ for all-actions setting and hypothesized that a
% better rate than $O(1/\sqrt{T})$ is not possible for the sample based setting.
Optimistic Mirror Descent (OMD) is an extragradient version of Mirror Descent, and OGDA can be
shown to be an instantiation of OMD that uses \textit{extrapolation from past}.
OMD updates are given by:
\begin{equation}
	\label{eqn:omd}
	\begin{split}
		x_{k+1} = P[x_k - 2\eta
				f(x_k, y_k) + \eta f(x_{k-1}, y_{k-1})] \\ y_{k+1} = P[y_k + 2\eta f(x_k, y_k) - \eta f(x_{k-1},
				y_{k-1})]
	\end{split}
\end{equation}

\cite{daskalakisTraining2018} studied OMD
to tackle the problem of instability in training GANs.
The also prove last-iterate convergence for OMD in bilinear games and detail an optimistic version
of the Adam optimizer.
OGDA has linear last-iterate convergence \cite{daskalakisTraining2018}.

OMD was also studied in \cite{gidelVariational2020} for the problem of convergence in GANs under
the framework of Variational Inequalities, dubbed \textit{extrapolation from the past}.
Here, extrapolation refers to the EG algorithm, and Optimism is interpreted as EG with the
extrapolation done using the past iterations gradient as an estimate of the extragradient.
They also prove convergence for stochastic VI problems with strong monotone operators for the
average of the iterates.
~\cite{mertikopoulosOptimistic2019} also study the EG method for training GANs by establishing a framework of
\textit{coherence}, and show that EG has last-iterate convergence for coherent VI problems.

% \subsection{Optimistic Gradient Descent Ascent}

Optimism is often interpreted as an adaptation of the Extragradient method with the idea to avoid
the additional call to a gradient oracle by estimating the extrapolation using the previous
iteration's gradient.
Extragradient and Optimistic Gradient Descent Ascent methods have been shown to be approximations
of proximal-point method for solving saddle point methods~\cite{mokhtariUnified2020}.

In our work, we study the effects of adapting extragradient and optimistic updates into the
algorithms discussed in the previous chapter for 2p0s games.

