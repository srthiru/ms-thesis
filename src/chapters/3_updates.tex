\chapter{Modified Updates for Mirror-descent based methods}

Our main contribution is in exploring existing techniques in multiagent learning and last-iterate convergence 
literature, and studying their effectiveness when combined with mirror-descent based algorithms
discussed in the previous section.
More specifically, we study the following three techniques - Neural Replicator Dynamics~\cite{hennesNeural2020}, 
the Extragradient algorithm~\cite{korpelevichextragradient1976}, 
and the Optimistic gradient algorithm~\cite{popovmodification1980}.
In this section we describe these techniques in more detail and how they fit into the problem that we are 
trying to address.
In the next section we perform a detailed experimental evaluation of these algorithms in two-player zero-sum 
games and summarize our findings.

\section{Neural Replicator Dynamics (NeuRD)}
Replicator Dynamics is an idea from Evolutionary game theory (EGT) that defines operators to update
the dynamics of a population in order to maximize some pay-off defined by a fitness function.
Neural Replicator Dynamics (NeuRD) is an extension of Replicator Dynamics 
to function approximation settings.

The single-population replicator dynamics is defined by the following system of differntial
equations:

\begin{equation}
	\label{eqn:rd} \dot{\pi}(a) = \pi(a)[u(a, \pi) -
		\bar{u}(\pi)], \forall a \in \mathcal{A}
\end{equation}

Hennes et.al,~\cite{hennesNeural2020} show equivalence between Softmax Policy gradients~\ref{sec:pg} and
continuous-time Replicator Dynamics~\cite[THEOREM 1, on p5]{hennesNeural2020}\label{thm:spgrd}.

With knowledge of this equivalence they detail how NeuRD can be implemented as a one line change to SPG.
This can be viewed as a fix to the regular SPG update to make it more suited to multi-agent settings 
by making the policy updates more responsive to changes in dynamics.

Replicator dynamics also have a strong connection to no-regret algorithms, and in this work, the authors also establish 
the equivalence between single-state all actions tabular NeuRD, Hedge, and discrete time RD~\cite[Statement 1, p5]{hennesNeural2020}.
Due to this equivalence, NeuRD also inherits the no-regret properties of algorithms like Hedge.
While NeuRD has average iterate convergence, the authors also induce last iterate convergence in imperfect 
information settings for NeuRD using reward transformation based regularization~\cite{perolatPoincare2021}.

e As seen in Fig.., tabular SPG does not converge to the Nash Equilibrium, 
whereas the average policy of tabular NeuRD converges to the Nash Equlibrium.

\subsection{Alternating vs Simultaneous updates}

We also observe that while alternating updates shows average-iterate convergence, simultaneous updates do not converge.
\b{More background about this}

\subsection{NeuRD fix in MMD, and MDPO}
The NeuRD fix can be applied to MMD, and MDPO in a similar way to SPG.


\section{Extrapolation methods}

The Extragradient method (EG) was first introduced by
G.M.Korpelevich~\cite{korpelevichextragradient1976} as a modification of gradient descent methods
in solving saddle point problems.
EG is a classical method for solving smooth and strongly convex-concave bilinear saddle
point problems with a linear rate of convergence.
Extragradient and Optimistic Gradient Descent Ascent methods have been shown to be approximations
of proximal-point method for solving saddle point methods~\cite{mokhtariUnified2020}.

EG also has linear last-iterate convergence guarantees for variational inequality problems with a 
strongly monotone operator. This 
\cite{tsenglinear1995} - Last iterate linear convergence using extragradient method for VI problems.

\subsection{MMD-EG}

\subsection{MDPO-EG}

\section{Optimism}

