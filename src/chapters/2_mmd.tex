\chapter{Mirror Descent in Reinforcement Learning}

There have been a few well-established efforts in adapting mirror descent from a general
first-order optimization algorithm as described above into a reinforcement learning algorithm.
In this work, we mainly consider two such algorithms, namely MDPO (Mirror Descent Policy
Optimization), and MMD (Magnetic Mirror Descent).
We provide a high-level description of the approach taken in arriving at these algorithms and how
they are connected to Mirror Descent before applying modifications on top of these algorithms in an
effort to improve convergence behaviors of these algorithms in two-player zero-sum settings.

\subsection[MDPO]{Mirror Descent Policy Optimization}

The first of these algorithms Mirror Descent Policy Optimization~\cite{tomarMirror2022} addresses
the problem of trust-region solving to stabilize reinforcement learning updates.
Reinforcement learning algorithms, especially Policy gradient methods are notoriously known for
having high-variance updates causing troubles in converging to the optimal policy \red{cite}.
One approach to tackle this problem from an optimization perspective is to ensure that the policy
does not change drastically with each gradient update to stabilize the learning.
This is popularly known as trust region optimization.
PPO, TRPO are popular single agent reinforcement learning algorithms that incorporate trust region
optimization into \dots \red{outline PPO, and TRPO}.
\cite{schulmanProximal2017}

While MDPO differs from the above approaches in that they arrive at a similar algorithm from the
perspective of Mirror Descent, they also show strong connections to PPO, TRPO, and SAC.

\subsection{Mirror Descent as an RL algorithm}
\red{RL objective is not convex; can still use MD..
	\cite{shaniAdaptive2020}}

$$ \pi_{k+1} <- \arg \max_{\pi \in \Pi} \E_{s \sim \rho_{\pi_k}} $$


\textbf{Gradient of KL-Divergence}

Let's consider a reference policy $Q$ and $P_{\theta}$, a policy that is being optimized. 
The reverse KL-Divergence between the two is given by

$$
D_{KL}(P_{\theta} \| Q) = \sum_{a \in A} P_{\theta}(a) \log \frac{P_{\theta}(a)}{Q(a)}
$$

The gradient of KL-Divergence with respect to the parameters theta is given by:

\begin{equation*}
	\begin{split}
\nabla_{\theta} D_{KL} &= \sum_{a \in A} \nabla_{\theta} [P_{\theta}(a) \log P_{\theta}(a)] - 
						\nabla_{\theta} [P_{\theta}(a) \log Q(a)] \\
						&= \sum_{a \in A} [\nabla_{\theta} P_{\theta}(a) \log P_{\theta}(a) + \nabla_{\theta} P_{\theta}(a)] -  
						 \nabla_{\theta} P_{\theta}(a) \log Q(a) \\
						&= \sum_{a \in A} \nabla_{\theta} P_{\theta}(a) (\log P_{\theta}(a) - \log Q(a) + 1)
	\end{split}
\end{equation*}

\section[MMD]{Magnetic Mirror Descent}

Another extension of Mirror Descent to reinforcement learning is Magnetic Mirror Descent~\cite{sokotaUnified2023}, 
that attempts to create a unified algorithm that works well in both single and multiagent settings.
In this work, first an equivalence between solving regularized saddle point problems
and Variational Inequality problems with composite structures is established. 
Then this connection is used to propose an equilibrium solving algorithm that has linear last iterate convergence guarantees. 
Moreover, the proposed method is extended as a reinforcement learning algorithm in single agent setting, and 
also as a multiagent reinforcement algorithm through selfplay.

\subsection{Connection between Variational Inequalities and QREs}

Finding the QRE of a two-player zero-sum game can be represented as the following entropy-regularized saddle
point problem. Given $\calX \subseteq \R^n$, $calY \subseteq \R^m$, and $g_1: \R^n \mapsto \R$, $g_2: \R^m \mapsto \R$, 
find:
\begin{equation}
	\label{eqn:saddle} \min_{x \in \mathcal{X}}
	\max_{y \in \mathcal{Y}} \alpha g_1(x) + f(x, y) + \alpha g_2(y),
\end{equation}

The solution $(x_{\ast}, y_{\ast})$ to the saddle point problem~\ref{eqn:saddle} has the following first-order optimality conditions:
\begin{equation}
	\label{eqn:optcon} 
	\begin{split}
		\langle \alpha \nabla g_1(x_*) + \nabla_{x_*}
		f(x_*, y_*), x - x_* \rangle \geq 0, \forall x \in \calX. \\
		\langle \alpha \nabla g_2(y_*) + \nabla_{y_*} f(x_*, y_*),
		y - y_* \rangle \geq 0, \forall y \in \calY.
	\end{split}
\end{equation}

A Variational Inequality (VI) problem, written as $VI(Z, F)$ is generally defined as follows:
\begin{definition}
	\label{def:vi}
	Given $\calZ
		\subseteq \R^n$ and mapping $F: \calZ \rightarrow \R^n$, the variational inequality problem VI
	($\calZ, F$) is to find $z_{\ast} \in \calZ$ such that,
	\[ \langle F(z_{\ast}),
		z - z_{\ast} \rangle \geq 0 \quad \forall z \in \calZ.
	\]
\end{definition}

The reguarlized saddle point problem~\ref{eqn:saddle} of solving for QREs can be related to the VI problem 
with a composite objective. If $G = F + \alpha \nabla g$, and $\calZ = \mathcal{X} \times \mathcal{Y}$, 
where 
The optimality conditions~\ref{eqn:optcon} are equivalent to VI$(\calZ, G)$, where $G = F + \alpha \nabla g$,
, and $g: \calZ \rightarrow \mathbb{R}$.
Hence, the solution the the VI problem ($z_{\ast}= (x_{\ast}, y_{\ast})$), corresponds to the
solution of the saddle point problem stated in~\ref{eqn:saddle}.

\subsection{MMD Algorithm}

Various algorithms have been proposed to solve the VI problem~\ref{def:vi}.
In particular, the proximal point method has linear last iterate convergence for Variational 
inequality problems with a monotone operator~\cite{rockafellarMonotone1976}.
This algorithm was extended to composite objectives~\cite{tsenglinear1995}, and to non-euclidean 
spaces with Bergman divergence as a proximity measure, 
that allows for non-euclidean proximal regularization~\cite{tsengApproximation2010}.

This non-euclidean proximal gradient algorithm, that is more generally applicable for VI problems 
with monotone operators, performs the following update at each iteration:

\begin{equation}
	\label{eqn:proxgrad} z_{t+1} = \arg \min_{z \in \calZ} \eta (\langle F(z_t), z\rangle + \alpha
	g(z)) + B_{\psi} (z; z_t).
\end{equation}

where $\psi$ is a strongly convex function with respect to $\|.\|$ over $\calZ$.

The algorithm that is termed Magnetic Mirror Descent (MMD) uses the same update as~\ref{eqn:proxgrad}, with $g$ taken to be
$\psi$ as a proximal regularization operator or $B_{\psi}(.;z')$ enforcing $z_{t+1}$ to move close to some reference vector 
$z'$. In the latter case, $z'$ is referred to as the magnet, hence the name Magnetic Mirror Descent.

We now restate the main algorithm as stated in Sokota et.al,~\cite{sokotaUnified2023},

\begin{alprocedure}[H] \algorithmcfname{MMD~\cite[(Algorithm
			3.6)]{sokotaUnified2023}} \label{alg:mmd} Starting with $z_1 \in \text{int dom } \psi \cap \calZ$,
	at each iteration $t$ do $$ z_{t+1} = \arg \min_{z \in \calZ} \eta (\langle F(z_t), z\rangle +
		\alpha \psi(z)) + B_{\psi} (z; z_t).
	$$
	or,
	Given some $z'$, do $$ z_{t+1} = \arg \min_{z \in \calZ} \eta (\langle F(z_t), z\rangle + \alpha
		B_{\psi}(z; z')) + B_{\psi} (z, z_t).
	$$
\end{alprocedure}

\hfill \break
Algorithm~\ref{alg:mmd} provides the following convergence guarantees.

\begin{theorem}
	\label{thm:mmdconv}
	\cite[Theorem 3.4]{sokotaUnified2023}
	Assuming that the solution $z_{\ast}$ to the problem VI ($\calZ, F + \alpha \nabla g$) lies in the
	int dom $\psi$, then

	\[ B_{\psi} (z_{\ast}; z_{t + 1}) \leq {\left(\frac{1}{1 +
				\eta \alpha}\right)}^t B_{\psi} (z_{\ast}; z_1), \]

	if $\alpha > 0$, and $\eta
		\leq \frac{\alpha}{L^2}$.
\end{theorem}

\subsection{Behavioral form MMD}

Algorithm~\ref{alg:mmd} can be either implemented in closed form for a given set of parameters or
by performing stochastic gradient descent at each step to approximate the closed form.
This approximation is especially useful when it is not possible to compute the closed form such as
in function approximation settings.

In the two form of updates specified in Algorithm~\ref{alg:mmd}, the second form involves a $z'$,
term that is referred to as the magnet.
This can either be a reference policy such as the ones used in Imitation learning, or one that is
trailing the current policy to stabilize learning.
Alternatively, one can also use a uniform policy in which case the term reduces to entropy and acts
as a regularization term and encourages exploration.
For the rest of our discussion on MMD, we assume the magnet to always be a uniform policy.

With $\psi$ taken to be negative entropy, the behavioral form of MMD is to perform the following
update at each information state,

\begin{equation}
	\label{eqn:mmdbf} \pi_{t+1}
	= \arg \max_{\pi} \mathbb{E}_{A \sim \pi} q_t(A) + \alpha H(\pi) - \frac{1}{\eta} KL(\pi, \pi_t),
\end{equation}

where $\pi_t$ is the current policy, $q_t$ is a vector
containing the q-values following the policy $\pi_t$, and $H(\pi)$ is the entropy of the policy
being optimized.

In single-agent settings MMD's performance is competitive with PPO in Atari and MuJoCo
environments.
And, in the multi-agent setting the performance of tabular MMD is on par with CFR, but worse than
CFR+.

\subsection{Comparison of MMD to other Mirror Decent based methods}

The non-euclidean proximal gradient method~\ref{eqn:proxgrad}, has strong connections to Mirror Descent. 
\cite[Appendix D.3]{sokotaUnified2023} also details this relationship by showing MMD is equivalent to Mirror 
Descent with an added regularization term.
Since MDPO is a direct extension of Mirror Descent to reinforcement learning, negative entropy based MMD 
is also equivalent to MDPO with an added negative entropy regularization as detailed in~\cite[Appendix L]{sokotaUnified2023}.