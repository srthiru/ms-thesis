\chapter{Mirror Descent in Reinforcement Learning}

Given the generality of the mirror descent algorithm as a first order optimization method, there
has been continued efforts to incorporate it as a single/multi-agent reinforcement learning
algorithm.
Extensive studies of mirror descent under various settings and assumptions help in deriving much
needed theoretical guarantees for mirror descent based reinforcement learning algorithms in terms
of sample complexity, and convergence rates.
In this work, we study two such algorithms, namely MDPO (Mirror Descent Policy Optimization), and
MMD (Magnetic Mirror Descent).
We first begin with a discussion of Softmax Policy Gradients to set a base framework for the rest
of the chapter before moving on to the above algorithms.
% For the following discussion, and the experiments, we consider a class of parameterized stochastic policies,
% i.e., $\Pi = \{\pi_{\theta}, \theta \in \Theta\}$.
% We also only consider on-policy learning.

% \fillin{RL objective is not convex; can still use MD..
% 	\cite{shaniAdaptive2020}}

\section[SPG]{Softmax Policy Gradients}
\label{sec:spg}
In case of a parameterized policy $\pi_\theta$, where $\theta \in \Theta$ represent the policy
parameter space, the aim of policy gradient methods is to maximize some objective $J(\theta)$.
Similar to the value-based approximation methods, the policy can also be directly learnt using
gradient ascent.
A prototypical performance measure is simply the value of the initial state under the current
policy: $J(\theta) = V_{\pi_\theta}(s_0)$.
The Policy Gradient Theorem~\cite[Chapter 13.2]{suttonReinforcement2018} establishes that the
gradient of this objective function can be estimated without knowledge of the environment's state
distribution as long as it is stationary conditioned on the current policy.
% One key challenge in Policy gradient methods is that the evaluation of performance of a policy% depends on the state distribution which could be unknown.

$$ \nabla_\theta J(\theta) \propto \sum_s \mu(s) \sum_a q_\pi(s,a) \nabla_\theta \pi_theta(a|s) $$

\textbf{Softmax Policy Gradients}
For discrete action settings, it is common to use a parameterized function $y_\theta(s, a)$ to
represent action preferences or \textit{logits}, and the policy is then extracted using a softmax
operator on top: $\pi(a|s, \theta) \doteq \frac{e^{y_\theta(s, a)}}{\sum_b e^{y_\theta(s,b)}}$.
Softmax parameterization is the most popular form of policy representation in RL under function
approximation settings.
Policy gradient method with a softmax parameterization is typically referred to as Softmax Policy
Gradients (SPG).

\textbf{Reinforce}:
The most fundamental policy gradient method \textit{REINFORCE}, uses monte-carlo estimations to approximate
the performance measure.
\begin{equation}
	\label{eqn:reinforce}
	\nabla_\theta J(\theta_t)_{|\theta=\theta_t} \propto \E_\pi \big[ G_t \nabla_\theta \log \pi_{\theta_t}(S_t, A_t) \big]
\end{equation}
Although Monte-carlo estimates of the returns are unbiased, they can be of high variance.
A baseline that is independent of the action can be used to reduce this variance.
A popular choice for such a baseline an approximate value function such as the one from value-based
approximation methods ($V(s;w)$).

\textbf{Actor-Critic Methods}
Apart from using this approximate value function as a baseline, we can also use them to better
estimate the peformance measure used in the objective.
This leads to \textit{actor-critic} methods, that learn a parameterized policy (called the actor),
guided by an approximate value function is referred to as the critic.

\subsection{Trust-region methods}
- Trust region methods
- PPO an approximation of TRPO with hueristic objective

\section[MDPO]{Mirror Descent Policy Optimization}

The first method we discuss is the Mirror Descent Policy Optimization~\cite{tomarMirror2022}
(MDPO).
MDPO tackles the problem developing a stable reinforcement learning algorithm through the framework
of trust-region optimization.

(\revdone{ This seems like it mostly belongs in 3.1.2 and then you need to rework this introduction;
	thiru: I moved the PG section back to Chapter 2, and it includes an introduction to these methods.})
% Trust-region optimization stabilizes learning by constraining the policy update at each iteration.
% Instances of trust region optimization methods such as PPO~\cite{schulmanProximal2017}, TRPO~
% \cite{schulmanTrust2015} and their variants have found large success in single agent RL with
% state-of-the-art performances in a wide array of tasks.

Due to the presence of a proximal regularization (\rev{Needs founddations somewhere}), mirror
descent in itself can be interpreted as a trust-region optimization method.
Building on top of existing theoretical guarantees for mirror descent in tabular RL settings
(\rev{Which are?
	need to establish some background either in 2 or here}), MDPO
derives practical RL algorithms that is performant in function approximation settings.
Using the formulation of a parameterized policy from~\ref{sec:spg}, MDPO performs an approximation
of the following update at each iteration:
\begin{equation}
	\label{eqn:mdrl} \pi_{k+1}(.
	|s) \leftarrow \arg
	\max_{\pi \in \Pi}
	\E_{a \sim \pi} [A_{\pi_k}(s,a)] -
	\frac{1}{t_k}
	KL(s; \pi, \pi_k)
\end{equation} using stochastic gradient ascent.

The gradient of the KL component of the above objective for one step of SGD is zero, and hence it
is necessary to take multiple gradient steps every iteration to correctly approximate the mirror
descent objective.
(\revdone{Make sure you include the citation at least the first time you reference them by name so that the reader can get evrything synced up})
Tomar et.~al~\cite{tomarMirror2022}, derive on-policy and off-policy variants of MDPO, and in this
work we mainly focus on the on-policy algorithm for easier comparison with the other methods discussed in this work,
and to remove additional factors involved in off-policy learning (\revdone{Needs an explanation why}).
For a parameterized policy $ \pi_{\theta}$ the on-policy MDPO update is given by: $$ \theta_{k+1}
	\leftarrow \arg \max_{\theta} J(\theta, \theta_k)$$ $$ \text{where}, J(\theta, \theta_k) = \E_{s
		\sim \rho_{\theta_k}} [ \E_{a \sim \pi_\theta}[A_{\theta_k}(s,a)] - \frac{1}{t_k} KL(s; \pi_\theta,
		\pi_{\theta_k}]$$

For $m$ steps MDPO uses the following gradient to update the
policy parameters,
\begin{equation}
	\label{eqn:mdpograd} \nabla_{\theta} J(\theta,
	\theta_k)|_{\theta = \theta_k^{(i)}} = \E_{s \sim \rho_{\theta_k} \\ a \sim \theta_k} [ \frac{
			\pi_{\theta_k}^{(i)}}{ \pi_{\theta_k}} \nabla_{\theta} \log \pi_{\theta_k}^{(i)} (a|s)
		A_{\theta_k}(s,a)] - \frac{1}{t_k} \E_{s \sim \rho_{\theta_k}} [\nabla_\theta KL(s; \pi_\theta,
		\pi_{\theta_k})_{|\theta= \theta_k^{(i)}}]
\end{equation} where $i=0,1,\ldots,m-1$.

% \textbf{Gradient of KL-Divergence}

% Let us consider a reference policy $Q$ and $P_{\theta}$, a policy that is being optimized. 
% The reverse KL-Divergence between the two is given by

% $$
% KL(P_{\theta} \| Q) = \sum_{a \in A} P_{\theta}(a) \log \frac{P_{\theta}(a)}{Q(a)}
% $$

% The gradient of KL-Divergence with respect to the parameters theta is given by:

% \begin{equation*}
% 	\begin{split}
% \nabla_{\theta} KL &= \sum_{a \in A} \nabla_{\theta} [P_{\theta}(a) \log P_{\theta}(a)] - 
% 						\nabla_{\theta} [P_{\theta}(a) \log Q(a)] \\
% 						&= \sum_{a \in A} [\nabla_{\theta} P_{\theta}(a) \log P_{\theta}(a) + \nabla_{\theta} P_{\theta}(a)] -  
% 						 \nabla_{\theta} P_{\theta}(a) \log Q(a) \\
% 						&= \sum_{a \in A} \nabla_{\theta} P_{\theta}(a) (\log P_{\theta}(a) - \log Q(a) + 1)
% 	\end{split}
% \end{equation*}

MDPO has strong connections to the other constrained optimization RL algorithms and entropy
regularized algorithms.
We refer the reader to~\cite{tomarMirror2022} for a detailed discussion of connections between
on-policy MDPO to PPO and TRPO, and off-policy MDPO to SAC.
% MDPO has been shown to have better empirical performance than PPO, and TRPO in continuous control tasks, and Atari environments.

\subsection{Tabular MDPO}
Normal form games allow for tabular policy representations with direct or softmax
parameterizations.
Algorithms can typically use exact value estimations for all possible actions (payoffs) given the
opponent policy, as opposed to a sample-based expected value estimations.
This is referred to as the all-actions setting (also known as \textit{full-feedback} or first-order
information setting).
In this setting, the gradient computation in~ \ref{eqn:mdpograd} becomes: \[ \nabla
	\theta_{|\theta=\theta_{k}^{(i)}} = \nabla_{\theta} \sum_{a \in A} [ \pi_{\theta_k}^{(i)} (a)
		A_{\theta_k}(a)] - \frac{1}{t_k} [\nabla_\theta KL(\pi_\theta, \pi_{\theta_k})] \]

We use this formulation of MDPO update for the tabular experiments and the
original on-policy formulation for the neural experiments.

\section[MMD]{Magnetic Mirror Descent}

Another extension of Mirror Descent to reinforcement learning is Magnetic Mirror Descent~
\cite{sokotaUnified2023}, that attempts to create a unified approach to work well in both single
and multiagent settings.
This work studies the relation between equilibrium solving and Variational inequalities with
composite structures.
Taking advantage of this connection, a equilibrium solving algorithm has linear last-iterate
convergence guarantees is proposed.
Moreover, this approach extends well as a reinforcement learning algorithm in single agent, and
multiagent settings.
In this section, first we outline the connection between Varational inequalities and equlibrium
solving as presented in~ \cite{sokotaUnified2023}.
Then we outline the Magnetic Mirror Descent algorithm and its convergence properties.

\subsection{Connection between Variational Inequalities and QREs}
A Variational Inequality (VI) problem, written as $VI(Z, F)$ is generally defined as follows:
\begin{definition}
	\label{def:vi} Given $\calZ \subseteq \R^n$ and mapping $F: \calZ \rightarrow
		\R^n$, the variational inequality problem VI($\calZ, F$) is to find $z_{\ast} \in \calZ$ such that,
	\[ \langle F(z_{\ast}), z - z_{\ast} \rangle \geq 0 \quad \forall z \in \calZ.
	\]
\end{definition}

The VI problem described above is very general, and as such a wide range of problems can be cast
into this framework~ \cite{facchineiFiniteDimensional2004}.
We mainly focus on the relation between VI problems with a similar structure, and QREs.

Finding the QRE of a two-player zero-sum game can be represented as the regularized saddle point
problem.
Given $\calX \subseteq \R^n$, $\calY \subseteq \R^m$, and $g_1: \R^n \mapsto \R$, $g_2: \R^m
	\mapsto \R$, find:
\begin{equation}
	\label{eqn:saddle} \min_{x \in \mathcal{X}} \max_{y \in
		\mathcal{Y}} \alpha g_1(x) + f(x, y) + \alpha g_2(y),
\end{equation} where $g_1$, and $g_2$, are
strongly-convex functions.
For a two-player zero-sum game, $f(x, y)$ represents the payoff matrix, and $g_1$, $g_2$ represent
entropy-regularization of the strategies of the two players.
(\revdone{It isn't immediately clear where the entropy and regularization are in
	(3.3)})
The
solution $(x_{\ast}, y_{\ast})$ to~ \ref{eqn:saddle} has the following first-order optimality conditions:
\begin{equation}
	\label{eqn:optcon}
	\begin{split}
		\langle \alpha
		\nabla g_1(x_*) + \nabla_{x_*} f(x_*, y_*), x - x_* \rangle \geq 0, \forall x \in \calX.
		\\
		\langle \alpha \nabla g_2(y_*) +
		\nabla_{y_*} f(x_*, y_*),
		y - y_* \rangle \geq 0, \forall y \in \calY.
	\end{split}
\end{equation}

The optimality conditions~\ref{eqn:optcon} are equivalent to the optimality conditions of a
VI$(\calZ, G)$ with the following composite objective $G = F + \alpha \nabla g$, where $\calZ =
	\mathcal{X} \times \mathcal{Y}$, $F(z) = [\nabla_x f(x,y) - \nabla_y f(x,y)]$, and $\nabla g =
	[\nabla_y g_1(x), \nabla_x g_2(y)]$.
Hence, the solution to the VI: $z^* = (x^*, y^*)$ is also the solution to the saddle point problem
stated in~\ref{eqn:saddle}.
So, the reguarlized saddle point problem~\ref{eqn:saddle} of finding QREs can be cast as a VI
problem allowing us to tap into the vast array of existing techniques for solving Variational
inequalities.

\subsection{MMD Algorithm}
Various algorithms have been proposed to solve the VI problem~\ref{def:vi}.
In particular, the proximal point method has linear last iterate convergence for Variational
inequality problems with a strongly monotone operator~\cite{rockafellarMonotone1976}.
This algorithm was extended to composite objectives~\cite{tsenglinear1995}, and to non-euclidean
spaces with Bergman divergence as a proximity measure, that allows for non-euclidean proximal
regularization~\cite{tsengApproximation2010}.

The non-euclidean proximal gradient algorithm, that is more generally applicable to any VI problem
with a monotone operator, performs the following update at each iteration:

\begin{equation}
	\label{eqn:proxgrad} z_{t+1} = \arg \min_{z \in \calZ} \eta (\langle F(z_t),
	z\rangle + \alpha g(z)) + B_{\psi} (z; z_t).
\end{equation}

where $z_1 \in \text{int dom } \psi \cap \calZ$, and $\psi$ is a strongly convex function with respect to $\|.\|$ over $\calZ$.

The algorithm that is termed Magnetic Mirror Descent (MMD) uses the same update
as~\ref{eqn:proxgrad} with $g$ taken to be either $\psi$, or $B_{\psi}(.
	;z')$.
In the former, $\psi$ is as a strongly convex regularizer that makes the objective smoother and
encouranges exploration.
In the latter form, $B_{\psi}$ is another proximity term that forces the iterates ($z_{t+1}$) to
stay close to some \textit{magnet} ($z'$).
For all our discussion, we only consider the former update rule which is more widely applicable.
In this case, the main MMD algorithm~\cite[(Algorithm 3.6)]{sokotaUnified2023} becomes,
(\revdone{This is an equation, not al algorithm, because it doesn't tell you how to compute the
	minimumum})

\begin{equation}
	\label{eqn:mmd} z_{t+1} = \arg \min_{z \in \calZ}
	\eta (\langle F(z_t), z\rangle + \alpha \psi(z)) + B_{\psi} (z; z_t).
\end{equation}

The MMD algorithm using the above update rule has the following linear convergence guarantee.
\begin{theorem}
	\label{thm:mmdconv}
	\cite[Theorem 3.4]{sokotaUnified2023}
	Assuming that the solution $z_{\ast}$ to the problem VI ($\calZ, F + \alpha \nabla g$) lies in the
	int dom $\psi$, then

	\[ B_{\psi} (z_{\ast}; z_{t + 1}) \leq { \left(\frac{1}{1
				+ \eta \alpha}\right)}^t B_{\psi} (z_{\ast}; z_1), \]

	if $\alpha > 0$, and
	$\eta \leq \frac{\alpha}{L^2}$.
\end{theorem}

\subsection{Behavioral form MMD}

The update rule~\ref{eqn:mmd} admits closed form in some instances, while requires approximation
through gradient updates in other cases.
For a parameterized policy $\pi_\theta$, when $\psi$ is negative entropy it can be restated in RL
terms as follows:
\begin{equation}
	\label{eqn:mmdbf} \pi_{ \theta_{k+1}}(s,a) = \arg \max_{\theta}
	\E_{s \sim \rho_{\theta_k}} \left[ \E_{a \sim \pi_{\theta_k}} [Q_{\theta_k}(s, a)] + \alpha
		H(\pi_\theta) - \frac{1}{\eta} KL(\pi_\theta, \pi_{\theta_k}) \right],
\end{equation}

where $H(\pi_\theta)$ is the entropy of the policy being optimized.

This behavioral form update can also be approximated using gradient updates similar to MDPO.
For $m$ steps MMD uses the following gradient to update the policy parameters,
\begin{equation}
	\label{eqn:mmdgrad} \nabla_{\theta} J(\theta, \theta_k)|_{\theta = \theta_k^{(i)}} = \E_{s \sim
		\rho_{\theta_k} \\ a \sim \theta_k} [ \frac{ \pi_{\theta_k}^{(i)}}{ \pi_{\theta_k}} \nabla_{\theta}
		\log \pi_{\theta_k}^{(i)} (a|s) A_{\theta_k}(s,a)] + \alpha H( \pi_{\theta_k}^{(i)}) -
	\frac{1}{\eta} \E_{s \sim \rho_{\theta_k}} [\nabla_\theta KL(s; \pi_\theta,
		\pi_{\theta_k})_{|\theta= \theta_k^{(i)}}]
\end{equation} where $i=0,1,\ldots,m-1$.

\textbf{Tabular MMD: }
Similar to tabular MDPO, the gradient computation for behavioral-form MMD in single-state
all-action setting becomes:

\begin{equation}
	\label{eqn:tabmmd} \nabla
	\theta_{|\theta=\theta_{k}^{(i)}} = \nabla_{\theta} \sum_{a \in A} [ \pi_{\theta_k}^{(i)} (a)
		A_{\theta_k}(a)] + \alpha \nabla_{\theta} H(\pi_{\theta_k}) - \frac{1}{\eta} [\nabla_\theta
		KL(\pi_\theta, \pi_{\theta_k})]
\end{equation}

\subsection{Closed-form vs
	Behavioral-form} MMD with a negative entropy mirror map~ \cite[equation (12)]{sokotaUnified2023}
has the following closed-form:

\begin{equation}
	\label{eqn:mmdcf} \pi_{k+1}
	\propto [\pi_t \rho^{\alpha \eta} e^{\eta Q_{\pi_k}}]^{ \frac{1}{1+\alpha \eta}}
\end{equation}

To examine the effect of the choice of $m$, we compare the performance of
tabular MMD~\ref{eqn:tabmmd} to the above closed-form.

In Fig~ \ref{fig:?
}, we plot the norm of the difference between the closed-form and behavioral form policies at each iteration.
It can be seen that the behavioral form approximates the closed form well as expected for most
choices of step size and $m$.
For large number of gradient steps, or large step sizes the updates are unstable.
For all our tabular experiments, we use a step-size of 0.1, and $m=10$.

\subsection{Comparison of MMD to other Mirror Decent based methods}

The non-euclidean proximal gradient method~ \ref{eqn:proxgrad}, has strong connections to Mirror
Descent~ \cite[Appendix D.3]{sokotaUnified2023}.
\fillin{TBD: other works also discuss this relationship between mirror descent and proximal gradient methods applied to VIs}.
Consequently negative entropy based MMD is also equivalent to MDPO with an added entropy
regularization as detailed in~ \cite[Appendix L]{sokotaUnified2023} and as can be seen from~
\ref{eqn:mdrl} and~ \ref{eqn:mmdbf}.

(\rev{Cite?
	Also, have you introduced CFR at some point?
})
Sokota et.~al~\cite{sokotaUnified2023} experimentally demonstrate MMD's strong performance in single, and
multi-agent settings by evaluating it against popular baselines.
In single agent settings MMD's performance is competitive with PPO in Atari and MuJoCo
environments.
MMD instantiated as a reinforcement learning algorithm performing behavioral form update at each
information state performs on the same level as CFR, but relatively worse compared to CFR+.
