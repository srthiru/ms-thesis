\chapter{Mirror Descent in Reinforcement Learning} 

There have been a few recent efforts in adapting mirror descent from a constrained convex optimization 
algorithm into a reinforcement learning algorithm.
In this work, we study two such algorithms, namely MDPO (Mirror Descent Policy
Optimization), and MMD (Magnetic Mirror Descent).
We first describe these algorithms and their connection to Mirror Descent in this section.
In the next section we propose some modifications to be applied on top of these algorithms 
to improve their convergence behaviors in two-player zero-sum settings.

For the following discussion, and the experiments, we consider a class of parameterized stochastic policies, 
i.e., $\Pi = \{\pi_{\theta}, \theta \in \Theta\}$.
We also only consider on-policy learning.

% \blue{RL objective is not convex; can still use MD..
% 	\cite{shaniAdaptive2020}}

\section[MDPO]{Mirror Descent Policy Optimization}

The first method we discuss is the Mirror Descent Policy Optimization~\cite{tomarMirror2022} (MDPO). 
MDPO deals with the problem of trust-region based policy learning. 
Trust-region methods aim to stabilize learning by constraining the policy update at each iteration.
State-of-the-art RL algorithms like PPO~\cite{schulmanProximal2017}, TRPO~\cite{schulmanTrust2015} are instances of trust region
optimization methods.

Mirror Descent as such can also be interpreted as a trust-region optimization algorithm due to the presense of 
proximal regularization.
MDPO adapts the mirror descent objective~\ref{eqn:mdobj} into RL terms is as follows:
\begin{equation}\label{eqn:mdrl}
	\pi_{k+1}(.|s) \leftarrow \arg\max_{\pi \in \Pi} \E_{a \sim \pi} [A_{\pi_k}(s,a)] - \frac{1}{t_k} KL(s; \pi, \pi_k) 	
\end{equation}

This objective is then approximately solved at each iteration through gradient ascent.
We focus on the on-policy variant of MDPO where, for a parameterized policy $\pi_{\theta}$ 
the above update rule is reframed into:
$$\theta_{k+1} \leftarrow \arg\max_{\theta} J(\theta, \theta_k)$$
$$\text{where}, J(\theta, \theta_k) = \E_{s \sim \rho_{\theta_k}} [ \E_{a \sim \pi_\theta}[A_{\theta_k}(s,a)] - \frac{1}{t_k} KL(s; \pi_\theta, \pi_{\theta_k}]$$
The gradient of the KL component of the above objective for one step of SGD is zero. Hence, it is necessary to 
take multiple steps of stochastic gradient updates to approximate the objective in~\ref{eqn:mdrl}.

For $m$ steps MDPO uses the following gradient to update the policy parameters, 
\begin{equation}
	\label{eqn:mdpograd}
	\nabla_{\theta} J(\theta, \theta_k)|_{\theta = \theta_k^{(i)}} = \E_{s \sim \rho_{\theta_k} \\ a \sim \theta_k} 
	[\frac{\pi_{\theta_k}^{(i)}}{\pi_{\theta_k}} \nabla_{\theta} \log \pi_{\theta_k}^(i) (a|s) A_{\theta_k}(s,a)] - 
	\frac{1}{t_k} \E_{s \sim \rho_{\theta_k}} [\nabla_\theta KL(s; \theta, \theta_k)]
\end{equation}
where $i=0,1,\ldots,m-1$.


% \textbf{Gradient of KL-Divergence}

% Let us consider a reference policy $Q$ and $P_{\theta}$, a policy that is being optimized. 
% The reverse KL-Divergence between the two is given by

% $$
% KL(P_{\theta} \| Q) = \sum_{a \in A} P_{\theta}(a) \log \frac{P_{\theta}(a)}{Q(a)}
% $$

% The gradient of KL-Divergence with respect to the parameters theta is given by:

% \begin{equation*}
% 	\begin{split}
% \nabla_{\theta} KL &= \sum_{a \in A} \nabla_{\theta} [P_{\theta}(a) \log P_{\theta}(a)] - 
% 						\nabla_{\theta} [P_{\theta}(a) \log Q(a)] \\
% 						&= \sum_{a \in A} [\nabla_{\theta} P_{\theta}(a) \log P_{\theta}(a) + \nabla_{\theta} P_{\theta}(a)] -  
% 						 \nabla_{\theta} P_{\theta}(a) \log Q(a) \\
% 						&= \sum_{a \in A} \nabla_{\theta} P_{\theta}(a) (\log P_{\theta}(a) - \log Q(a) + 1)
% 	\end{split}
% \end{equation*}

On-policy MDPO has strong connections to TRPO, and PPO, and has been shown to have better empirical performance 
in continuous control tasks, and Atari environments~\cite{tomarMirror2022}.

\subsection{MDPO in MARL}
Although MDPO shows strong empirical performance in single-agent RL, it is not directly extendable to multi-agent settings.
The main challenge here is the non-stationarity problem that is discussed in~\ref{sec:marl}.

In this section, we derive the update rule for a tabular all-actions MDPO and apply it to the PerturbedRPS problem discussed in section 
 to show 
the convergence behavior of MDPO in a multi-agent setting.

%For tabular all-actions MDPO in MARL, the policy update rule at iteration k for agent i is given by: 
\blue{derivation tbd}

Fig \blue{add trajectory plots} shows the convergence behavior of MDPO in the Perturbed RPS problem. 
We can see that MDPO fails to converge to a Nash equilibrium (indicated by the red dot).
This is due to the non-stationarity problem caused by the simultaneous updates of both agents.
In this next chapter we discuss how to overcome this problem, and how to adapt MDPO for last-iterate convergence in two-player zero-sum games.

\section[MMD]{Magnetic Mirror Descent}

Another extension of Mirror Descent to reinforcement learning is Magnetic Mirror Descent~\cite{sokotaUnified2023}, 
that attempts to create a unified approach to work well in both single and multiagent settings.
This work studies the relation between equilibrium solving and Variational inequalities with composite structures. 
Taking advantage of this connection, a equilibrium solving algorithm has linear last-iterate convergence guarantees is proposed. 
Moreover, this approach extends well as a reinforcement learning algorithm in single agent, and multiagent settings.
In this section, first we outline the connection between Varational inequalities and equlibrium solving as presented in~\cite{sokotaUnified2023}. 
Then we outline the Magnetic Mirror Descent algorithm and its convergence properties.

\subsection{Connection between Variational Inequalities and QREs}
A Variational Inequality (VI) problem, written as $VI(Z, F)$ is generally defined as follows:
\begin{definition}
	\label{def:vi}
	Given $\calZ
		\subseteq \R^n$ and mapping $F: \calZ \rightarrow \R^n$, the variational inequality problem VI($\calZ, F$) 
		is to find $z_{\ast} \in \calZ$ such that,
	\[ \langle F(z_{\ast}),
		z - z_{\ast} \rangle \geq 0 \quad \forall z \in \calZ.
	\]
\end{definition}

The VI problem described above is very general, and as such a wide range of problems can be cast into this 
framework~\cite{facchineiFiniteDimensional2004}. We mainly focus on the relation between VI problems with a similar structure, 
and QREs.

Finding the QRE of a two-player zero-sum game can be represented as the following entropy-regularized saddle
point problem. Given $\calX \subseteq \R^n$, $calY \subseteq \R^m$, and $g_1: \R^n \mapsto \R$, $g_2: \R^m \mapsto \R$, 
find:
\begin{equation}
	\label{eqn:saddle} \min_{x \in \mathcal{X}}
	\max_{y \in \mathcal{Y}} \alpha g_1(x) + f(x, y) + \alpha g_2(y),
\end{equation}

The solution $(x_{\ast}, y_{\ast})$ to the saddle point problem~\ref{eqn:saddle} has the following first-order optimality conditions:
\begin{equation}
	\label{eqn:optcon} 
	\begin{split}
		\langle \alpha \nabla g_1(x_*) + \nabla_{x_*}
		f(x_*, y_*), x - x_* \rangle \geq 0, \forall x \in \calX. \\
		\langle \alpha \nabla g_2(y_*) + \nabla_{y_*} f(x_*, y_*),
		y - y_* \rangle \geq 0, \forall y \in \calY.
	\end{split}
\end{equation}

The reguarlized saddle point problem~\ref{eqn:saddle} of solving for QREs is equivalent to the VI problem 
with the following composite objective $G = F + \alpha \nabla g$,  where $\calZ = \mathcal{X} \times \mathcal{Y}$, 
$F(z) = [\nabla_x f(x,y) - \nabla_y f(x,y)]$, and $\nabla g = [\nabla_y g_1(x), \nabla_x g_2(y)]$.
The optimality conditions~\ref{eqn:optcon} are equivalent to the VI$(\calZ, G)$, and thus the solution 
to the VI: $z^* = (x^*, y^*)$ is also the solution to the saddle point problem stated in~\ref{eqn:saddle}.

\subsection{MMD Algorithm}

Various algorithms have been proposed to solve the VI problem~\ref{def:vi}.
In particular, the proximal point method has linear last iterate convergence for Variational 
inequality problems with a monotone operator~\cite{rockafellarMonotone1976}.
This algorithm was extended to composite objectives~\cite{tsenglinear1995}, and to non-euclidean 
spaces with Bergman divergence as a proximity measure, 
that allows for non-euclidean proximal regularization~\cite{tsengApproximation2010}.

The non-euclidean proximal gradient algorithm, that is more generally applicable to any VI problem
with a monotone operator, performs the following update at each iteration:

\begin{equation}
	\label{eqn:proxgrad} z_{t+1} = \arg \min_{z \in \calZ} \eta (\langle F(z_t), z\rangle + \alpha
	g(z)) + B_{\psi} (z; z_t).
\end{equation}

where $\psi$ is a strongly convex function with respect to $\|.\|$ over $\calZ$.

The algorithm that is termed Magnetic Mirror Descent (MMD) uses the same update as~\ref{eqn:proxgrad} with $g$ taken to be
either $\psi$, or $B_{\psi}(.;z')$. In the former, $\psi$ is as a strongly convex regularizer that makes the objective smoother and encouranges 
exploration. In the latter form, $B_{\psi}$ is another proximity term that forces the iterates ($z_{t+1}$) to stay close to some  
\textit{magnet} ($z'$). For all our discussion, we only consider the former update rule which is more widely 
applicable.

We now restate the main algorithm as stated in Sokota et.al,~\cite{sokotaUnified2023},

\begin{alprocedure}[H] \algorithmcfname{MMD~\cite[(Algorithm
			3.6)]{sokotaUnified2023}} \label{alg:mmd} Starting with $z_1 \in \text{int dom } \psi \cap \calZ$,
	at each iteration $t$ do $$ z_{t+1} = \arg \min_{z \in \calZ} \eta (\langle F(z_t), z\rangle +
		\alpha \psi(z)) + B_{\psi} (z; z_t).
	$$
\end{alprocedure}

\hfill \break
Algorithm~\ref{alg:mmd} provides the following convergence guarantees.
\begin{theorem}
	\label{thm:mmdconv}
	\cite[Theorem 3.4]{sokotaUnified2023}
	Assuming that the solution $z_{\ast}$ to the problem VI ($\calZ, F + \alpha \nabla g$) lies in the
	int dom $\psi$, then

	\[ B_{\psi} (z_{\ast}; z_{t + 1}) \leq {\left(\frac{1}{1 +
				\eta \alpha}\right)}^t B_{\psi} (z_{\ast}; z_1), \]

	if $\alpha > 0$, and $\eta
		\leq \frac{\alpha}{L^2}$.
\end{theorem}

\subsection{Behavioral form MMD}

The update rule from Algorithm~\ref{alg:mmd} admits closed form in some instances, while requires approximation 
through gradient updates in other cases.
For a parameterized policy $pi_\theta$, when $\psi$ is negative entropy the update rule from Algorithm~\ref{alg:mmd} can be restated in 
RL terms as follows:
\begin{equation}
	\label{eqn:mmdbf} \pi_{\theta_{k+1}}
	= \arg \max_{\theta} \E_{s \sim \rho_{\theta_k}} \left[ \E_{a \sim \pi_{\theta_k}} [Q_{\theta_k}(s, a)] + \alpha H(\pi_\theta) - \frac{1}{\eta} KL(\pi_\theta, \pi_{\theta_k}) \right],
\end{equation}

where $H(\pi_\theta)$ is the entropy of the policy
being optimized.

\subsection{Comparison of MMD to other Mirror Decent based methods}

The non-euclidean proximal gradient method~\ref{eqn:proxgrad}, has strong connections to Mirror Descent~\cite[Appendix D.3]{sokotaUnified2023}.
\blue{other works also discuss this relationship between mirror descent and proximal gradient methods applied to VIs}.
Consequently negative entropy based MMD is also equivalent to MDPO with an added entropy regularization 
as detailed in~\cite[Appendix L]{sokotaUnified2023} and as can be seen from~\ref{eqn:mdrl} and~\ref{eqn:mmdbf}.


MMD as a reinforcement learning algorithm performs on par with CFR.
In single agent settings MMD's performance is competitive with PPO in Atari and MuJoCo
environments.