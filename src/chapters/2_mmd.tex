\chapter{Mirror Descent in Reinforcement Learning}

\section{Mirror Descent}

In this section we discuss about Mirror Descent, and two mirror descent-based reinforcement
learning algorithms (MDPO, and MMD).
Mirror descent is a popular first order optimization algorithm that has seen wide applications in
machine learning, and reinforcement learning.

There are different views of arriving at Mirror Descent as an optimization algorithm, here we
present the Mirror Descent framework through the lens of mirror maps.

\subsection{Mirror Maps}

% Define Bregman divergence
\begin{definition}
	\label{def:bregman}
	Given a convex set $\calX \subset \R^n$, and a differentiable convex function $f: \calX \mapsto
		\R$, the Bregman Divergence associated with the function $f$ is defined as,

	$$
		D_f(x, y) = f(x) - f(y) - \nabla f(y)^T (x-y).
	$$
\end{definition}

We can also define a few properties that the function $f$ has based on the Bregman Divergence.
\begin{definition}[Strongly Smooth]~\label{def:strsmooth}
	Given a convex set $\calX \in \R^n$, a convex function $f: \calX \mapsto \R$ is $\sigma$-strongly
	smooth with respect to a norm $\|.
		% Define Strongly convex, strongly smooth.
		\|$, if $\|\nabla f(x) - \nabla f(y) \|_{\ast} \leq \sigma \|x - y \|, \forall x, y \in \calX$.
\end{definition}

For an alternate view of deriving Mirror Descent as an improvement of FoReL, please refer
to~\cite{shalev-shwartzOnline2012}[Section 2.6].
% A disadvantage of FoReL~\ref{sec:forel} in solving online learning problems is that, there is a
% minimization that must be performed at every step.
% Mirror descent overcomes this by using a recursive update rule that does not required us to perform
% a minimization at every step.

There have been a few well-established efforts in adapting mirror descent from a general
first-order optimization algorithm as described above into a reinforcement learning algorithm.
In this work, we mainly consider two such algorithms, namely MDPO (Mirror Descent Policy
Optimization), and MMD (Magnetic Mirror Descent).
We provide a high-level description of the approach taken in arriving at these algorithms and how
they are connected to Mirror Descent before applying modifications on top of these algorithms in an
effort to improve convergence behaviors of these algorithms in two-player zero-sum settings.

\subsection[MDPO]{Mirror Descent Policy Optimization}

The first of these algorithms Mirror Descent Policy Optimization~\cite{tomarMirror2022} addresses
the problem of trust-region solving to stabilize reinforcement learning updates.
Reinforcement learning algorithms, especially Policy gradient methods are notoriously known for
having high-variance updates causing troubles in converging to the optimal policy \red{cite}.
One approach to tackle this problem from an optimization perspective is to ensure that the policy
does not change drastically with each gradient update to stabilize the learning.
This is popularly known as trust region optimization.
PPO, TRPO are popular single agent reinforcement learning algorithms that incorporate trust region
optimization into \dots \red{outline PPO, and TRPO}.
\cite{schulmanProximal2017}

While MDPO differs from the above approaches in that they arrive at a similar algorithm from the
perspective of Mirror Descent, they also show strong connections to PPO, TRPO, and SAC.

\subsection{Mirror Descent as an RL algorithm}
\red{RL objective is not convex; can still use MD..
	\cite{shaniAdaptive2020}}

$$ \pi_{k+1} <- \arg \max_{\pi \in \Pi} \E_{s \sim \rho_{\pi_k}} $$


\textbf{Gradient of KL-Divergence}

Let's consider a reference policy $Q$ and $P_{\theta}$, a policy that is being optimized. 
The reverse KL-Divergence between the two is given by

$$
D_{KL}(P_{\theta} \| Q) = \sum_{a \in A} P_{\theta}(a) \log \frac{P_{\theta}(a)}{Q(a)}
$$

The gradient of KL-Divergence with respect to the parameters theta is given by:

\begin{equation*}
	\begin{split}
\nabla_{\theta} D_{KL} &= \sum_{a \in A} \nabla_{\theta} [P_{\theta}(a) \log P_{\theta}(a)] - 
						\nabla_{\theta} [P_{\theta}(a) \log Q(a)] \\
						&= \sum_{a \in A} [\nabla_{\theta} P_{\theta}(a) \log P_{\theta}(a) + \nabla_{\theta} P_{\theta}(a)] -  
						 \nabla_{\theta} P_{\theta}(a) \log Q(a) \\
						&= \sum_{a \in A} \nabla_{\theta} P_{\theta}(a) (\log P_{\theta}(a) - \log Q(a) + 1)
	\end{split}
\end{equation*}

\section[MMD]{Magnetic Mirror Descent}

Another extension of Mirror Descent to Reinforcement learning is Magnetic Mirror Descent~\cite{sokotaUnified2023}.
The main aim of this work is creating an RL algorithm that performs well in both single, and 
multiagent settings.
The authors start-off by casting QRE solving as a bilinear saddle point problem and establish its equivalence 
to solving Variational Inequality problems with specific structures.

\subsection{Connection between Variational Inequalities and QREs}

Variational inequalities are a general class of problems that have a wide range of applications.
A Variational Inequality problem is generally of the following form:

\begin{definition}
	Given $\calZ
		\subseteq \R^n$ and mapping $G: \calZ \rightarrow \R^n$, the variational inequality problem VI
	($\calZ, G$) is to find $z_{\ast} \in \calZ$ such that,

	\[ \langle G(z_{\ast}),
		z - z_{\ast} \rangle \geq 0 \quad \forall z \in \calZ.
	\]
\end{definition}

Solving for QREs in two-player zero-sum games can be represented as the following bilinear saddle
point problem:

\begin{equation}
	\label{eqn:saddle} \min_{x \in \mathcal{X}}
	\max_{y \in \mathcal{Y}} \alpha g_1(x) + f(x, y) + \alpha g_2(y),
\end{equation}

where $\mathcal{X} \subset \mathbb{R}^n$, $\mathcal{Y} \subset \mathbb{R}^m$
are closed and convex, and $g_1: \mathbb{R}^n \rightarrow \mathbb{R}$, $g_2: \mathbb{R}^m
	\rightarrow \mathbb{R}$, $f : \mathbb{R}^n \times \mathbb{R}^m \rightarrow \mathbb{R}$.

The solution $(x_{\ast}, y_{\ast})$ to the saddle point problem~\ref{eqn:saddle}, corresponds to
the Nash equilibrium of the reguarlized game with the following first-order optimality conditions:

\begin{equation}
	\label{eqn:optcon1} \langle \nabla g_1(x_*) + \nabla_{x_*}
	f(x_*, y_*), x - x_* \rangle \geq 0, \forall x \in \calX.
\end{equation}

\begin{equation}
	\label{eqn:optcon2}
	\langle \nabla g_2(y_*) + \nabla_{y_*} f(x_*, y_*),
	y - y_* \rangle \geq 0, \forall y \in \calY.
\end{equation}

These optimality conditions are equivalent to VI$(\calZ, G)$, where $G = F + \alpha \nabla g$,
$\calZ = \mathcal{X} \times \mathcal{Y}$, and $g: \calZ \rightarrow \mathbb{R}$.
Hence, the solution the the VI problem ($z_{\ast}= (x_{\ast}, y_{\ast})$), corresponds to the
solution of the saddle point problem stated in~\ref{eqn:saddle}.


% Sokota et al.~\cite{sokotaUnified2023} cast solving for
% QREs in two-player zero-sum games as a and apply MMD to solve for QREs with linear last-iterate
% convergence guarantees.
% They do so by first representing the two-player zero-sum games as a bilinear saddle point problem
% and then showing the equivalence between solving the saddle point problem and solving variation
% inequality problems under specific conditions.
% They then use this connection to derive an algorithm that is termed Magnetic Mirror Descent (MMD).
% In this section we detail how this relation is established and also outline the algorithm for
% Magnetic Mirror Descent as presented in~\cite{sokotaUnified2023}.



% Represent QRE as a bilinear saddle point problem



\subsection{MMD Algorithm}

Based on the above connection they propose a non-euclidean proximal gradient method as an
equilibrium finding algorithm in two-player zero-sum games.
They also prove a linear last-iterate convergence guarantee for the algorithm as a QRE solver.

The authors first propose a non-Euclidean proximal gradient method to solve the VI ($\calZ, F +
	\alpha \nabla g$) with the following update at each iteration:

\begin{equation}
	\label{eqn:proxgrad} z_{t+1} = \arg \min_{z \in \calZ} \eta (\langle F(z_t), z\rangle + \alpha
	g(z)) + B_{\psi} (z; z_t).
\end{equation}

With the following assumptions, $z_{t+1}$ is well defined:

\begin{itemize}
	\item $\psi$ is 1-strongly convex with respect to $\|.
		      \|$ over $\calZ$, and for
	      any $l$, stepsize $\eta > 0$, $\alpha > 0$,
	      $z_{t+1} = \arg \min_{z \in \calZ} \eta (\langle l, z \rangle + \alpha g(z)) + B_{\psi}(z;z_t) \in \text{int dom } \psi.$

	\item $F$ is monotone and $L$-smooth with respect to $\|.\|$ and $g$ is 1-strongly convex
	      relative to $\psi$ over $\calZ$ with $g$ differentiable over int dom $\psi$.
\end{itemize}

The algorithm that is termed MMD uses the same update as~\ref{eqn:proxgrad}, with $g$ taken to be
$\psi$ or $B_{\psi}(.
	;z')$ for some z'.

We now restate the main algorithm as stated in Sokota et.al,~\cite{sokotaUnified2023},

\begin{alprocedure}[H] \algorithmcfname{MMD~\cite[(Algorithm
			3.6)]{sokotaUnified2023}} \label{alg:mmd} Starting with $z_1 \in \text{int dom } \psi \cap \calZ$,
	at each iteration $t$ do $$ z_{t+1} = \arg \min_{z \in \calZ} \eta (\langle F(z_t), z\rangle +
		\alpha \psi(z)) + B_{\psi} (z; z_t).
	$$
	or,
	Given some $z'$, do $$ z_{t+1} = \arg \min_{z \in \calZ} \eta (\langle F(z_t), z\rangle + \alpha
		B_{\psi}(z; z')) + B_{\psi} (z, z_t).
	$$
\end{alprocedure}

\hfill \break
Algorithm~\ref{alg:mmd} provides the following convergence guarantees.

\begin{theorem}
	\label{thm:mmdconv}
	\cite[Theorem 3.4]{sokotaUnified2023}
	Assuming that the solution $z_{\ast}$ to the problem VI ($\calZ, F + \alpha \nabla g$) lies in the
	int dom $\psi$, then

	\[ B_{\psi} (z_{\ast}; z_{t + 1}) \leq {\left(\frac{1}{1 +
				\eta \alpha}\right)}^t B_{\psi} (z_{\ast}; z_1), \]

	if $\alpha > 0$, and $\eta
		\leq \frac{\alpha}{L^2}$.
\end{theorem}

\subsection{Behavioral form MMD}

Algorithm~\ref{alg:mmd} can be either implemented in closed form for a given set of parameters or
by performing stochastic gradient descent at each step to approximate the closed form.
This approximation is especially useful when it is not possible to compute the closed form such as
in function approximation settings.

In the two form of updates specified in Algorithm~\ref{alg:mmd}, the second form involves a $z'$,
term that is referred to as the magnet.
This can either be a reference policy such as the ones used in Imitation learning, or one that is
trailing the current policy to stabilize learning.
Alternatively, one can also use a uniform policy in which case the term reduces to entropy and acts
as a regularization term and encourages exploration.
For the rest of our discussion on MMD, we assume the magnet to always be a uniform policy.

With $\psi$ taken to be negative entropy, the behavioral form of MMD is to perform the following
update at each information state,

\begin{equation}
	\label{eqn:mmdbf} \pi_{t+1}
	= \arg \max_{\pi} \mathbb{E}_{A \sim \pi} q_t(A) + \alpha H(\pi) - \frac{1}{\eta} KL(\pi, \pi_t),
\end{equation}

where $\pi_t$ is the current policy, $q_t$ is a vector
containing the q-values following the policy $\pi_t$, and $H(\pi)$ is the entropy of the policy
being optimized.

% Here is a sketch of an algorithm for on-policy MMD using the behavioral form update in RL terms
% similar to

% \begin{alprocedure}
% 	\algorithmcfname{MMD-BF}\label{alg:mmdbf}

% \end{alprocedure}

% \textbf{Gradient of MMD}

In single-agent settings MMD's performance is competitive with PPO in Atari and MuJoCo
environments.
And, in the multi-agent setting the performance of tabular MMD is on par with CFR, but worse than
CFR+.

\subsection{Equivalence of MMD and MDPO}

- MDPO with an added entropy term is equivalent to MMD with negative entropy mirror map and
uniform magnet.