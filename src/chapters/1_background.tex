\chapter{Background}
In this section we lay out some background on the topics relevant to this work.
We first discuss foundational concepts within reinforcement learning that serves as a base for the
rest of the discussion.
The framework of multiagent learning problems have also been conventionally rooted in Game Theory,
and hence we cover some key ideas that help establishing this consistency.
Finally, we also present some preliminary concepts from online learning and optimization algorithms
that are useful in understanding the approaches we study in this work.

\section{Game Theory}

Game theory is the mathematical study of interaction between agents to produce outcomes while
trying to uphold certain individual or group preferences.
It has a wide range of applications including economics, biology, and computer science.
In overcoming the challenges mentioned above, many algorithms have adopted game-theoretic
constrtucts and ideas when designing Reinforcement learning algorithms for multi-agent settings.
It is also a common practice to use game theoretic constructs in evaluating the performance of
multi-agent algorithms and provide theoretical guarantees.
In this work, we mainly focus on a branch of game theory called non-cooperative game theory in
which each agent has their own indivudual preference.

\subsection{Problem Representations}
\label{subsec:reps}

In discussing agent interactions, and preferences, we need a formal notion of how agents act, and
how agent preferences can be defined.
In game theory, agent preferences are formalized using Utility theory, where each agent has a
utility function that maps the agents preferences over outcomes to a real value.

The primary way of modeling problems in game theory is through a \textit{game} that encodes
information about the agents, possible actions agents can take in different situations, their
preferences, and the outcome of an interaction.
There are many types of such representations, a few relevant of which we introduce below.
Before we introduce such representations, we first cover some preliminary concepts that will be
helpful in formally defining those representations and agent preferences.

- what are utilities, and strategies?

\textbf{Normal-Form Games:}
Normal-Form games are a popular way of representing situations in which all agents act
simultaneously and the outcome is revealed after each agent has taken their action.
A few popular games that can be represented in this form are rock-paper-scissors, matching pennies,
prisoner's dilemma etc. A more formal definition of a normal-form game is as follows:

\begin{definition}[Normal-form games]

	A $(N, A, u)$ tuple
	is a n-player normal form game, where $N$ is the set of players, $A = A_1 \times A_2 \ldots \times
		A_n$, with $A_i$ being the set of actions available to player $i$, and $u = (u_i \forall i \in N)$
	is the set of utility functions that map an action profile to a real utility value for each agent,
	$u_i: A \mapsto \R$.

\end{definition}

Normal-form games are typically represented using a n-dimensional tensor, where each dimension
represents the possible actions available to each agent, and every entry represents an outcome.
The actual entries of the

An NFG that is widely popular in the literature is
the \textbf{Biased/Perturbed RPS} problem.
\blue{add details.}

\subsubsection*{Sequential Games}

Although normal-form games provide a neat representation, many real-world scenarios necessitate
agents act sequentially which is difficult to represent as a matrix.
These problems require a tree-like representation where each node is an agent's turn to make a
choice, and each edge is a possible action.
There are a few ways to represent such scenarios, one being normal-form games themselves.
A downside is that the size of the normal-form representation for sequential games explode
exponentially in the size of the game tree.
Other possible representations include the Extensive-form, and Sequence-form.

\begin{definition}[Extensive-form games]
\end{definition}

\begin{definition}[Sequence form games]
\end{definition}

\subsection{Solution Concepts}

Now that we have - what are solution concepts?
- what are the common solution concepts?
Nash equilibrium, Quantal response equilibrium.
- what are the relevant information related to solution conepts for this work?
Existence of a nash equilibrium Uniqueness of QRE

\section{Reinforcement
  Learning} Reinforcement learning (RL) is sub-domain of machine learning that deals with designing
interactive \textit{agents} that learn to maximize a \textit{reward} signal in an
\textit{environment}.
The reward signal encodes information about the goal that the agent has to learn to achieve without
any specific directions about how that goal should be achevied.
Reinforcement learning has been shown to be effective in many application domains to learn and
solve an arbitrary problem as long as it can be encapsulated into an interactive environment with a
well-defined reward signal.
\blue{ - cite RL applications}

% Motivation to study and design RL algorithms
\textbf{Markov Decision Process.}
Reinforcement learning problems are formally modeled as Markov Decision Processes (MDPs), usually
represented as a tuple $(\calS, \calA, P, R, \gamma, \mu)$ where: $\calS$ is the state space,
$\calA$ is the action space, $P(s'|s,a): \calS \times \calA \times \calS \mapsto [0, 1]$ is the
transition or the dynamics function, $R(s,a) \subset \R$ is the reward function, $\gamma \in [0,1]$
is the discount factor and $\mu$ is the initial state distribution.

The objective is to maximize the expected discounted reward:
\begin{equation}
	\label{eqn:rlobj} G_t
	= \sum_{k=0}^{\infty} \gamma^k R(s_{t+k}, a_{t+k})
\end{equation}

In learning
to take actions that maximize this objective, algorithms usually involve learning value functions
and policies.
A policy is a mapping from a state to an action distribution $\pi: \calS \mapsto \calA$, and a
value function, $V_{\pi}(s)$ estimates the expected reward that can be achieved from the current
state under the policy $\pi$: $V_{\pi}(s) = \E_{\pi}[G_t | S_t = s]$.

Now, the problem of learning a policy that maximizes the objective~\ref{eqn:rlobj} can stated in
terms of the value function as follows.

\begin{definition}[Reinforcement Learning (RL)]
	Given an MDP: $\mathcal{M} = (\calS, \calA, P, R, \gamma, \mu)$, the Reinforcement learning problem is to find an
	optimal policy mapping $\pi^*: \calS \mapsto \calA$, such that:
	$$\pi^* = \arg \max_\pi \E_{s_0 \sim \mu}[V_{\pi}(s_0)]$$
\end{definition}

There maybe more than one optimal policy, but they all share the same value function
$V_{\pi^{\ast}} = \max_{\pi} V_\pi(s), \forall s \in \calS$.

Apart from the value function, a Q-value function estimates the expected reward of taking a
specific action $a$ at a given state $s$ and then following the policy $\pi$: $ Q_{\pi}(s, a) =
	\E_{\pi}[G_t | S_t = s, A_t = a] $.
The difference between Q and V functions is referred to as the advantage function $A_\pi(s,a) =
	Q_\pi(s,a) - V_\pi(s)$.
This is the advantage of taking a particular action over following the average policy.
The objective can also framed in terms of these alternate value functions.

\textbf{Generalized Policy Iteration (GPI).}
For small, finite state spaces optimal policies can be learnt through tabular methods and dynamic
programming.
Policy iteration is an iterative method to learn the optimal policy.
The process of a policy iteration step involves both policy evaluation, and policy improvement.
Policy evaluation is done to estimate the value function for the current policy using the following
iterative update until it converges.
$$V_{k+1}(s) = \E_{\pi} [R_{t+1} + \gamma V_k(S_t+1 | S_t = s)]$$

Policy improvement then uses this estimated value function to greedily improve the current policy,
$\pi_{k+1}(s) = \arg \max_{\pi(s)} V_{k+1}(s)$.
Policy improvement always yields a strictly better policy except when the policy is already
optimal~\cite{suttonReinforcement2018}.
Requiring exact convergence of policy evaluation step might make this learning process slower.
However, this step can be truncated to only evaluate the value for the immediate states (i.e.
perform only one step of policy evaluation).
This is known as \textbf{value iteration}.
In a more general algorithm called the \textit{Generalized policy iteration (GPI)}, these steps are
run independently, and asynchronously until convergence.

For very large state spaces that is common in many practical applications, there is a need to learn
approximate value functions and parameterized policies.
This is typically done through function approximation.

\blue{
	\begin{itemize}
		\item Bellman eqn
		\item
	\end{itemize}
}

Fixed points of the Bellman quation are optimal policies and optimal value functions.
For small MDPs that strictly satisfy the Markov property, Bellman equiation can be explicity
solved.
In settings where the transitions and rewards can be represented explicitly in a tabular manner,
dynamic programming is one approach to solve the Bellman equation.
However, for most practical applications it is common to use parameterized policies and approximate
value functions.

% In formulating Reinforcement learning algorithms, there are two major approaches, and their
% derivatives.
% Most of the tabular, and iterative methods require an
% explicit model of an environment with access to the transition functions.
% In the absence of a model or transition functions, one can use Monet Carlo methods to approximate
% these probabilities through sampling.
% This method of learning from experience by iteracting with a blackbox enivornment or a simulator is
% the setting in most of the recent efforts in machine learning.
% This is because it is difficult to explictly design a model that accurately captures all the
% properties of a real-world artifact, and exhaustively model the effects of taking all the actions
% that are available to the agent.
% It is also sometimes impossible or unknown.
% However, it is possible to have access to such a real-world artifact, and we only need to provide
% an interface for the agent to the artifact and expose available actions to the agent.
% We can use the real-world artifact to evaluate these actions and their effects and convert them
% into rewards to provide a signal for the agent to learn from.

\subsection{Multi-agent Reinforcement Learning (MARL)}\label{sec:marl}
Reinforcement learning algorithms have also been studied in settings that have more than one agent
where the agents try to maximize some objective in a cooperative or competitive manner.
Athough many of these algorithms have exhibited strong empirical performance in multi-agent
settings, there are a few key challenges when extending Reinforcement learning algorithms to
multi-agent settings.
From a theoretical perspective, the foundational assumption for RL algorithms is that the MDP's
transition dynamics remain stationary.
However, in multi-agent settings non-stationary is induced due to the presence of the other agents.
This makes it difficult in adapting the algorithms disucssed above directly to multi-agent
settings.
From an algorithm design perspective, the action space explodes exponentially in multi-agent
settings making it computationally challenging to apply reinforcement algorithms without
decomposing the problem into more managable sub-problems first.

\section{Online Learning and Mirror Descent}

Online learning is the study of designing algorithms that use historical knowledge in making
predictions for future rounds while trying to minimize some loss function in an adaptive (possibly
adversarial) setting.

% Why is it relevant?
The problem of training agents in a potentially non-stationary setting can be cast into an online
learning problem.
Hence, it is useful to study online learning algorithms from the perspective of designing
reinforcement learning algorithms as they provide a framework for the analysis of the algorithms
and deriving theoretical guarantees.
% In this work, we also study algorithms that can be derived from an online-learning perspective, and
% also can be shown equivalent to popular online learning algorithms.
The brief background provided in this section closely follows the details as presented
in~\cite{shalev-shwartzOnline2012}.
For a more in-depth introduction into Online learning and Online Convex Optimization, please refer
to the above work.

% Definition
In Online Learning, a learner is tasked with predicting the answer to a set of questions over a
sequence of consecutive rounds.
We now define an Online learning problem more formally as follows:

\begin{definition}
	\label{def:olearning} For each round $t$, given an instance $x_t \in \calX$, and
	a prediction $p_t \in \calY$, a loss function $l(p_t, y_t) \mapsto \R$
\end{definition}

At each round t, a question $x_t$ is taken from an instance domain $\calX$, and
the learner is required to predict an answer, $p_t$ to this question.
After the prediction is made, the correct answer $y_t$, from a target domain $\calY$ is revealed
and the learner suffers a loss $l(p_t, y_t)$.
The prediction $p_t$ could belong to $\calY$ or a larger set, $\mathcal{D}$.

% Assumptions around the problem definition
% It is known that without any further assumptions about the learning problem, a learner's regret can
% be unbounded in an adversarial setting~\cite{coverBehavior1965}.

% A popular assumption in Online learning settings is called Realizability, where we assume the
% target mapping arises from a fixed hypothesis class $\calH$.
The main aim of an online learning algorithm $A$, is to minimize the cumulative regret of a learner
with respect to the best competing hypothesis $h^*$ from the assumed hypothesis class $\calH$.

\begin{equation}
	\label{eqn:regret}
	Regret_T(h^*) = \sum_{t=1}^T l(p_t, y_t) - \sum_{t=1}^T l(h^*(x_t), y_t),
\end{equation}

The regret of $A$ with $\calH$ is,

\begin{equation}
	\label{eqn:regret_all} Regret_T(\calH) = \max_{h^* \in \calH} Regret_T(h^*)
\end{equation}

A popular framework for studying and designing Online learning algorithms is
through Convex optimization.
The assumptions made around the framework provides properties that are useful in deriving
convergence guarantees.
We define a few terms below that are used in this section and the following ones.

% Convex sets and Convex functions

\begin{definition}[Strongly Smooth]~\label{def:strsmooth}
	Given a convex set $\calX \in \R^n$, a convex function $f: \calX \mapsto \R$ is $\sigma$-strongly
	smooth with respect to a norm $\|.
		\|$, if $\|\nabla f(x) - \nabla f(y) \|_{\ast} \leq \sigma \|x - y \|, \forall x, y \in \calX$.
	For a given constant $L$, this is also referred to as $L-smooth$.
\end{definition}

% Strongly smooth/L-smooth

% Monotonicity/Strong monotonicity

The typical structure of online learning expressed as an online convex optimization problem is as
follows:

\begin{alprocedure}[H] \algorithmcfname{OCO}~\label{alg:oco}

	input: a convex set S for t = 1, 2, $\ldots$

	predict a
	vector $w_t \in S$

	receive a convex loss function $f_t: S \mapsto \mathbb{R}$
\end{alprocedure}

Reframing~\ref{eqn:regret} in terms of convex optimization,
we refer to a competing hypothesis here as some vector $u$ from the convex set $S$.

\begin{equation}
	Regret_T(u) = \sum_{t=1}^T f_t(w_t) - \sum_{t=1}^T f_t(u)
\end{equation}

and
similarly, the regret with respect to a set of competing vectors $U$ is,
\begin{equation}
	Regret_T(U) = \max_{u \in U} Regret_T(u)
\end{equation}

The set $U$ can be same
as $S$ or different in other cases.
Here we assume $U=S$ and $S=\mathbb{R^d}$ unless specified otherwise.

\subsection{FoReL}\label{sec:forel}

Follow-the-Regularized-leader (FoReL) is a classic online learning algorithm that acts as a base in
deriving various regret mimization algorithms.
The idea of FoReL is to include a regularization term to stabilize the updates in each iteration
leading to better convergence behaviors.

The learning rule can be written as,

$$\forall t, w_t = argmin_{w \in S}
	\sum_{i=1}^{t-1} f_i(w) + R(w).
$$

where $R(w)$ is the regularization term.
The choice of the regularization function lead to different algorithms with varying regret bounds.

\subsection{Gradient Descent}

In the case of linear loss functions with respect to some $z_t$, i.e., $f_t(w) = \langle w, z_t
	\rangle$, and $S=\mathbb{R}^d$, if FoReL is run with $l_2$-norm regularization $R(w) = \frac{1}{2
		\eta} \|w\|_2^2$, then the learning rule can be written as,

\begin{equation}
	w_{t+1} = -\eta \sum_{i=1}^t z_i = w_t - \eta z_t
\end{equation}

Since, $\nabla
	f_t(w_t) = z_t$, this can also be written as, $w_{t+1} = w_t - \eta \nabla f_t(w_t)$.
This update rule is also commonly known as Online Gradient Descent.
The regret of FoReL run on Online linear optimization with a euclidean-norm regularizer is:

$$Regret_T(U) \leq BL \sqrt {2T}.
$$

where $U = {u : \|u\| \leq B}$ and $\frac{1}{T} \sum_{t=1}^T \|z_t\|_2^2 \leq L^2$ with $\eta = \frac{B}{L\sqrt{2T}}$.

Beyond Euclidean regularization, FoReL can also be run with other regularization functions and
yield similar regret bounds given that the regularization functions are strongly convex.

\begin{definition}
	For any $\sigma$-strongly-convex function $f: S \mapsto \mathbb{R}$ with respect to a norm $\|.
		\|$, for any $w \in S$,
	\begin{equation}
		\forall z \in \partial f(w), \forall u \in S, f(u) \geq f(w) + \langle z, u - w\rangle + \frac{\sigma}{2}\| u - w \|^2.
	\end{equation}
\end{definition}

\begin{lemma}
	\label{lem:forelrb}
	For a FoReL algorithm producing a sequence of vectors $w_1, \ldots, w_T$ with a sequence of loss
	functions $f_1, \ldots, f_T$, for all $u \in S$, $$\sum_{t=1}^T (f_t(w_t) - f_t(u)) \leq R(u) -
		R(w_1) + \sum_{t=1}^T (f_t(w_t) - f_t(w_{t+1}))$$
\end{lemma}

\subsection{Hedge} - what is hedge?

Gradient descent as a FTRL variant

\section{Mirror Descent}

In this section we discuss about Mirror Descent, and two mirror descent-based reinforcement
learning algorithms (MDPO, and MMD).
Mirror descent is a popular first order optimization algorithm that has seen wide applications in
machine learning, and reinforcement learning.

There are different views of arriving at Mirror Descent as an optimization algorithm, here we
present the Mirror Descent framework through the lens of mirror maps.

\subsection{Mirror Maps}

% Define Bregman divergence
\begin{definition}
	\label{def:bregman}
	Given a convex set $\calX \subset \R^n$, and a differentiable convex function $f: \calX \mapsto
		\R$, the Bregman Divergence associated with the function $f$ is defined as,

	$$
		D_f(x, y) = f(x) - f(y) - \nabla f(y)^T (x-y).
	$$
\end{definition}

For an alternate view of deriving Mirror Descent as an improvement of FoReL, please refer
to~\cite{shalev-shwartzOnline2012}[Section 2.6].
% A disadvantage of FoReL~\ref{sec:forel} in solving online learning problems is that, there is a
% minimization that must be performed at every step.
% Mirror descent overcomes this by using a recursive update rule that does not required us to perform
% a minimization at every step.