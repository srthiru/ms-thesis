\chapter{Background}
This work mainly discusses algorithms that are present in the intersection of three subject areas
namely, Reinforcement learning, Game Theory, and Online Learning.
We provide some brief background to relevant concepts required to follow the ideas discussed in the following sections 
and point to more comprehensive resources when applicable.

\section{Reinforcement
  Learning}

% What is it?
Reinforcement learning (RL) is sub-domain of machine learning that deals with designing interactive
agents that learn to maximize a reward signal in an environment.
The reward signal encodes information about the goal that the designer wants the agent to learn to
achieve without informing anything about how that goal should be achevied.
Reinforcement learning has been shown to be
effective in many application domains to learn and solve an arbitrary problem as long as it can be
encapsulated into an interactive environment with a well-defined reward signal.
\blue{ - cite RL applications}

% Motivation to study and design RL algorithms
\textbf{Markov Decision Process.} Reinforcement learning problems are formally modeled as Markov Decision Processes (MDPs), that represented 
as a 
tuple $(\calS, \calA, P, R, \gamma, \mu)$ where: $\calS$ is the state space, $\calA$ is the action space, 
$P(s'|s,a): \calS \times \calA \times \calS \mapsto [0, 1]$ is the transition or the dynamics function, 
$R(s,a) \subset \R$ is the reward function,  
$\gamma \in [0,1]$ is the discount factor and $\mu$ is the initial state distribution.

The objective is to maximize the expected discounted reward:
$$G_t = \sum_{k=0}^{\infty} \gamma^k R(s_{t+k}, a_{t+k})$$

In learning to take actions that maximize this objective, algorithms usually involve learning value functions 
and policies. A policy is a mapping from a state to an action distribution $\pi: \calS \mapsto \calA$, and a value 
function $V_{\pi}(s)$ estimates the expected reward that can be achieved from the current state by following a 
given policy: $V_{\pi}(s) = \E_{\pi}[G_t | S_t = s]$.

The objective can then be reformulated as to find a policy that maximizes the value of the initial state under 
the starting distribution $\mu$:
$$\max_\pi \E_{s_0 \sim \mu}[V_{\pi}(s_0)]$$

Action-value or Q-value  function estimates the expected reward of taking a specific action $a$ at a given 
state $s$ and then following the policy $\pi$: $ Q_{\pi}(s, a) = \E_{\pi}[G_t | S_t = s, A_t = a] $.
The difference between Q and V functions is referred to as the advantage function $A_\pi(s,a) = Q_\pi(s,a) - V_\pi(s)$. 
This is the advantage of taking a particular action over following the average policy.
The \textbf{optimal policy} $\pi^{\ast}$ is one that maximizes the value function. There maybe more than one optimal policy, but 
they all share the same value function $V_{\pi^{\ast}} = \max_{\pi} V_\pi(s), \forall s \in \calS$.

\textbf{Generalized Policy Iteration (GPI).} For small, finite state spaces optimal policies can be learnt through tabular methods and dynamic programming. 
Policy iteration is an iterative method that employs policy evaluation, and improvement steps to learn the optimal policy. 
The policy evaluation step learns the value function for the current policy using the following iterative update 
until it converges.
$$V_{k+1}(s) = \E_{\pi} [R_{t+1} + \gamma V_k(S_t+1 | S_t = s)]$$

Policy improvement step then uses the value function to greedily improve the current policy, $\pi_{k+1}(s) = \arg \max_a Q_{k}(s,a)$.
Policy improvement always yields a strictly better policy except when the policy is already optimal~\cite{suttonReinforcement2018}.
Exact convergence of policy evaluation might make the learning process slower. 
This step can be truncated to only evaluate the value for the immediate states (i.e. perform only one step of policy evaluation).
This is known as \textbf{value iteration}.
In \textit{Generalized policy iteration (GPI)}, these steps are run independently, and asynchronously until convergence.

For very large state spaces that is common in many practical applications, there is a need to learn approximate 
value functions and parameterized policies. This is typically done through function approximation. 

\blue{
	\begin{itemize}
		\item Bellman eqn
		\item 
	\end{itemize}
}

Fixed points of the Bellman quation are optimal policies and optimal value functions.
For small MDPs that strictly satisfy the Markov property, Bellman equiation can be explicity
solved.
In settings where the transitions and rewards can be represented explicitly in a tabular manner,
dynamic programming is one approach to solve the Bellman equation.
However, for most practical applications it is common to use parameterized policies and approximate
value functions.

In formulating Reinforcement learning algorithms, there are two major approaches, and their
derivatives.
% Most of the tabular, and iterative methods require an
% explicit model of an environment with access to the transition functions.
% In the absence of a model or transition functions, one can use Monet Carlo methods to approximate
% these probabilities through sampling.
% This method of learning from experience by iteracting with a blackbox enivornment or a simulator is
% the setting in most of the recent efforts in machine learning.
% This is because it is difficult to explictly design a model that accurately captures all the
% properties of a real-world artifact, and exhaustively model the effects of taking all the actions
% that are available to the agent.
% It is also sometimes impossible or unknown.
% However, it is possible to have access to such a real-world artifact, and we only need to provide
% an interface for the agent to the artifact and expose available actions to the agent.
% We can use the real-world artifact to evaluate these actions and their effects and convert them
% into rewards to provide a signal for the agent to learn from.

\label{sec:spg} \subsection{Policy gradient methods}\label{sec:pg}

Policy gradient method involve directly learning a parameterized policy that enables action selection 
without the use of a value function.
These are generally called Policy gradient methods, and are a major area of study within Reinforcement learning.
Policy gradient methods sometimes have the advantage that the policy space could be simpler to
learn compared to the value function space.
In this case, the policy is parameterized by $\theta \in \Theta$ and $\pi(s, a) = P(s, a; \theta)$.

Given some objective $J(\theta)$, these methods seek to learn the parameters that maximize this objective through 
gradient ascent.


One key challenge in Policy gradient methods is that the evaluation of performance of a policy
depends on the state distribution which could be unknown.
However, the Policy Gradient Theorem establishes that the gradient is independent of underlying
environment's state distribution as long as it is stationary conditioned on the current policy.

The most popular policy gradient method is Reinforce, which is a Monte Carlo policy gradient
method.

A value function may still be used in guiding the policy learning, and these are called actor-critic methods.
Here the actor refers to the policy, and the critic refers to the value function that evaluates the
action taking by the policy guiding the policy learning.

\textbf{Reinforce}: 
\blue{
	- Widely popular PG algorithm, many insantiations possible including variants with baselines to
	reduce variance.
	- Convergence is proven using PG theorem.
}

\textbf{Softmax Policy Gradients}
\blue{
	- Parametrizing policies
	- PG with softmax parameterization is referred to as Softmax Policy gradients.
	- General significance of softmax
}


\subsection{PPO}

- Trust region methods
- PPO an approximation of TRPO with hueristic objective

\subsection{Multi-agent Reinforcement Learning (MARL)}\label{sec:marl}

There are a few key challenges in extending Reinforcement learning algorithms to multi-agent
settings.
From a theoretical perspective, many of the function approximation based algorithms assume that the
state distribution $\mu(s)$ remains stationary for a given policy.
However, in multi-agent settings the state distribution can become non-stationary due to the
changes in the behavior of the other agents.
This makes it difficult in adapting the algorithms disucssed above directly to multi-agent
settings.

From an algorithm design perspective, the action space explodes exponentially in multi-agent
settings making it computationally challenging to apply reinforcement algorithms without
decomposing the problem into more managable sub-problems first.

\section{Game Theory}

Game theory is the mathematical study of interaction between agents to produce outcomes while
trying to uphold certain individual or group preferences.
It has a wide range of applications including economics, biology, and computer science.
In overcoming the challenges mentioned above, many algorithms have adopted game-theoretic
constrtucts and ideas when designing Reinforcement learning algorithms for multi-agent settings.
It is also a common practice to use game theoretic constructs in evaluating the performance of
multi-agent algorithms and provide theoretical guarantees.
In this work, we mainly focus on a branch of game theory called non-cooperative game theory in
which each agent has their own indivudual preference.

\subsection{Problem Representations}
\label{subsec:reps}

In discussing agent interactions, and preferences, we need a formal notion of how agents act, and
how agent preferences can be defined.
In game theory, agent preferences are formalized using Utility theory, where each agent has a
utility function that maps the agents preferences over outcomes to a real value.

The primary way of modeling problems in game theory is through a \textit{game} that encodes
information about the agents, possible actions agents can take in different situations, their
preferences, and the outcome of an interaction.
There are many types of such representations, a few relevant of which we introduce below.
Before we introduce such representations, we first cover some preliminary concepts that will be
helpful in formally defining those representations and agent preferences.

- what are utilities, and strategies?

\textbf{Normal-Form Games:} Normal-Form games are a popular way of representing situations in which all agents act
simultaneously and the outcome is revealed after each agent has taken their action.
A few popular games that can be represented in this form are rock-paper-scissors, matching pennies,
prisoner's dilemma etc. A more formal definition of a normal-form game is as follows:

\begin{definition}[Normal-form games]

	A $(N, A, u)$ tuple
	is a n-player normal form game, where $N$ is the set of players, $A = A_1 \times A_2 \ldots \times
		A_n$, with $A_i$ being the set of actions available to player $i$, and $u = (u_i \forall i \in N)$
	is the set of utility functions that map an action profile to a real utility value for each agent,
	$u_i: A \mapsto \R$.

\end{definition}

Normal-form games are typically represented using a n-dimensional tensor, where each dimension
represents the possible actions available to each agent, and every entry represents an outcome.
The actual entries of the

An NFG that is widely popular in the literature is the \textbf{Biased/Perturbed RPS} problem. \blue{add details.}

\subsubsection*{Sequential Games}

Although normal-form games provide a neat representation, many real-world scenarios necessitate
agents act sequentially which is difficult to represent as a matrix.
These problems require a tree-like representation where each node is an agent's turn to make a
choice, and each edge is a possible action.
There are a few ways to represent such scenarios, one being normal-form games themselves.
A downside is that the size of the normal-form representation for sequential games explode
exponentially in the size of the game tree.
Other possible representations include the Extensive-form, and Sequence-form.

\begin{definition}[Extensive-form games]
\end{definition}

\begin{definition}[Sequence form games]
\end{definition}

\subsection{Solution Concepts}

Now that we have - what are solution concepts?
- what are the common solution concepts?
Nash equilibrium, Quantal response equilibrium.
- what are the relevant information related to solution conepts for this work?
Existence of a nash equilibrium Uniqueness of QRE

\section{Online Learning and Mirror Descent}

Online learning is the study of designing
algorithms that use historical knowledge in making predictions for future rounds while trying to
minimize some loss function in an adaptive (possibly adversarial) setting.

% Why is it relevant?
The problem of training agents in a potentially non-stationary setting can be cast into an online
learning problem.
Hence, it is useful to study online learning algorithms from the perspective of designing
reinforcement learning algorithms as they provide a framework for the analysis of the algorithms
and deriving theoretical guarantees.
% In this work, we also study algorithms that can be derived from an online-learning perspective, and
% also can be shown equivalent to popular online learning algorithms.
The brief background provided in this section closely follows the details as presented
in~\cite{shalev-shwartzOnline2012}.
For a more in-depth introduction into Online learning and Online Convex Optimization, please refer
to the above work.

% Definition
In Online Learning, a learner is tasked with predicting the answer to a set of questions over a
sequence of consecutive rounds.
We now define an Online learning problem more formally as follows:

\begin{definition}
	\label{def:olearning} For each round $t$, given an instance $x_t \in \calX$, and
	a prediction $p_t \in \calY$, a loss function $l(p_t, y_t) \mapsto \R$
\end{definition}

At each round t, a question $x_t$ is taken from an instance domain $\calX$, and
the learner is required to predict an answer, $p_t$ to this question.
After the prediction is made, the correct answer $y_t$, from a target domain $\calY$ is revealed
and the learner suffers a loss $l(p_t, y_t)$.
The prediction $p_t$ could belong to $\calY$ or a larger set, $\mathcal{D}$.

% Assumptions around the problem definition
% It is known that without any further assumptions about the learning problem, a learner's regret can
% be unbounded in an adversarial setting~\cite{coverBehavior1965}.

% A popular assumption in Online learning settings is called Realizability, where we assume the
% target mapping arises from a fixed hypothesis class $\calH$.
The main aim of an online learning algorithm $A$, is to minimize the cumulative regret of a learner
with respect to the best competing hypothesis $h^*$ from the assumed hypothesis class $\calH$.

\begin{equation}
	\label{eqn:regret}
	Regret_T(h^*) = \sum_{t=1}^T l(p_t, y_t) - \sum_{t=1}^T l(h^*(x_t), y_t),
\end{equation}

The regret of $A$ with $\calH$ is,

\begin{equation}
	\label{eqn:regret_all} Regret_T(\calH) = \max_{h^* \in \calH} Regret_T(h^*)
\end{equation}

A popular framework for studying and designing Online learning algorithms is
through Convex optimization.
The assumptions made around the framework provides properties that are useful in deriving
convergence guarantees.
We define a few terms below that are used in this section and the following ones.

% Convex sets and Convex functions

\begin{definition}[Strongly Smooth]~\label{def:strsmooth}
	Given a convex set $\calX \in \R^n$, a convex function $f: \calX \mapsto \R$ is $\sigma$-strongly
	smooth with respect to a norm $\|.
		\|$, if $\|\nabla f(x) - \nabla f(y) \|_{\ast} \leq \sigma \|x - y \|, \forall x, y \in \calX$.
	For a given constant $L$, this is also referred to as $L-smooth$.
\end{definition}

% Strongly smooth/L-smooth

% Monotonicity/Strong monotonicity

The typical structure of online learning expressed as an online convex optimization problem is as
follows:

\begin{alprocedure}[H] \algorithmcfname{OCO}~\label{alg:oco}

	input: a convex set S for t = 1, 2, 
	$\ldots$

	predict a vector $w_t \in S$

	receive a convex loss function $f_t: S \mapsto \mathbb{R}$
\end{alprocedure}

Reframing~\ref{eqn:regret} in terms of convex optimization, we refer to a competing hypothesis here
as some vector $u$ from the convex set $S$.

\begin{equation}
	Regret_T(u) = \sum_{t=1}^T f_t(w_t) - \sum_{t=1}^T f_t(u)
\end{equation}

and
similarly, the regret with respect to a set of competing vectors $U$ is, \begin{equation}
	Regret_T(U) = \max_{u \in U} Regret_T(u)
\end{equation}

The set $U$ can be same
as $S$ or different in other cases.
Here we assume $U=S$ and $S=\mathbb{R^d}$ unless specified otherwise.

\subsection{FoReL}\label{sec:forel}

Follow-the-Regularized-leader (FoReL) is a classic online learning algorithm that acts as a base in
deriving various regret mimization algorithms.
The idea of FoReL is to include a regularization term to stabilize the updates in each iteration
leading to better convergence behaviors.

The learning rule can be written as,

$$\forall t, w_t = argmin_{w \in S}
	\sum_{i=1}^{t-1} f_i(w) + R(w).
$$

where $R(w)$ is the regularization term.
The choice of the regularization function lead to different algorithms with varying regret bounds.

\subsection{Gradient Descent}

In the case of linear loss functions with respect to some $z_t$, i.e., $f_t(w) = \langle w, z_t
	\rangle$, and $S=\mathbb{R}^d$, if FoReL is run with $l_2$-norm regularization $R(w) = \frac{1}{2
		\eta} \|w\|_2^2$, then the learning rule can be written as,

\begin{equation}
	w_{t+1} = -\eta \sum_{i=1}^t z_i = w_t - \eta z_t
\end{equation}

Since, $\nabla
	f_t(w_t) = z_t$, this can also be written as, $w_{t+1} = w_t - \eta \nabla f_t(w_t)$.
This update rule is also commonly known as Online Gradient Descent.
The regret of FoReL run on Online linear optimization with a euclidean-norm regularizer is:

$$Regret_T(U) \leq BL \sqrt {2T}.
$$

where $U = {u : \|u\| \leq B}$ and $\frac{1}{T} \sum_{t=1}^T \|z_t\|_2^2 \leq L^2$ with $\eta = \frac{B}{L\sqrt{2T}}$.

Beyond Euclidean regularization, FoReL can also be run with other regularization functions and
yield similar regret bounds given that the regularization functions are strongly convex.

\begin{definition}
	For any $\sigma$-strongly-convex function $f: S \mapsto \mathbb{R}$ with respect to a norm $\|.
		\|$, for any $w \in S$,
	\begin{equation}
		\forall z \in \partial f(w), \forall u \in S, f(u) \geq f(w) + \langle z, u - w\rangle + \frac{\sigma}{2}\| u - w \|^2.
	\end{equation}
\end{definition}

\begin{lemma}
	\label{lem:forelrb}
	For a FoReL algorithm producing a sequence of vectors $w_1, \ldots, w_T$ with a sequence of loss
	functions $f_1, \ldots, f_T$, for all $u \in S$, $$\sum_{t=1}^T (f_t(w_t) - f_t(u)) \leq R(u) -
		R(w_1) + \sum_{t=1}^T (f_t(w_t) - f_t(w_{t+1}))$$
\end{lemma}

\subsection{Hedge} - what is hedge?

Gradient descent as a FTRL variant


\section{Mirror Descent}

In this section we discuss about Mirror Descent, and two mirror descent-based reinforcement
learning algorithms (MDPO, and MMD).
Mirror descent is a popular first order optimization algorithm that has seen wide applications in
machine learning, and reinforcement learning.

There are different views of arriving at Mirror Descent as an optimization algorithm, here we
present the Mirror Descent framework through the lens of mirror maps.

\subsection{Mirror Maps}

% Define Bregman divergence
\begin{definition}
	\label{def:bregman}
	Given a convex set $\calX \subset \R^n$, and a differentiable convex function $f: \calX \mapsto
		\R$, the Bregman Divergence associated with the function $f$ is defined as,

	$$
		D_f(x, y) = f(x) - f(y) - \nabla f(y)^T (x-y).
	$$
\end{definition}

For an alternate view of deriving Mirror Descent as an improvement of FoReL, please refer
to~\cite{shalev-shwartzOnline2012}[Section 2.6].
% A disadvantage of FoReL~\ref{sec:forel} in solving online learning problems is that, there is a
% minimization that must be performed at every step.
% Mirror descent overcomes this by using a recursive update rule that does not required us to perform
% a minimization at every step.