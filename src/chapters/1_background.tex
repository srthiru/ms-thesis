\chapter{Background}
In this section we lay out some background on the topics relevant to this work.
We first discuss foundational concepts within reinforcement learning that serves as a base for the
rest of the discussion.
The framework of multiagent learning problems have also been conventionally rooted in Game Theory,
and hence we cover some key ideas that help establishing this consistency.
Finally, we also present some preliminary concepts from online learning and optimization algorithms
that are useful in understanding the approaches we study in this work.

\section{Game Theory}

Game theory is the mathematical study of interaction between agents to produce outcomes while
trying to uphold certain individual or group preferences.
It has a wide range of applications including economics, biology, and computer science.
% In overcoming the challenges mentioned above, many algorithms have adopted game-theoretic
% constrtucts and ideas when designing Reinforcement learning algorithms for multi-agent settings.
% It is also a common practice to use game theoretic constructs in evaluating the performance of
% multi-agent algorithms and provide theoretical guarantees.
In this work, we mainly focus on a branch of game theory called non-cooperative game theory that
assumes each agent has their own indivudual preference to uphold as opposed to a global preference.

\subsection{Problem Representations}
\label{subsec:reps}
% In discussing agent interactions, and preferences, we need a formal notion of how agents act, and
% how agent preferences can be defined.
In game theory, the ordering of an agent's preferences over certain outcomes are formalized through
a Utility or a \textit{Payoff function} that maps outcomes to a real value.
Problems are commonly represented as a \textit{Game} that encodes information about the agents,
possible actions agents can take in different situations, their preferences, and the outcome of a
specific interaction.
A \textit{Strategy} typically refers to the agent's choice of taking actions given the information
it has.
Agent's strategies can be \textit{pure} (deterministic) or \textit{mixed} (distribution over
possible actions).
Beyond these preliminary notions, there are different representations of games depending on the
structure of the problem.
We now briefly describe two common representations that are relevant to this work.
For a more in-depth discussion of game representations, and their significance we refer the reader
to~\cite{shohamMultiagent2008}.

\textbf{Normal-Form Games:}
Normal-Form or Strategic form games are the most basic and foundational form of representing
problems in game theory.
In a Normal-form game (NFG), all agents act simultaneously to reveal the outcome immediately after,
and a payoff is assigned to each agent.
A more formal definition of a normal-form game is as follows:

\begin{definition}[Normal-form games]

	A n-player, general-sum normal form game
	is given by the $(N, A, u)$ tuple, where $N$ is the set of players, $A = A_1 \times A_2 \ldots
		\times A_n$, with $A_i$ being the set of actions available to player $i$, and $u = (u_i \forall i
		\in N)$ is the set of utility functions that map an action profile to a real utility value for each
	agent, $u_i: A \mapsto \R$.

\end{definition}

Normal-form games are typically represented using an n-dimensional matrix, where each dimension
represents an action available to each agent, and each entry is the payoff assigned to each agent
given that action profile.
A few popular examples of NFGs are Rock-paper-scissors (RPS), Matching pennies, and Prisoner's
dilemma etc.

\subsubsection*{Sequential Games} Normal-form games are limited in
representing many real-world problems that necessitate agents to act sequentially along the
temporal dimension.
A common choice in representing such problems is the Extensive-form game which facilitiates
sequential decision making.

\begin{definition}[Extensive-form games]
\end{definition}

Given their tree-like representation, EFGs can be inefficient to operate over.
Reduced-form NFGs are the normal-form representation of an EFG, where each row/column represents a
pure-strategy of the entire EFG, instead of just a single action.
Given this exponential blowup of action space, reduced-form NFGs are also computationally
challenging to work with.
Another useful representation is the Sequence-form, where instead of the entire pure-strategy,
entries represent histories of actions taken so far.
% This leads to a much more compact representation, though at the cost of additional complexity in
% updating the representation as new actions are taken.

\begin{definition}[Sequence form games]
\end{definition}

% There are a few ways to represent such scenarios, one being normal-form games themselves.
% A downside is that the size of the normal-form representation for sequential games explode
% exponentially in the size of the game tree.

\subsection{Solution Concepts}
While the above representations provide a well-defined framework to encode information about the
problem, solution concepts define a notion of optimality of the agent's strategies given these
representations.
The main premise is identifying an \textit{equilibrium} point in the joint strategy space of all
the agents.
However, how this equlibrium point is defined, and what guarantees it provides leads to various
definitions of equilibrium points.

\fillin{
	\begin{itemize}
		\item Nash equilibrium - Existence of a nash equilibrium
		\item Quantal response equilibrium - Uniqueness of QRE
	\end{itemize}
}

% \subsection{Learning Strategies}

\section{Reinforcement
  Learning}
Reinforcement learning (RL) is sub-domain of machine learning that deals with designing interactive
\textit{agents} that learn to maximize a \textit{reward} signal in an \textit{environment}.
The reward signal encodes information about the goal that the agent has to learn to achieve without
any specific directions about how that goal should be achevied.
% Reinforcement learning has been shown to be effective in many application domains to learn and
% solve an arbitrary problem as long as it can be encapsulated into an interactive environment with a
% well-defined reward signal.
% Motivation to study and design RL algorithms
% \fillin{ - cite RL applications}.

\textbf{Markov Decision Process.}
Markov Decision Processes (MDPs) provide a framework to formalize reinforcement learning problems.
A $\gamma$-discounted MDP ($\mathcal{M}$) is given by the tuple, $\mathcal{M}=(\calS, \calA, P, R,
	\gamma)$ where: $\calS$ is the state space, $\calA$ is the action space,
$P(s',r|s,a):~\calS\times\calA\times\calS~\mapsto~[0,1]$ is the transition dynamics function,
$R(s,a) \subset \R$ is the reward function, $\gamma \in [0,1]$ is the discount factor.

The objective of an agent interacting with this MDP is to learn to act in a way that maximize its
\textit{expected return} at each state.
% While $R_t(s_t, a_t)$ is the immediate reward, to prevent the agent from becoming myopic of future 
% rewards, the alternate form discounted reward is used in practice.
% To encode the consequence of taking specific actions, and to prevent the agent from becoming myopic of 
% future rewards, the discounted reward $G_t = \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k$ is commonly used rather 
% than the immediate reward $R_t$.
The expected return, $G_t$, at time step $t$ is the discounted sum of rewards accumulated by the
agent starting from $t$ till the end of the episode (\fillin{episodes?
}).
Formally, $G_t = \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k$, where the discount rate $\gamma$ indicates
how much importance is placed upon future rewards over immediate rewards.
% Discounting also helps in keeping the return bounded in infinite horizon settings.
The objective of an agent within an MDP is to learn to act at each state so as to maximize their
\textit{expected returns}.

% In addition to rewards, and returns, reinforcement learning also involves the notion of policies
% and value functions.
While the expected return quantifies the objective, policy and value functions are representations
that dictate how the agent should act in a given state.
Reinforcement learning algorithms typically involve learning polices and estimating value
functions.
A \textit{Policy} is a mapping from a state to an action distribution $\pi: \calS \mapsto \calA$.
A \textit{Value function}, $V_{\pi}(s)$ estimates the expected reward that can be achieved from the
current state under the policy $\pi$: $V_{\pi}(s) = \E_{\pi}[G_t | S_t = s]$.
Analogously, the \textit{Q-value function} estimates the expected reward of taking a specific
action $a$ at a given state $s$ and then following the policy $\pi$: $ Q_{\pi}(s, a) = \E_{\pi}[G_t
		| S_t = s, A_t = a] $.
The difference between Q and V functions is referred to as the advantage function $A_\pi(s,a) =
	Q_\pi(s,a) - V_\pi(s)$.
This is the advantage of taking a particular action over following the average policy.

Value functions impose a ranking over policies in terms of the expected return achieved by an agent
following a given policy.
An \textit{optimal policy} ($\pi^\ast$) is one that is atleast as better as any other policy ($\pi
	\in \Pi$) in terms of this ranking, i.e. $V_{\pi^\ast}(s) \geq V_\pi(s), \forall s \in \calS$.
While there maybe more than one optimal policy, they all share the same unique \textit{optimal
	value function}: $V_{\pi^{\ast}} = \max_{\pi} V_\pi(s), \forall s \in \calS$.

\subsection{Tabular methods}
For a given policy, the value function satisfies the \textbf{Bellman equation}:
\begin{equation}
	\label{eqn:bellman} V_\pi(s) \doteq \E_\pi \left[r + \gamma V_\pi(s') | R_t = r, S_{t+1} = s'
		\right]
\end{equation}

For small state spaces, it is possible to learn optimal
value functions by iterating using Dynamic programming.
Starting with an arbitrary policy, we can learn an optimal policy interleaving Policy evaluation,
and Policy improvement.
In the \textit{Policy Evaluation} step, we first estimate the value function until it
satisfies~\ref{eqn:bellman}.
Next, in the \textit{Policy Improvement} step, we update the policy greedily at every state until
we acheive a strictly better policy.
This method of interleaving these two steps is called \textit{Policy Iteration}.
Alternatively, \textit{Value Iteration} approximates this improvement by only updating the policy
greedily with respect to the immediate next step.

At step $k$ of learning a tabular policy,
\begin{itemize}
	\item \textbf{Policy Evaluation}:
	      $v_{k+1}(s) \doteq \E_\pi \left[R_{t+1} + \gamma v_k(S_{t+1}) |S_t =s\right]$
	\item \textbf{Value
		      Iteration}: $v_{k+1}(s) \doteq \E_\pi \left[R_{t+1} + \gamma v_k(S_{t+1}) |S_t =s\right]$
	\item
	      \textbf{Policy Iteration}:
\end{itemize}

We can construct an optimal policy
simply by acting greedily with respect to the optimal value function.

Most of the tabular, and iterative methods require an explicit model of an environment with access
to the transition functions.
In the absence of a model or transition functions, one can use Monet Carlo methods to approximate
these probabilities through sampling.

Tabular methods are realizable in settings with small state spaces, where we have access to the
perfect model of the environment in the form of MDPs.
Tabular methods are also useful in establishing theory and guarantees for various algorithms.
However, for most practical applications it is common to use parameterized policies and approximate
value functions.

\subsection{Approximate Value-based methods}
% This method of learning from experience by iteracting with a blackbox enivornment or a simulator is
% a common problem setting in many reinforcement learning problems.
% This is because it is difficult to explictly design a model that accurately captures all the
% properties of a real-world artifact, and exhaustively model the effects of taking all the actions
% that are available to the agent (which are also sometimes unknown).
% However, it is possible to have access to such a real-world artifact, and we only need to provide
% an interface for the agent to the artifact and expose available actions to the agent.
% We can use the real-world artifact to evaluate these actions and their effects and convert them
% into rewards to provide a signal for the agent to learn from.
In approximate value-based methods, the objective is to learn the weights $w$ of a function
$\hat{V}(s;w)$ that best approximates the value function $V_\pi$ under some policy $\pi$.

Predictive Objective: $\overline{\text{VE}}(w) \doteq \sum_{s \in \calS} \mu(s) [V_\pi(s) -
		\hat{V}(s;w)]^2$.

$\mu$ is the on-policy distribution, that determines the importance given to states in-terms of the prediction error.
\begin{itemize}
	\item If $v_\pi$ is not available, on can use monte-carlo estimates for target updates
	\item If you use bootstrapping, then the assumption that the gradient is independent of the target becomes invalid (semi gradient methods)
	\item For linear function approximation, bootstrapping still works robustly
	\item On-policy control can be done in the same way as tabular case with $\epsilon$-greedy policies
	\item Off-policy methods use importance sampling to weight the error based on target and behavior policies
	\item Off-policy methods face divergence in value estimation (due to the difference in state distributions induced by behavior and target policies)
	      when combined with function approximation and boostrapping (deadly triad)
\end{itemize}

\subsection{Policy Gradient methods}
\label{sec:spg}
Policy gradient methods allow for directly learning a parameterized policy that enables action
selection without the use of a (action-)value function.
% These are generally called Policy gradient methods, and are a major area of study within
% Reinforcement learning.
Policy gradient methods have the advantage that in many cases the policy space could be simpler to
approximate compared to the value function space.
We discuss Policy gradient methods in more detail in the next section as it serves as a base for
the rest of our work.

\section{Multi-agent Reinforcement Learning (MARL)}\label{sec:marl}
% While game theory provides a framework in defining problems that are multiagent in nature and
% optimiality of solutions to those problems, reinforcement learning can be leveraged to learn
% such optimal strategies.
There has been increasing interest in designing RL algorithms for the multiagent setting.

\subsection{Challenges in MARL}
Although Policy Gradient methods and their derivatives have theoretical guarantees in single-agent
settings, they are not directly extendable to multi-agent settings due to the non-stationarity of
the dynamics.
% Reinforcement learning algorithms have also been studied in settings that have more than one agent
% where the agents try to maximize some objective in a cooperative or competitive manner.
% Athough many of these algorithms have exhibited strong empirical performance in multi-agent
% settings, there are a few key challenges when extending Reinforcement learning algorithms to
% multi-agent settings.
% From a theoretical perspective, the foundational assumption for RL algorithms is that the MDP's
% transition dynamics remain stationary.
% However, in multi-agent settings non-stationary is induced due to the presence of the other agents.
% This makes it difficult in adapting the algorithms disucssed above directly to multi-agent
% settings.
Also, from an algorithm design perspective, the action space explodes exponentially in multi-agent
settings making it computationally challenging to apply reinforcement algorithms without
decomposing the problem into more managable sub-problems first.

\subsection{Methods in MARL}
\textbf{Independent RL}
\begin{itemize}
	\item {Independent PG}
	      \item{Centralized Training, Decentralized Execution}
\end{itemize}

\textbf{Self-play RL}

\textbf{Decentralized RL}

\section{Online Convex Optimization and Mirror Descent}
\textit{(The brief background provided in this section closely follows the details as presented
	in~\cite{shalev-shwartzOnline2012}.
	For a more in-depth introduction into Online learning and Online Convex Optimization, please refer
	to the above work.)
}

% Definition
In Online Learning, a learner is tasked with predicting the answer to a set of questions over a
sequence of consecutive rounds.
We now define an Online learning problem more formally as follows:

\begin{definition}
	\label{def:olearning} For each round $t$, given an instance $x_t \in \calX$, and
	a prediction $p_t \in \calY$, a loss function $l(p_t, y_t) \mapsto \R$
\end{definition}

At each round t, a question $x_t$ is taken from an instance domain $\calX$, and
the learner is required to predict an answer, $p_t$ to this question.
After the prediction is made, the correct answer $y_t$, from a target domain $\calY$ is revealed
and the learner suffers a loss $l(p_t, y_t)$.
The prediction $p_t$ could belong to $\calY$ or a larger set, $\mathcal{D}$.

% Assumptions around the problem definition
% It is known that without any further assumptions about the learning problem, a learner's regret can
% be unbounded in an adversarial setting~\cite{coverBehavior1965}.

% A popular assumption in Online learning settings is called Realizability, where we assume the
% target mapping arises from a fixed hypothesis class $\calH$.
The main aim of an online learning algorithm $A$, is to minimize the cumulative regret of a learner
with respect to the best competing hypothesis $h^*$ from the assumed hypothesis class $\calH$.

\begin{equation}
	\label{eqn:regret}
	Regret_T(h^*) = \sum_{t=1}^T l(p_t, y_t) - \sum_{t=1}^T l(h^*(x_t), y_t),
\end{equation}

The regret of $A$ with $\calH$ is,

\begin{equation}
	\label{eqn:regret_all} Regret_T(\calH) = \max_{h^* \in \calH} Regret_T(h^*)
\end{equation}

A popular framework for studying and designing Online learning algorithms is
through Convex optimization.
The assumptions made around the framework provides properties that are useful in deriving
convergence guarantees.
We define a few terms below that are used in this section and the following ones.

% Convex sets and Convex functions

\begin{definition}[Strongly Smooth]~\label{def:strsmooth}
	Given a convex set $\calX \in \R^n$, a convex function $f: \calX \mapsto \R$ is $\sigma$-strongly
	smooth with respect to a norm $\|.
		\|$, if $\|\nabla f(x) - \nabla f(y) \|_{\ast} \leq \sigma \|x - y \|, \forall x, y \in \calX$.
	For a given constant $L$, this is also referred to as $L-smooth$.
\end{definition}

% Strongly smooth/L-smooth

% Monotonicity/Strong monotonicity

The typical structure of online learning expressed as an online convex optimization problem is as
follows:

\begin{alprocedure}[H] \algorithmcfname{OCO}~\label{alg:oco}

	input: a convex set S for t = 1, 2, $\ldots$

	predict a
	vector $w_t \in S$

	receive a convex loss function $f_t: S \mapsto \mathbb{R}$
\end{alprocedure}

Reframing~\ref{eqn:regret} in terms of convex optimization,
we refer to a competing hypothesis here as some vector $u$ from the convex set $S$.

\begin{equation}
	Regret_T(u) = \sum_{t=1}^T f_t(w_t) - \sum_{t=1}^T f_t(u)
\end{equation}

and
similarly, the regret with respect to a set of competing vectors $U$ is,
\begin{equation}
	Regret_T(U) = \max_{u \in U} Regret_T(u)
\end{equation}

The set $U$ can be same
as $S$ or different in other cases.
Here we assume $U=S$ and $S=\mathbb{R^d}$ unless specified otherwise.

\subsection{FoReL}\label{sec:forel}

Follow-the-Regularized-leader (FoReL) is a classic online learning algorithm that acts as a base in
deriving various regret mimization algorithms.
The idea of FoReL is to include a regularization term to stabilize the updates in each iteration
leading to better convergence behaviors.

The learning rule can be written as,

$$\forall t, w_t = \arg \min_{w \in S}
	\sum_{i=1}^{t-1} f_i(w) + R(w).
$$

where $R(w)$ is the regularization term.
The choice of the regularization function lead to different algorithms with varying regret bounds.

\subsection{Gradient Descent}

In the case of linear loss functions with respect to some $z_t$, i.e., $f_t(w) = \langle w, z_t
	\rangle$, and $S=\mathbb{R}^d$, if FoReL is run with $l_2$-norm regularization $R(w) = \frac{1}{2
		\eta} \|w\|_2^2$, then the learning rule can be written as,

\begin{equation}
	w_{t+1} = -\eta \sum_{i=1}^t z_i = w_t - \eta z_t
\end{equation}

Since, $\nabla
	f_t(w_t) = z_t$, this can also be written as, $w_{t+1} = w_t - \eta \nabla f_t(w_t)$.
This update rule is also commonly known as Online Gradient Descent.
The regret of FoReL run on Online linear optimization with a euclidean-norm regularizer is:

$$Regret_T(U) \leq BL \sqrt {2T}.
$$

where $U = {u : \|u\| \leq B}$ and $\frac{1}{T} \sum_{t=1}^T \|z_t\|_2^2 \leq L^2$ with $\eta = \frac{B}{L\sqrt{2T}}$.

Beyond Euclidean regularization, FoReL can also be run with other regularization functions and
yield similar regret bounds given that the regularization functions are strongly convex.

\begin{definition}
	For any $\sigma$-strongly-convex function $f: S \mapsto \mathbb{R}$ with respect to a norm $\|.
		\|$, for any $w \in S$,
	\begin{equation}
		\forall z \in \partial f(w), \forall u \in S, f(u) \geq f(w) + \langle z, u - w\rangle + \frac{\sigma}{2}\| u - w \|^2.
	\end{equation}
\end{definition}

\begin{lemma}
	\label{lem:forelrb}
	For a FoReL algorithm producing a sequence of vectors $w_1, \ldots, w_T$ with a sequence of loss
	functions $f_1, \ldots, f_T$, for all $u \in S$, $$\sum_{t=1}^T (f_t(w_t) - f_t(u)) \leq R(u) -
		R(w_1) + \sum_{t=1}^T (f_t(w_t) - f_t(w_{t+1}))$$
\end{lemma}

\subsection{Hedge} - what is hedge?

Gradient descent as a FTRL variant

\begin{equation}
	\label{eqn:eucproj}
	P_\calX(u) = \arg \min_{x \in \calX} \| u - x \|.
\end{equation}

\textbf{Proximal mapping}
\begin{definition}[Proximal mapping] Given a function $f: \calX \mapsto \R$, the proximal mapping
	of $f$ is given by: $$\text{prox}_f(x) = \arg \min_{u \in \R} \left\{ f(u) + \frac{1}{2} \| u-x
		\|^2 \right\} $$ for any $x \in \calX$.
\end{definition}

\textbf{Projected subgradient method}

\begin{equation}
	\label{eqn:projsubg}
	x_{k+1} = P_{\calX}(x_k - t_k f'(x_k))
\end{equation}

where $f'(x_k) \in \partial f(x_k)$.

\subsection{Mirror Descent}
Mirror descent is a popular first order optimization algorithm that has seen wide applications in
machine learning, and reinforcement learning.
Mirror Descent generalizes the notion of projected subgradient method~\ref{eqn:projsubg} to
non-euclidean settings through a generalized norm called Bergman Divergence.

\begin{definition}[Bergman Divergence]
	\label{def:bregman}
	Given a convex set $\calX \subset \R^n$, and a differentiable $\sigma$-strongly convex function
	$\omega: \calX \mapsto \R$, the Bregman Divergence associated with the function $\omega$ is defined
	as,

	$$ B_\omega(x, y) = \omega(x) - \omega(y) - \langle \nabla \omega(y), x-y
		\rangle.
	$$
\end{definition}

Bergman Divergence is not a true norm, since it does not satisfy the triangle inequality.
% However it serves as a proximal measure for non-euclidean settings.

\textbf{Non-euclidean Proximal Gradient method}

\begin{equation}
	\label{eqn:noneucproxgrad}
	x_{k+1} = \arg \min_{x \in \calX} \left \{ \left \langle \frac{1}{L_k} \nabla f(x_k) - \nabla \omega(x_k), x \right \rangle + \frac{1}{L_k} g(x) + \omega(x) \right \}.
\end{equation}
For an alternate view of deriving Mirror Descent as an improvement of FoReL, please refer
to~\cite{shalev-shwartzOnline2012}[Section 2.6].
% A disadvantage of FoReL~\ref{sec:forel} in solving online learning problems is that, there is a
% minimization that must be performed at every step.
% Mirror descent overcomes this by using a recursive update rule that does not required us to perform
% a minimization at every step.