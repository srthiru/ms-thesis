\addvspace {\baselineskip }
\contentsline {chapter}{\xnumberline {1\hskip 2em}\uppercase {Introduction}}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1} Outline}{6}{section.1.1}%
\addvspace {\baselineskip }
\contentsline {chapter}{\xnumberline {2\hskip 2em}\uppercase {Background}}{7}{chapter.2}%
\contentsline {section}{\numberline {2.1} Game Theory}{7}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1} Problems Formulations in Game Theory}{8}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2} Game Representations}{9}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3} Equilibrium Concepts}{12}{subsection.2.1.3}%
\contentsline {section}{\numberline {2.2} Reinforcement Learning}{14}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1} Tabular methods}{16}{subsection.2.2.1}%
\contentsline {section}{\numberline {2.3} Multi-agent Reinforcement Learning (MARL)}{17}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1} Challenges in MARL}{18}{subsection.2.3.1}%
\contentsline {section}{\numberline {2.4} Online Learning, and Convex Optimization}{19}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1} Convergence analysis}{20}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2} Convex Optimization}{21}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3} FTRL}{23}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4} Mirror Descent}{24}{subsection.2.4.4}%
\contentsline {subsection}{\numberline {2.4.5} Proximal gradient methods}{25}{subsection.2.4.5}%
\contentsline {subsection}{\numberline {2.4.6} Regret Minimization and CFR}{26}{subsection.2.4.6}%
\addvspace {\baselineskip }
\contentsline {chapter}{\xnumberline {3\hskip 2em}\uppercase {Mirror Descent in Reinforcement Learning}}{28}{chapter.3}%
\contentsline {section}{\numberline {3.1} SPG}{28}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1} Trust-region methods}{30}{subsection.3.1.1}%
\contentsline {section}{\numberline {3.2} MDPO}{31}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1} Tabular MDPO}{32}{subsection.3.2.1}%
\contentsline {section}{\numberline {3.3} MMD}{33}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1} Connection between Variational Inequalities and QREs}{33}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2} MMD Algorithm}{35}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3} Behavioral form MMD}{36}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4} Closed-form vs Behavioral-form}{37}{subsection.3.3.4}%
\contentsline {subsection}{\numberline {3.3.5} Comparison of MMD to other Mirror Decent based methods}{37}{subsection.3.3.5}%
\addvspace {\baselineskip }
\contentsline {chapter}{\xnumberline {4\hskip 2em}\uppercase {Modified Updates for Last-iterate Convergence}}{39}{chapter.4}%
\contentsline {section}{\numberline {4.1} Neural Replicator Dynamics (NeuRD)}{40}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1} Replicator Dynamics and No-regret Learning}{40}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2} NeuRD}{41}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3} Relation to Natural Policy Gradients}{42}{subsection.4.1.3}%
\contentsline {subsection}{\numberline {4.1.4} Alternating vs Simultaneous gradient updates}{42}{subsection.4.1.4}%
\contentsline {section}{\numberline {4.2} Extragradient and Optimistic Gradient}{43}{section.4.2}%
\addvspace {\baselineskip }
\contentsline {chapter}{\xnumberline {5\hskip 2em}\uppercase {Tabular Experiments}}{47}{chapter.5}%
\contentsline {section}{\numberline {5.1} Experimental Domains}{47}{section.5.1}%
\contentsline {section}{\numberline {5.2} Evaluation Metrics}{48}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1} Divergence to the equilibrium}{49}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2} Exploitability}{49}{subsection.5.2.2}%
\contentsline {section}{\numberline {5.3} Experiment Setup}{49}{section.5.3}%
\contentsline {section}{\numberline {5.4} Results}{50}{section.5.4}%
\addvspace {\baselineskip }
\contentsline {chapter}{\xnumberline {6\hskip 2em}\uppercase {Deep Multi-Agent RL Experiments}}{55}{chapter.6}%
\contentsline {section}{\numberline {6.1} Experimental Domains}{55}{section.6.1}%
\contentsline {section}{\numberline {6.2} Evaluation Metrics}{56}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1} Approximate Exploitability}{56}{subsection.6.2.1}%
\contentsline {section}{\numberline {6.3} Experiment Setup}{57}{section.6.3}%
\contentsline {section}{\numberline {6.4} Results}{59}{section.6.4}%
\addvspace {\baselineskip }
\contentsline {chapter}{\xnumberline {7\hskip 2em}\uppercase {Discussion}}{62}{chapter.7}%
\contentsline {section}{\numberline {7.1} Algorithm Design Choices:}{62}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1} NeuRD-fix}{62}{subsection.7.1.1}%
\contentsline {section}{\numberline {7.2} Entropy Regularization}{63}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1} Trust-region constraints}{63}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2} EG and Optimism}{64}{subsection.7.2.2}%
\contentsline {section}{\numberline {7.3} Directions for Future Work}{64}{section.7.3}%
\addvspace {\baselineskip }
\contentsline {chapter}{\xnumberline {8\hskip 2em}\uppercase {Conclusion}}{66}{chapter.8}%
\addvspace {\baselineskip }
\contentsline {chapter}{\xnumberline {\ }APPENDICES}{67}{chapter.8}%
\contentsline {chapter}{\xnumberline {\ }\ \ \ \ \ Appendix A}{68}{appendix.A}%
\contentsline {chapter}{\xnumberline {\ }\ \ \ \ \ Appendix B}{69}{appendix.B}%
\contentsline {chapter}{\xnumberline {\ }\ \ \ \ \ Appendix C}{70}{appendix.C}%
\contentsline {chapter}{\xnumberline {\ }\ \ \ \ \ Appendix D}{71}{appendix.D}%
\addvspace {\baselineskip }
\contentsline {chapter}{\xnumberline {\ }CITED LITERATURE}{72}{appendix.D}%
