\documentclass{uicthesi}

\usepackage{booktabs} % For formal tables

\usepackage{framed}
\usepackage{hyperref}
\usepackage{balance}
\usepackage[dvips]{graphics,color}
\usepackage[dvipsnames,usenames]{xcolor}

\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{multirow,tabularx}
\usepackage{placeins}
%\usepackage{miniltx}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{bm}

\usepackage{csquotes}
\usepackage{array}
\usepackage[yyyymmdd,hhmmss]{datetime}
\usepackage[subject={Todo}]{pdfcomment}
\usepackage[textsize=scriptsize,bordercolor=black!20]{todonotes}
\usepackage{colortbl}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage[algo2e,titlenumbered,ruled]{algorithm2e} 
\usepackage{lipsum,environ,amsmath}
\usepackage{slashbox,booktabs,amsmath}
%\usepackage{todonotes}
\usepackage{cite}


\usepackage{dsfont}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{float}
\usepackage{xspace}


\hyphenation{PageRank Convex dictatorship Dic-ta-tor-ship}

\newcounter{Lcount}
\newcommand{\numsquishlist}{
   \begin{list}{\arabic{Lcount}. }
    { \usecounter{Lcount}
 \setlength{\itemsep}{-.1ex}      \setlength{\parsep}{0ex}
      \setlength{\topsep}{0ex}       \setlength{\partopsep}{0ex}
      \setlength{\leftmargin}{1em} \setlength{\labelwidth}{1em}
      \setlength{\labelsep}{0.1em} } }
\newcommand{\numsquishend}{\end{list}}

\newcommand{\squishlist}{
   \begin{list}{$\bullet$}
    { \setlength{\itemsep}{-.1ex}      \setlength{\parsep}{0ex}
      \setlength{\topsep}{0ex}       \setlength{\partopsep}{0ex}
      \setlength{\leftmargin}{.8em} \setlength{\labelwidth}{1em}
      \setlength{\labelsep}{0.5em} } }
\newcommand{\squishend}{\end{list}}



\makeatletter
\DeclareOldFontCommand{\rm}{\normalfont\rmfamily}{\mathrm}
\DeclareOldFontCommand{\sf}{\normalfont\sffamily}{\mathsf}
\DeclareOldFontCommand{\tt}{\normalfont\ttfamily}{\mathtt}
\DeclareOldFontCommand{\bf}{\normalfont\bfseries}{\mathbf}
\DeclareOldFontCommand{\it}{\normalfont\itshape}{\mathit}
\DeclareOldFontCommand{\sl}{\normalfont\slshape}{\@nomath\sl}
\DeclareOldFontCommand{\sc}{\normalfont\scshape}{\@nomath\sc}
\makeatother


\newcounter{problem}
\newenvironment{problem}[1][htb]
  {\renewcommand{\algorithmcfname}{Problem}% Update algorithm name
   \begin{algorithm2e}[#1]%
   \SetAlFnt{\small}
    \SetAlCapFnt{\small}
    \SetAlCapNameFnt{\small}
    \SetAlCapHSkip{0pt}
  }{\end{algorithm2e}}

  \newenvironment{alprocedure}[1][htb]
  {\renewcommand{\algorithmcfname}{Algorithm}% Update algorithm name
   \begin{algorithm2e}[#1]%
    \SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

  }{\end{algorithm2e}}

% T: Addditions to the template
% For darkheaded arrows
\usepackage{wasysym}

% For plots
\usepackage{pgfplots}

% For equations
\usepackage{amsmath}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

% Color comments
\definecolor{teal2}{RGB}{115, 191, 184}
\definecolor{lav}{RGB}{108, 110, 160}
\definecolor{orang}{RGB}{222, 100, 73}
\definecolor{gren}{RGB}{97, 148, 101}
\definecolor{yel}{RGB}{246, 202, 131}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\fillin}[1]{\textcolor{blue}{#1}}
\newcommand{\rev}[1]{\textcolor{orang}{#1}}
\newcommand{\revdone}[1]{\textcolor{gren}{#1}}
\newcommand{\correct}[1]{\textcolor{teal2}{#1}}
\newcommand{\note}[1]{\textcolor{lav}{#1}}
\newcommand{\old}[1]{\textcolor{gray}{#1}}


% Shorthand for notations
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calA}{\mathcal{A}}

% For wrapping things around the text
\usepackage{wrapfig}

% For subtables
\usepackage{subcaption}


\begin{document}

% Declarations for Front Matter

\title{Modified Updates for Mirror Descent-based methods in Two-player Zero-sum games.}
\author{Thiruvenkadam Sivaprakasam Radhakrishnan}
\pdegrees{}
\degree{Master's in Computer Science}
\committee{
	Prof.~Ian Kash, Chair and Advisor \\
	Prof.~Anastasios Sidiropoulos \\
	Prof.~Ugo Buy} \maketitle

\acknowledgements {The thesis has been completed.
	% \dedication% {\null\vfil% {\large% \begin{center}% To myself,\\\vspace{12pt}% Perry H. Disdainful,\\\vspace{12pt}% the only person worthy of my company.% \end{center}}% \vfil\null}
	.. (INSERT YOUR TEXTS)\\

	\begin{flushright}
		YOUR INITIAL
	\end{flushright}}

\tableofcontents \listoftables \listoffigures
\listofabbreviations %\begin{noindent}
\begin{list} {} {\setlength {\labelwidth}{1in} \setlength{\leftmargin}{1.5in}
		\setlength{\labelsep}{.5in} \setlength{\rightmargin}{\leftmargin}}
    \item[$\pi$\hfill] A policy.
	  \item[$\theta$\hfill] Parameters of a function approximator.
		% \acknowledgements% {I want to ``thank'' my committee, without whose ridiculous demands, I% would have graduated so, so, very much faster.}% \preface% This preface is purely optional at UIC.
\end{list}
%\end{noindent}

\summary Due to the rising
popularity of reinforcement learning as a practical approach in tackling problems in various
domains, there has been increased efforts in designing reinforcement algorithms for multi-agent
settings that are theoretically grounded, and practically useful.
Reinforcement learning has seen a wide adaptation as a practical approach in tackling various
problem domains with strides made in theoretical understanding, methods to curb limitations, and
strong empirical performance in a diverse range of applications.
This has accelerated the study of designing reinforcement learning algorithms for the multi-agent
setting through the formulation of novel frameworks, and adaptation of ideas from other research
areas.
However, many such algorithms have complex implementation details and training procedures compared
to their single-agent counterparts.
It is of practical importance to design multi-agent reinforcement algorithms that are less
convoluted and exhibit strong empirical performance in converging to an optimal or a near-optimal
solution.
To this end, we study reinforcement learning algorithms in the context of the fundamental
multi-agent setting of two-player zero-sum games.
We examine recent advancements in adapting mirror descent as a practical deep reinforcement
learning algorithm.
Specifically, we study two algorithms that are theoretically well-motivated and exhibit a strong
performance in single, and multi-agent settings, namely - Mirror Descent Policy Optimization and
Magnetic Mirror Descent.
We propose the incorporation of certain modifications to these algorithms to improve their
convergence behavior (particularly to induce or speed up last-iterate convergence) in two-player
zero-sum games.
The modifications we study, namely - the Neural Replicator Dynamics, Extragradient updates, and
Optimistic Updates were originally proposed and studied under varying contexts.
In this work, we aim to bring these together as potential candidates that can be incorporated into
reinforcement learning algorithms and study their effects when combined with the above-mentioned
algorithms.
We experimentally evaluate the performance of these algorithms in tabular normal-form settings and
in extensive-form games under function approximation.
Through this study, we provide some recommendations for designing multi-agent reinforcement
learning algorithms and close with some remarks about future research directions.

\include{chapters/0_intro.tex}
\include{chapters/outline.tex}
\include{chapters/1_background.tex}
\include{chapters/2_mmd.tex}
\include{chapters/3_updates.tex}
\include{chapters/4_experiments.tex}
\include{chapters/5_neural.tex}
\include{chapters/6_discussion.tex}
\include{chapters/7_conclusion.tex}

% \include{derivations.tex}
% \include{oco.tex}

\appendices
\newpage
\appendix
\include{chapters/appendix.tex}

%\nocite{*}
\bibformb
\bibliography{BibFile,Citations}
\newpage
% \vita
% This is where the vita goes.  Its organization is left as an exercise.

\end{document}